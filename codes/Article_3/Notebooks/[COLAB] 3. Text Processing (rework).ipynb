{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55422da",
   "metadata": {
    "id": "c55422da"
   },
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e8e3a",
   "metadata": {
    "id": "1e2e8e3a"
   },
   "source": [
    "After having cleaned all the discharged notes from irrelevant information, we will move onto processing the text (*removing stopwords and lemmatizing everything*) and performing the classical Bag of Word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c66ca5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646133142022,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "3c66ca5d",
    "outputId": "49256fe0-d4b2-4f9b-a828-68e3af9de261"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luca9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import utils\n",
    "from nltk.corpus import stopwords # stopword library\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS # stopword library\n",
    "try:\n",
    "  from stop_words import get_stop_words # stopword library\n",
    "except:\n",
    "  !pip install stop_words\n",
    "  from stop_words import get_stop_words # stopword library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d93e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "icu_stays = True # set to TRUE if we want to have only ICU stays\n",
    "med_7 = False # set to false if we want to avoid using Med7 preprocessing\n",
    "\n",
    "if med_7 == False: \n",
    "    tag_med7 = '_nomed7'\n",
    "else:\n",
    "    tag_med7 = ''\n",
    "    \n",
    "if icu_stays == True:\n",
    "    tag_icu = '_icu'\n",
    "    icu_folder = 'icu_only'\n",
    "else:\n",
    "    tag_icu = ''\n",
    "    icu_folder = 'all_hosp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9zzTu_nPLIB2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21708,
     "status": "ok",
     "timestamp": 1646133071885,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "9zzTu_nPLIB2",
    "outputId": "99ca4993-9167-4f41-e84e-c6804f242050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "if IN_COLAB:  \n",
    "  # Mount the Google Drive at mount\n",
    "  mount='/content/gdrive'\n",
    "  print(\"Colab: mounting Google drive on \", mount)\n",
    "  # connect your colab with the drive\n",
    "  drive.mount(mount)\n",
    "\n",
    " # Switch to the directory on the Google Drive that you want to use\n",
    "  import os\n",
    "  path_to_repo = mount + \"/My Drive/MIMIC-III Text Mining/Readmission\"\n",
    "\n",
    "else:\n",
    "   path_to_repo = os.path.dirname(os.getcwd())\n",
    "\n",
    "  \n",
    "print(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18514026",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1646133076100,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "18514026",
    "outputId": "da3ad484-fe30-4ba5-d836-bc1326c3684e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\\Readmission\\data\\icu_only\\\n"
     ]
    }
   ],
   "source": [
    "path_to_data = os.path.join(path_to_repo, \"Readmission\",\"data\", icu_folder,\"\")\n",
    "print(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18ebb76",
   "metadata": {
    "id": "c18ebb76"
   },
   "outputs": [],
   "source": [
    "# load it back\n",
    "df_final = pd.read_feather(f'{path_to_data}df_processed{tag_med7}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b9759",
   "metadata": {
    "id": "8e6b9759"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d0a93c",
   "metadata": {
    "id": "33d0a93c"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    \"\"\"\n",
    "    Function to split each text into tokens\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cdbe81",
   "metadata": {
    "id": "11cdbe81"
   },
   "source": [
    "We first need to lowercase everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de30e6da",
   "metadata": {
    "id": "de30e6da"
   },
   "outputs": [],
   "source": [
    "# lowercase everything\n",
    "df_final.clean_text = df_final.clean_text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8574ae88",
   "metadata": {
    "id": "8574ae88"
   },
   "outputs": [],
   "source": [
    "# lowercase everything\n",
    "df_final.final_diagnosis = df_final.final_diagnosis.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15067bf1",
   "metadata": {
    "id": "15067bf1"
   },
   "source": [
    "Then we define an expanded stopword list pulling terms from multiple libraries (SpaCy, NLTK, Gensim and Stop-Words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c20c48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1646133149273,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "a6c20c48",
    "outputId": "d330d813-4cf2-483a-85ee-d5b70e39051b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords\n",
    "stop_words = stopwords.words('english') # nltk stopwords\n",
    "stop_words.extend(list(STOP_WORDS)) # add spacy stopwords\n",
    "stop_words.extend(get_stop_words('en')) # add stop_words stopwords\n",
    "stop_words.extend(list(gensim.parsing.preprocessing.STOPWORDS)) # add Gensim set\n",
    "stop_words = list(set(stop_words)) # get only unique values\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57245b1f",
   "metadata": {
    "id": "57245b1f"
   },
   "source": [
    "And then proceed with stopwords removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f412ed6d",
   "metadata": {
    "id": "f412ed6d"
   },
   "outputs": [],
   "source": [
    "# Gensim Functions - > the version I had on my PC did not include custom stopwords\n",
    "def remove_stopwords(s, stopwords=None):\n",
    "    \"\"\"Remove :const:`~gensim.parsing.preprocessing.STOPWORDS` from `s`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "    stopwords : iterable of str, optional\n",
    "        Sequence of stopwords\n",
    "        If None - using :const:`~gensim.parsing.preprocessing.STOPWORDS`\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Unicode string without `stopwords`.\n",
    "    Examples\n",
    "    --------\n",
    "    .. sourcecode:: pycon\n",
    "        >>> from gensim.parsing.preprocessing import remove_stopwords\n",
    "        >>> remove_stopwords(\"Better late than never, but better never late.\")\n",
    "        u'Better late never, better late.'\n",
    "    \"\"\"\n",
    "    s = utils.to_unicode(s)\n",
    "    return \" \".join(remove_stopword_tokens(s.split(), stopwords))\n",
    "\n",
    "def remove_stopword_tokens(tokens, stopwords=None):\n",
    "    \"\"\"Remove stopword tokens using list `stopwords`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : iterable of str\n",
    "        Sequence of tokens.\n",
    "    stopwords : iterable of str, optional\n",
    "        Sequence of stopwords\n",
    "        If None - using :const:`~gensim.parsing.preprocessing.STOPWORDS`\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        List of tokens without `stopwords`.\n",
    "    \"\"\"\n",
    "    if stopwords is None:\n",
    "        stopwords = STOPWORDS\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb3c635",
   "metadata": {
    "id": "1cb3c635"
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df_final.clean_text = df_final.clean_text.apply(lambda x: remove_stopwords(x, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb518ef",
   "metadata": {
    "id": "dcb518ef"
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df_final.final_diagnosis = df_final.final_diagnosis.apply(lambda x: remove_stopwords(x, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06734886",
   "metadata": {
    "id": "06734886"
   },
   "source": [
    "Finally we will tokenize words, ignoring punctuation, and lemmatize everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6703638d",
   "metadata": {
    "id": "6703638d"
   },
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225d68b8",
   "metadata": {
    "id": "225d68b8"
   },
   "outputs": [],
   "source": [
    "def full_cleaning(text, lemmatize = True, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"\n",
    "    Concatenates all the previously defined functions to clean and lemmatize our text\n",
    "    \"\"\"\n",
    "    data_stop = list(sent_to_words(text))\n",
    "    if lemmatize == True:\n",
    "      data_lemma = lemmatization(data_stop, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    else:\n",
    "      data_lemma = data_stop\n",
    "    return data_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kPALnCpPOcxs",
   "metadata": {
    "id": "kPALnCpPOcxs"
   },
   "source": [
    "We create two columns: one with text processed with lemmatization, one without it. The goal is to check whether pre-trained word embedding methods can spot more words without lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "GPKE76QaNNZf",
   "metadata": {
    "id": "GPKE76QaNNZf"
   },
   "outputs": [],
   "source": [
    "df_final[\"text_def_nolemma\"]= full_cleaning(df_final.clean_text, lemmatize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "RiuvR9ZMNN-N",
   "metadata": {
    "id": "RiuvR9ZMNN-N"
   },
   "outputs": [],
   "source": [
    "df_final[\"diagnosis_def_nolemma\"]= full_cleaning(df_final.final_diagnosis, lemmatize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34264183",
   "metadata": {
    "id": "34264183"
   },
   "outputs": [],
   "source": [
    "df_final[\"text_def\"]= full_cleaning(df_final.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2abb618e",
   "metadata": {
    "id": "2abb618e"
   },
   "outputs": [],
   "source": [
    "df_final[\"diagnosis_def\"]= full_cleaning(df_final.final_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad0c8374",
   "metadata": {
    "id": "ad0c8374"
   },
   "outputs": [],
   "source": [
    "y = pd.DataFrame(df_final.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef55d098",
   "metadata": {
    "id": "ef55d098"
   },
   "outputs": [],
   "source": [
    "# need to reset the index\n",
    "y.reset_index(inplace=True, drop = True)\n",
    "# save our dataset up to now in feather format\n",
    "y.to_feather(f'{path_to_data}y{tag_med7}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9afae5d1",
   "metadata": {
    "id": "9afae5d1"
   },
   "outputs": [],
   "source": [
    "# load it back\n",
    "y = pd.read_feather(f'{path_to_data}y{tag_med7}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1b52bc",
   "metadata": {
    "id": "9d1b52bc"
   },
   "outputs": [],
   "source": [
    "# need to reset the index\n",
    "df_final.reset_index(inplace=True, drop = True)\n",
    "# save our dataset up to now in feather format\n",
    "df_final.to_feather(f'{path_to_data}df_stem{tag_med7}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1755b34a",
   "metadata": {
    "id": "1755b34a"
   },
   "outputs": [],
   "source": [
    "# load it back\n",
    "df_final = pd.read_feather(f'{path_to_data}df_stem{tag_med7}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[COLAB] 3. Text Processing (rework).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
