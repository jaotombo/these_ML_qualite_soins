{"cells":[{"cell_type":"markdown","metadata":{"id":"3PbINWsQka8c"},"source":["# LIME Expansion"]},{"cell_type":"markdown","metadata":{"id":"kbPGmB6glPcn"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":154971,"status":"ok","timestamp":1659884658719,"user":{"displayName":"Luca Adorni","userId":"07135966571450304185"},"user_tz":300},"id":"1Ze7w73-kaOq","outputId":"d4cb1696-eff0-422b-ecda-75f4dddef9f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 6.8 MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","Successfully installed pip-22.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n","Collecting setuptools\n","  Downloading setuptools-63.4.2-py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n","Installing collected packages: setuptools\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","Successfully installed setuptools-63.4.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pkg_resources"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting autogluon==0.4.1\n","  Downloading autogluon-0.4.1-py3-none-any.whl (9.5 kB)\n","Collecting autogluon.core[all]==0.4.1\n","  Downloading autogluon.core-0.4.1-py3-none-any.whl (189 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting autogluon.vision==0.4.1\n","  Downloading autogluon.vision-0.4.1-py3-none-any.whl (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting autogluon.text==0.4.1\n","  Downloading autogluon.text-0.4.1-py3-none-any.whl (162 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting autogluon.tabular[all]==0.4.1\n","  Downloading autogluon.tabular-0.4.1-py3-none-any.whl (267 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.2/267.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting autogluon.features==0.4.1\n","  Downloading autogluon.features-0.4.1-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon==0.4.1) (2.23.0)\n","Requirement already satisfied: pandas<1.4,>=1.2.5 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.3.5)\n","Collecting autogluon.common==0.4.1\n","  Downloading autogluon.common-0.4.1-py3-none-any.whl (37 kB)\n","Requirement already satisfied: scikit-learn<1.1,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.0.2)\n","Requirement already satisfied: numpy<1.23,>=1.21 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.21.6)\n","Collecting boto3\n","  Downloading boto3-1.24.46-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dask<=2021.11.2,>=2021.09.1\n","  Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting distributed<=2021.11.2,>=2021.09.1\n","  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy<1.8.0,>=1.5.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.7.3)\n","Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon==0.4.1) (4.64.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon==0.4.1) (3.2.2)\n","Collecting ray<1.11,>=1.10\n","  Downloading ray-1.10.0-cp37-cp37m-manylinux2014_x86_64.whl (59.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting psutil<5.9,>=5.7.3\n","  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.3/296.3 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (2.6.3)\n","Collecting catboost<1.1,>=1.0\n","  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastai<2.6,>=2.3.1\n","  Downloading fastai-2.5.6-py3-none-any.whl (188 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xgboost<1.5,>=1.4\n","  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lightgbm<3.4,>=3.3\n","  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch<1.11,>=1.0\n","  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1102413824 bytes == 0x37f9e000 @  0x7f1ffd72a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x593fce\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting omegaconf<2.2.0,>=2.1.1\n","  Downloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fairscale<0.5.0,>=0.4.5\n","  Downloading fairscale-0.4.6.tar.gz (248 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting torchmetrics<0.8.0,>=0.7.2\n","  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.2/398.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nptyping<1.5.0,>=1.4.4\n","  Downloading nptyping-1.4.4-py3-none-any.whl (31 kB)\n","Collecting transformers<4.17.0,>=4.16.2\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open<5.3.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.text==0.4.1->autogluon==0.4.1) (5.2.1)\n","Collecting autogluon-contrib-nlp==0.0.1b20220208\n","  Downloading autogluon_contrib_nlp-0.0.1b20220208-py3-none-any.whl (157 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scikit-image<0.20.0,>=0.19.1\n","  Downloading scikit_image-0.19.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-lightning<1.7.0,>=1.5.10\n","  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting timm<0.6.0\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Pillow<9.1.0,>=9.0.1\n","  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setuptools<=59.5.0\n","  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece<0.2.0,>=0.1.95\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gluoncv<0.10.6,>=0.10.5\n","  Downloading gluoncv-0.10.5.post0-py2.py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting contextvars\n","  Downloading contextvars-2.4.tar.gz (9.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tokenizers>=0.9.4\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flake8\n","  Downloading flake8-5.0.4-py2.py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yacs>=0.1.6\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon==0.4.1) (6.0.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon==0.4.1) (2022.6.2)\n","Collecting sacremoses>=0.0.38\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon==0.4.1) (3.17.3)\n","Collecting sentencepiece<0.2.0,>=0.1.95\n","  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (1.15.0)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (0.10.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (5.5.0)\n","Collecting fsspec>=0.6.0\n","  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.2/141.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (3.13)\n","Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (21.3)\n","Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (0.12.0)\n","Collecting partd>=0.3.10\n","  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n","Collecting cloudpickle>=1.1.1\n","  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2.2.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2.11.3)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2.4.0)\n","Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (7.1.2)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.7.0)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.0.4)\n","Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (5.1.1)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (0.13.0+cu113)\n","Collecting fastcore<1.5,>=1.3.27\n","  Downloading fastcore-1.4.5-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (1.0.3)\n","Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (0.0.7)\n","Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (3.4.1)\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (22.2.2)\n","Collecting autocfg\n","  Downloading autocfg-0.0.8-py3-none-any.whl (13 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.6,>=0.10.5->autogluon.vision==0.4.1->autogluon==0.4.1) (4.6.0.66)\n","Collecting portalocker\n","  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm<3.4,>=3.3->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (0.37.1)\n","Collecting typish>=1.7.0\n","  Downloading typish-1.9.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyyaml\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.4,>=1.2.5->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.4,>=1.2.5->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2.8.2)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (4.1.1)\n","Collecting pyDeprecate>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (2.8.0)\n","Collecting redis>=3.5.0\n","  Downloading redis-4.3.4-py3-none-any.whl (246 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.2/246.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon==0.4.1) (4.3.3)\n","Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.47.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon==0.4.1) (22.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon==0.4.1) (3.7.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.text==0.4.1->autogluon==0.4.1) (1.3.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.text==0.4.1->autogluon==0.4.1) (2.9.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.text==0.4.1->autogluon==0.4.1) (2021.11.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1,>=1.0.0->autogluon.core[all]==0.4.1->autogluon==0.4.1) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1,>=1.0.0->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.1.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.17.0,>=4.16.2->autogluon.text==0.4.1->autogluon==0.4.1) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting botocore<1.28.0,>=1.27.46\n","  Downloading botocore-1.27.46-py3-none-any.whl (9.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core[all]==0.4.1->autogluon==0.4.1) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core[all]==0.4.1->autogluon==0.4.1) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.4.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.4.1->autogluon==0.4.1) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (3.8.1)\n","Collecting locket\n","  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n","Collecting deprecated>=1.2.3\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon==0.4.1) (4.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<4.17.0,>=4.16.2->autogluon.text==0.4.1->autogluon==0.4.1) (3.8.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (2.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (0.6.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (3.0.9)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (1.0.7)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (2.0.6)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (8.1.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (0.4.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (1.0.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (3.3.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (3.0.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (1.9.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (2.4.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (0.10.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (1.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (3.4.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (0.4.6)\n","Collecting torchvision>=0.8.2\n","  Downloading torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.13.0-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.11.3-cp37-cp37m-manylinux1_x86_64.whl (23.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.0.1)\n","Collecting immutables>=0.9\n","  Downloading immutables-0.18-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.5/116.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting importlib-metadata\n","  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n","Collecting pyflakes<2.6.0,>=2.5.0\n","  Downloading pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mccabe<0.8.0,>=0.7.0\n","  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n","Collecting pycodestyle<2.10.0,>=2.9.0\n","  Downloading pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2.0.1)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon==0.4.1) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon==0.4.1) (5.9.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost<1.1,>=1.0->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (8.0.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon==0.4.1) (4.9.1)\n","Collecting colorama\n","  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon==0.4.1) (0.8.10)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.14.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (1.3.1)\n","Collecting markdown>=2.6.8\n","  Downloading Markdown-3.4-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon==0.4.1) (0.7.8)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.8.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (2.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (1.3.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon==0.4.1) (6.0.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon==0.4.1) (3.2.0)\n","Building wheels for collected packages: fairscale, antlr4-python3-runtime, sacremoses, contextvars\n","  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307225 sha256=f2666cccc85deb94014ccf2a7e4498a99fda4453a2a8ba16babec6a1bbdb931c\n","  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=9547d9e22f72bc043abd235ae5e771a56ac7dbd9b73d42ec7242260ae90454c0\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=5937df067fa476c4e7cdffcaad855747376b59e945665ddecd9caab393071e39\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7664 sha256=22a03b4b11fa49ce46ec77a8a25b5152c53dfd6774fa790d81f67aae6b33c5b0\n","  Stored in directory: /root/.cache/pip/wheels/0a/11/79/e70e668095c0bb1f94718af672ef2d35ee7a023fee56ef54d9\n","Successfully built fairscale antlr4-python3-runtime sacremoses contextvars\n","Installing collected packages: typish, tokenizers, sentencepiece, antlr4-python3-runtime, urllib3, torch, setuptools, sacremoses, pyyaml, pyflakes, pyDeprecate, pycodestyle, psutil, portalocker, Pillow, nptyping, mccabe, locket, jmespath, importlib-metadata, immutables, fsspec, deprecated, colorama, cloudpickle, yacs, xgboost, torchvision, torchmetrics, sacrebleu, redis, partd, omegaconf, markdown, flake8, fastcore, fairscale, contextvars, botocore, autocfg, timm, scikit-image, s3transfer, ray, lightgbm, huggingface-hub, gluoncv, dask, catboost, autogluon-contrib-nlp, transformers, distributed, boto3, fastai, autogluon.common, pytorch-lightning, autogluon.features, autogluon.core, autogluon.vision, autogluon.text, autogluon.tabular, autogluon\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.0+cu113\n","    Uninstalling torch-1.12.0+cu113:\n","      Successfully uninstalled torch-1.12.0+cu113\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 63.4.2\n","    Uninstalling setuptools-63.4.2:\n","      Successfully uninstalled setuptools-63.4.2\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 4.12.0\n","    Uninstalling importlib-metadata-4.12.0:\n","      Successfully uninstalled importlib-metadata-4.12.0\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Attempting uninstall: xgboost\n","    Found existing installation: xgboost 0.90\n","    Uninstalling xgboost-0.90:\n","      Successfully uninstalled xgboost-0.90\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.13.0+cu113\n","    Uninstalling torchvision-0.13.0+cu113:\n","      Successfully uninstalled torchvision-0.13.0+cu113\n","  Attempting uninstall: markdown\n","    Found existing installation: Markdown 3.4.1\n","    Uninstalling Markdown-3.4.1:\n","      Successfully uninstalled Markdown-3.4.1\n","  Attempting uninstall: fastcore\n","    Found existing installation: fastcore 1.5.13\n","    Uninstalling fastcore-1.5.13:\n","      Successfully uninstalled fastcore-1.5.13\n","  Attempting uninstall: scikit-image\n","    Found existing installation: scikit-image 0.18.3\n","    Uninstalling scikit-image-0.18.3:\n","      Successfully uninstalled scikit-image-0.18.3\n","  Attempting uninstall: lightgbm\n","    Found existing installation: lightgbm 2.2.3\n","    Uninstalling lightgbm-2.2.3:\n","      Successfully uninstalled lightgbm-2.2.3\n","  Attempting uninstall: dask\n","    Found existing installation: dask 2.12.0\n","    Uninstalling dask-2.12.0:\n","      Successfully uninstalled dask-2.12.0\n","  Attempting uninstall: distributed\n","    Found existing installation: distributed 1.25.3\n","    Uninstalling distributed-1.25.3:\n","      Successfully uninstalled distributed-1.25.3\n","  Attempting uninstall: fastai\n","    Found existing installation: fastai 2.7.7\n","    Uninstalling fastai-2.7.7:\n","      Successfully uninstalled fastai-2.7.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.10.2 which is incompatible.\n","torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.10.2 which is incompatible.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Pillow-9.0.1 antlr4-python3-runtime-4.8 autocfg-0.0.8 autogluon-0.4.1 autogluon-contrib-nlp-0.0.1b20220208 autogluon.common-0.4.1 autogluon.core-0.4.1 autogluon.features-0.4.1 autogluon.tabular-0.4.1 autogluon.text-0.4.1 autogluon.vision-0.4.1 boto3-1.24.46 botocore-1.27.46 catboost-1.0.6 cloudpickle-2.1.0 colorama-0.4.5 contextvars-2.4 dask-2021.11.2 deprecated-1.2.13 distributed-2021.11.2 fairscale-0.4.6 fastai-2.5.6 fastcore-1.4.5 flake8-5.0.4 fsspec-2022.7.1 gluoncv-0.10.5.post0 huggingface-hub-0.8.1 immutables-0.18 importlib-metadata-4.2.0 jmespath-1.0.1 lightgbm-3.3.2 locket-1.0.0 markdown-3.3.4 mccabe-0.7.0 nptyping-1.4.4 omegaconf-2.1.2 partd-1.2.0 portalocker-2.5.1 psutil-5.8.0 pyDeprecate-0.3.2 pycodestyle-2.9.1 pyflakes-2.5.0 pytorch-lightning-1.6.5 pyyaml-6.0 ray-1.10.0 redis-4.3.4 s3transfer-0.6.0 sacrebleu-2.2.0 sacremoses-0.0.53 scikit-image-0.19.3 sentencepiece-0.1.95 setuptools-59.5.0 timm-0.5.4 tokenizers-0.12.1 torch-1.10.2 torchmetrics-0.7.3 torchvision-0.11.3 transformers-4.16.2 typish-1.9.3 urllib3-1.25.11 xgboost-1.4.2 yacs-0.1.8\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# First install package from terminal:\n","!pip install -U pip\n","!pip install -U setuptools wheel\n","!pip install autogluon==0.4.1  # autogluon==0.4.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5kDKfWaNhu4"},"outputs":[],"source":["# import all required modules\n","import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.impute import KNNImputer\n","\n","pd.set_option('display.max_rows', None)  ###\n","pd.set_option('display.max_columns', None)  ###\n","pd.set_option('display.width', None)  ###\n","pd.set_option('display.max_colwidth', None)  ###\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import auc\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.inspection import permutation_importance\n","from numpy.lib.shape_base import row_stack\n","import pickle\n","import re"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17979,"status":"ok","timestamp":1659948369149,"user":{"displayName":"Franck Jaotombo","userId":"13595873829587329663"},"user_tz":-120},"id":"-HubBWl7NlsV","outputId":"0111ef01-7f8e-4d9c-ae98-ac79fb031307"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# connect colab with google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"sN6leQATFA44"},"source":["## LIME Expansion - Class Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YGJqY2wFC8h"},"outputs":[],"source":["class LimeError(Exception):\n","    \"\"\"Raise for errors\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQn_eGyRDIm_"},"outputs":[],"source":["\"\"\"\n","Explanation class, with visualization functions.\n","\"\"\"\n","from io import open\n","import os\n","import os.path\n","import json\n","import string\n","import numpy as np\n","\n","\n","from sklearn.utils import check_random_state\n","\n","\n","def id_generator(size=15, random_state=None):\n","    \"\"\"Helper function to generate random div ids. This is useful for embedding\n","    HTML into ipython notebooks.\"\"\"\n","    chars = list(string.ascii_uppercase + string.digits)\n","    return ''.join(random_state.choice(chars, size, replace=True))\n","\n","\n","class DomainMapper(object):\n","    \"\"\"Class for mapping features to the specific domain.\n","    The idea is that there would be a subclass for each domain (text, tables,\n","    images, etc), so that we can have a general Explanation class, and separate\n","    out the specifics of visualizing features in here.\n","    \"\"\"\n","\n","    def __init__(self):\n","        pass\n","\n","    def map_exp_ids(self, exp, **kwargs):\n","        \"\"\"Maps the feature ids to concrete names.\n","        Default behaviour is the identity function. Subclasses can implement\n","        this as they see fit.\n","        Args:\n","            exp: list of tuples [(id, weight), (id,weight)]\n","            kwargs: optional keyword arguments\n","        Returns:\n","            exp: list of tuples [(name, weight), (name, weight)...]\n","        \"\"\"\n","        return exp\n","\n","    def visualize_instance_html(self,\n","                                exp,\n","                                label,\n","                                div_name,\n","                                exp_object_name,\n","                                **kwargs):\n","        \"\"\"Produces html for visualizing the instance.\n","        Default behaviour does nothing. Subclasses can implement this as they\n","        see fit.\n","        Args:\n","             exp: list of tuples [(id, weight), (id,weight)]\n","             label: label id (integer)\n","             div_name: name of div object to be used for rendering(in js)\n","             exp_object_name: name of js explanation object\n","             kwargs: optional keyword arguments\n","        Returns:\n","             js code for visualizing the instance\n","        \"\"\"\n","        return ''\n","\n","\n","class Explanation(object):\n","    \"\"\"Object returned by explainers.\"\"\"\n","\n","    def __init__(self,\n","                 domain_mapper,\n","                 mode='classification',\n","                 class_names=None,\n","                 random_state=None):\n","        \"\"\"\n","        Initializer.\n","        Args:\n","            domain_mapper: must inherit from DomainMapper class\n","            type: \"classification\" or \"regression\"\n","            class_names: list of class names (only used for classification)\n","            random_state: an integer or numpy.RandomState that will be used to\n","                generate random numbers. If None, the random state will be\n","                initialized using the internal numpy seed.\n","        \"\"\"\n","        self.random_state = random_state\n","        self.mode = mode\n","        self.domain_mapper = domain_mapper\n","        self.local_exp = {}\n","        self.intercept = {}\n","        self.score = {}\n","        self.local_pred = {}\n","        if mode == 'classification':\n","            self.class_names = class_names\n","            self.top_labels = None\n","            self.predict_proba = None\n","        elif mode == 'regression':\n","            self.class_names = ['negative', 'positive']\n","            self.predicted_value = None\n","            self.min_value = 0.0\n","            self.max_value = 1.0\n","            self.dummy_label = 1\n","        else:\n","            raise LimeError('Invalid explanation mode \"{}\". '\n","                            'Should be either \"classification\" '\n","                            'or \"regression\".'.format(mode))\n","\n","    def available_labels(self):\n","        \"\"\"\n","        Returns the list of classification labels for which we have any explanations.\n","        \"\"\"\n","        try:\n","            assert self.mode == \"classification\"\n","        except AssertionError:\n","            raise NotImplementedError('Not supported for regression explanations.')\n","        else:\n","            ans = self.top_labels if self.top_labels else self.local_exp.keys()\n","            return list(ans)\n","\n","    def as_list(self, label=1, **kwargs):\n","        \"\"\"Returns the explanation as a list.\n","        Args:\n","            label: desired label. If you ask for a label for which an\n","                explanation wasn't computed, will throw an exception.\n","                Will be ignored for regression explanations.\n","            kwargs: keyword arguments, passed to domain_mapper\n","        Returns:\n","            list of tuples (representation, weight), where representation is\n","            given by domain_mapper. Weight is a float.\n","        \"\"\"\n","        label_to_use = label if self.mode == \"classification\" else self.dummy_label\n","        ans = self.domain_mapper.map_exp_ids(self.local_exp[label_to_use], **kwargs)\n","        ans = [(x[0], float(x[1])) for x in ans]\n","        return ans\n","\n","    def as_map(self):\n","        \"\"\"Returns the map of explanations.\n","        Returns:\n","            Map from label to list of tuples (feature_id, weight).\n","        \"\"\"\n","        return self.local_exp\n","\n","    def as_pyplot_figure(self, label=1, figsize=(4,4), **kwargs):\n","        \"\"\"Returns the explanation as a pyplot figure.\n","        Will throw an error if you don't have matplotlib installed\n","        Args:\n","            label: desired label. If you ask for a label for which an\n","                   explanation wasn't computed, will throw an exception.\n","                   Will be ignored for regression explanations.\n","            figsize: desired size of pyplot in tuple format, defaults to (4,4).\n","            kwargs: keyword arguments, passed to domain_mapper\n","        Returns:\n","            pyplot figure (barchart).\n","        \"\"\"\n","        import matplotlib.pyplot as plt\n","        exp = self.as_list(label=label, **kwargs)\n","        fig = plt.figure(figsize=figsize)\n","        vals = [x[1] for x in exp]\n","        names = [x[0] for x in exp]\n","        vals.reverse()\n","        names.reverse()\n","        colors = ['green' if x > 0 else 'red' for x in vals]\n","        pos = np.arange(len(exp)) + .5\n","        plt.barh(pos, vals, align='center', color=colors)\n","        plt.yticks(pos, names)\n","        if self.mode == \"classification\":\n","            title = 'Local explanation for class %s' % self.class_names[label]\n","        else:\n","            title = 'Local explanation'\n","        plt.title(title)\n","        return fig\n","\n","    def show_in_notebook(self,\n","                         labels=None,\n","                         predict_proba=True,\n","                         show_predicted_value=True,\n","                         **kwargs):\n","        \"\"\"Shows html explanation in ipython notebook.\n","        See as_html() for parameters.\n","        This will throw an error if you don't have IPython installed\"\"\"\n","\n","        from IPython.core.display import display, HTML\n","        display(HTML(self.as_html(labels=labels,\n","                                  predict_proba=predict_proba,\n","                                  show_predicted_value=show_predicted_value,\n","                                  **kwargs)))\n","\n","    def save_to_file(self,\n","                     file_path,\n","                     labels=None,\n","                     predict_proba=True,\n","                     show_predicted_value=True,\n","                     **kwargs):\n","        \"\"\"Saves html explanation to file. .\n","        Params:\n","            file_path: file to save explanations to\n","        See as_html() for additional parameters.\n","        \"\"\"\n","        file_ = open(file_path, 'w', encoding='utf8')\n","        file_.write(self.as_html(labels=labels,\n","                                 predict_proba=predict_proba,\n","                                 show_predicted_value=show_predicted_value,\n","                                 **kwargs))\n","        file_.close()\n","\n","    def as_html(self,\n","                labels=None,\n","                predict_proba=True,\n","                show_predicted_value=True,\n","                **kwargs):\n","        \"\"\"Returns the explanation as an html page.\n","        Args:\n","            labels: desired labels to show explanations for (as barcharts).\n","                If you ask for a label for which an explanation wasn't\n","                computed, will throw an exception. If None, will show\n","                explanations for all available labels. (only used for classification)\n","            predict_proba: if true, add  barchart with prediction probabilities\n","                for the top classes. (only used for classification)\n","            show_predicted_value: if true, add  barchart with expected value\n","                (only used for regression)\n","            kwargs: keyword arguments, passed to domain_mapper\n","        Returns:\n","            code for an html page, including javascript includes.\n","        \"\"\"\n","\n","        def jsonize(x):\n","            return json.dumps(x, ensure_ascii=False)\n","\n","        if labels is None and self.mode == \"classification\":\n","            labels = self.available_labels()\n","\n","        this_dir, _ = os.path.split(__file__)\n","        bundle = open(os.path.join(this_dir, 'bundle.js'),\n","                      encoding=\"utf8\").read()\n","\n","        out = u'''<html>\n","        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF8\">\n","        <head><script>%s </script></head><body>''' % bundle\n","        random_id = id_generator(size=15, random_state=check_random_state(self.random_state))\n","        out += u'''\n","        <div class=\"lime top_div\" id=\"top_div%s\"></div>\n","        ''' % random_id\n","\n","        predict_proba_js = ''\n","        if self.mode == \"classification\" and predict_proba:\n","            predict_proba_js = u'''\n","            var pp_div = top_div.append('div')\n","                                .classed('lime predict_proba', true);\n","            var pp_svg = pp_div.append('svg').style('width', '100%%');\n","            var pp = new lime.PredictProba(pp_svg, %s, %s);\n","            ''' % (jsonize([str(x) for x in self.class_names]),\n","                   jsonize(list(self.predict_proba.astype(float))))\n","\n","        predict_value_js = ''\n","        if self.mode == \"regression\" and show_predicted_value:\n","            # reference self.predicted_value\n","            # (svg, predicted_value, min_value, max_value)\n","            predict_value_js = u'''\n","                    var pp_div = top_div.append('div')\n","                                        .classed('lime predicted_value', true);\n","                    var pp_svg = pp_div.append('svg').style('width', '100%%');\n","                    var pp = new lime.PredictedValue(pp_svg, %s, %s, %s);\n","                    ''' % (jsonize(float(self.predicted_value)),\n","                           jsonize(float(self.min_value)),\n","                           jsonize(float(self.max_value)))\n","\n","        exp_js = '''var exp_div;\n","            var exp = new lime.Explanation(%s);\n","        ''' % (jsonize([str(x) for x in self.class_names]))\n","\n","        if self.mode == \"classification\":\n","            for label in labels:\n","                exp = jsonize(self.as_list(label))\n","                exp_js += u'''\n","                exp_div = top_div.append('div').classed('lime explanation', true);\n","                exp.show(%s, %d, exp_div);\n","                ''' % (exp, label)\n","        else:\n","            exp = jsonize(self.as_list())\n","            exp_js += u'''\n","            exp_div = top_div.append('div').classed('lime explanation', true);\n","            exp.show(%s, %s, exp_div);\n","            ''' % (exp, self.dummy_label)\n","\n","        raw_js = '''var raw_div = top_div.append('div');'''\n","\n","        if self.mode == \"classification\":\n","            html_data = self.local_exp[labels[0]]\n","        else:\n","            html_data = self.local_exp[self.dummy_label]\n","\n","        raw_js += self.domain_mapper.visualize_instance_html(\n","                html_data,\n","                labels[0] if self.mode == \"classification\" else self.dummy_label,\n","                'raw_div',\n","                'exp',\n","                **kwargs)\n","        out += u'''\n","        <script>\n","        var top_div = d3.select('#top_div%s').classed('lime top_div', true);\n","        %s\n","        %s\n","        %s\n","        %s\n","        </script>\n","        ''' % (random_id, predict_proba_js, predict_value_js, exp_js, raw_js)\n","        out += u'</body></html>'\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"ECCtBe9ZO8fM"},"source":["#### LIME BASE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBlipqEsFIT1"},"outputs":[],"source":["\"\"\"\n","Contains abstract functionality for learning locally linear sparse model.\n","\"\"\"\n","import numpy as np\n","import scipy as sp\n","from sklearn.linear_model import Ridge, lars_path\n","from sklearn.utils import check_random_state\n","\n","\n","class LimeBase(object):\n","    \"\"\"Class for learning a locally linear sparse model from perturbed data\"\"\"\n","    def __init__(self,\n","                 kernel_fn,\n","                 verbose=False,\n","                 random_state=None):\n","        \"\"\"Init function\n","        Args:\n","            kernel_fn: function that transforms an array of distances into an\n","                        array of proximity values (floats).\n","            verbose: if true, print local prediction values from linear model.\n","            random_state: an integer or numpy.RandomState that will be used to\n","                generate random numbers. If None, the random state will be\n","                initialized using the internal numpy seed.\n","        \"\"\"\n","        self.kernel_fn = kernel_fn\n","        self.verbose = verbose\n","        self.random_state = check_random_state(random_state)\n","\n","    @staticmethod\n","    def generate_lars_path(weighted_data, weighted_labels):\n","        \"\"\"Generates the lars path for weighted data.\n","        Args:\n","            weighted_data: data that has been weighted by kernel\n","            weighted_label: labels, weighted by kernel\n","        Returns:\n","            (alphas, coefs), both are arrays corresponding to the\n","            regularization parameter and coefficients, respectively\n","        \"\"\"\n","        x_vector = weighted_data\n","        alphas, _, coefs = lars_path(x_vector,\n","                                     weighted_labels,\n","                                     method='lasso',\n","                                     verbose=False)\n","        return alphas, coefs\n","\n","    def forward_selection(self, data, labels, weights, num_features):\n","        \"\"\"Iteratively adds features to the model\"\"\"\n","        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n","        used_features = []\n","        for _ in range(min(num_features, data.shape[1])):\n","            max_ = -100000000\n","            best = 0\n","            for feature in range(data.shape[1]):\n","                if feature in used_features:\n","                    continue\n","                clf.fit(data[:, used_features + [feature]], labels,\n","                        sample_weight=weights)\n","                score = clf.score(data[:, used_features + [feature]],\n","                                  labels,\n","                                  sample_weight=weights)\n","                if score > max_:\n","                    best = feature\n","                    max_ = score\n","            used_features.append(best)\n","        return np.array(used_features)\n","\n","    def feature_selection(self, data, labels, weights, num_features, method):\n","        \"\"\"Selects features for the model. see explain_instance_with_data to\n","           understand the parameters.\"\"\"\n","        if method == 'none':\n","            return np.array(range(data.shape[1]))\n","        elif method == 'forward_selection':\n","            return self.forward_selection(data, labels, weights, num_features)\n","        elif method == 'highest_weights':\n","            clf = Ridge(alpha=0.01, fit_intercept=True,\n","                        random_state=self.random_state)\n","            clf.fit(data, labels, sample_weight=weights)\n","\n","            coef = clf.coef_\n","            if sp.sparse.issparse(data):\n","                coef = sp.sparse.csr_matrix(clf.coef_)\n","                weighted_data = coef.multiply(data[0])\n","                # Note: most efficient to slice the data before reversing\n","                sdata = len(weighted_data.data)\n","                argsort_data = np.abs(weighted_data.data).argsort()\n","                # Edge case where data is more sparse than requested number of feature importances\n","                # In that case, we just pad with zero-valued features\n","                if sdata < num_features:\n","                    nnz_indexes = argsort_data[::-1]\n","                    indices = weighted_data.indices[nnz_indexes]\n","                    num_to_pad = num_features - sdata\n","                    indices = np.concatenate((indices, np.zeros(num_to_pad, dtype=indices.dtype)))\n","                    indices_set = set(indices)\n","                    pad_counter = 0\n","                    for i in range(data.shape[1]):\n","                        if i not in indices_set:\n","                            indices[pad_counter + sdata] = i\n","                            pad_counter += 1\n","                            if pad_counter >= num_to_pad:\n","                                break\n","                else:\n","                    nnz_indexes = argsort_data[sdata - num_features:sdata][::-1]\n","                    indices = weighted_data.indices[nnz_indexes]\n","                return indices\n","            else:\n","                weighted_data = coef * data[0]\n","                feature_weights = sorted(\n","                    zip(range(data.shape[1]), weighted_data),\n","                    key=lambda x: np.abs(x[1]),\n","                    reverse=True)\n","                return np.array([x[0] for x in feature_weights[:num_features]])\n","        elif method == 'lasso_path':\n","            weighted_data = ((data - np.average(data, axis=0, weights=weights))\n","                             * np.sqrt(weights[:, np.newaxis]))\n","            weighted_labels = ((labels - np.average(labels, weights=weights))\n","                               * np.sqrt(weights))\n","            nonzero = range(weighted_data.shape[1])\n","            _, coefs = self.generate_lars_path(weighted_data,\n","                                               weighted_labels)\n","            for i in range(len(coefs.T) - 1, 0, -1):\n","                nonzero = coefs.T[i].nonzero()[0]\n","                if len(nonzero) <= num_features:\n","                    break\n","            used_features = nonzero\n","            return used_features\n","        elif method == 'auto':\n","            if num_features <= 6:\n","                n_method = 'forward_selection'\n","            else:\n","                n_method = 'highest_weights'\n","            return self.feature_selection(data, labels, weights,\n","                                          num_features, n_method)\n","\n","    def explain_instance_with_data(self,\n","                                   neighborhood_data,\n","                                   neighborhood_labels,\n","                                   distances,\n","                                   label,\n","                                   num_features,\n","                                   feature_selection='auto',\n","                                   model_regressor=None):\n","        \"\"\"Takes perturbed data, labels and distances, returns explanation.\n","        Args:\n","            neighborhood_data: perturbed data, 2d array. first element is\n","                               assumed to be the original data point.\n","            neighborhood_labels: corresponding perturbed labels. should have as\n","                                 many columns as the number of possible labels.\n","            distances: distances to original data point.\n","            label: label for which we want an explanation\n","            num_features: maximum number of features in explanation\n","            feature_selection: how to select num_features. options are:\n","                'forward_selection': iteratively add features to the model.\n","                    This is costly when num_features is high\n","                'highest_weights': selects the features that have the highest\n","                    product of absolute weight * original data point when\n","                    learning with all the features\n","                'lasso_path': chooses features based on the lasso\n","                    regularization path\n","                'none': uses all features, ignores num_features\n","                'auto': uses forward_selection if num_features <= 6, and\n","                    'highest_weights' otherwise.\n","            model_regressor: sklearn regressor to use in explanation.\n","                Defaults to Ridge regression if None. Must have\n","                model_regressor.coef_ and 'sample_weight' as a parameter\n","                to model_regressor.fit()\n","        Returns:\n","            (intercept, exp, score, local_pred):\n","            intercept is a float.\n","            exp is a sorted list of tuples, where each tuple (x,y) corresponds\n","            to the feature id (x) and the local weight (y). The list is sorted\n","            by decreasing absolute value of y.\n","            score is the R^2 value of the returned explanation\n","            local_pred is the prediction of the explanation model on the original instance\n","        \"\"\"\n","\n","        weights = self.kernel_fn(distances)\n","        labels_column = neighborhood_labels[:, label]\n","        used_features = self.feature_selection(neighborhood_data,\n","                                               labels_column,\n","                                               weights,\n","                                               num_features,\n","                                               feature_selection)\n","        if model_regressor is None:\n","            model_regressor = Ridge(alpha=1, fit_intercept=True,\n","                                    random_state=self.random_state)\n","        easy_model = model_regressor\n","        easy_model.fit(neighborhood_data[:, used_features],\n","                       labels_column, sample_weight=weights)\n","        prediction_score = easy_model.score(\n","            neighborhood_data[:, used_features],\n","            labels_column, sample_weight=weights)\n","\n","        local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n","\n","        if self.verbose:\n","            print('Intercept', easy_model.intercept_)\n","            print('Prediction_local', local_pred,)\n","            print('Right:', neighborhood_labels[0, label])\n","        return (easy_model.intercept_,\n","                sorted(zip(used_features, easy_model.coef_),\n","                       key=lambda x: np.abs(x[1]), reverse=True),\n","                prediction_score, local_pred)"]},{"cell_type":"markdown","metadata":{"id":"tgoTTm5MO-lx"},"source":["#### Discretizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nSfufT_xFj_r"},"outputs":[],"source":["\"\"\"\n","Discretizers classes, to be used in lime_tabular\n","\"\"\"\n","import numpy as np\n","import sklearn\n","import sklearn.tree\n","import scipy\n","from sklearn.utils import check_random_state\n","from abc import ABCMeta, abstractmethod\n","\n","\n","class BaseDiscretizer():\n","    \"\"\"\n","    Abstract class - Build a class that inherits from this class to implement\n","    a custom discretizer.\n","    Method bins() is to be redefined in the child class, as it is the actual\n","    custom part of the discretizer.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta  # abstract class\n","\n","    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None,\n","                 data_stats=None):\n","        \"\"\"Initializer\n","        Args:\n","            data: numpy 2d array\n","            categorical_features: list of indices (ints) corresponding to the\n","                categorical columns. These features will not be discretized.\n","                Everything else will be considered continuous, and will be\n","                discretized.\n","            categorical_names: map from int to list of names, where\n","                categorical_names[x][y] represents the name of the yth value of\n","                column x.\n","            feature_names: list of names (strings) corresponding to the columns\n","                in the training data.\n","            data_stats: must have 'means', 'stds', 'mins' and 'maxs', use this\n","                if you don't want these values to be computed from data\n","        \"\"\"\n","        self.to_discretize = ([x for x in range(data.shape[1])\n","                               if x not in categorical_features])\n","        self.data_stats = data_stats\n","        self.names = {}\n","        self.lambdas = {}\n","        self.means = {}\n","        self.stds = {}\n","        self.mins = {}\n","        self.maxs = {}\n","        self.random_state = check_random_state(random_state)\n","\n","        # To override when implementing a custom binning\n","        bins = self.bins(data, labels)\n","        bins = [np.unique(x) for x in bins]\n","\n","        # Read the stats from data_stats if exists\n","        if data_stats:\n","            self.means = self.data_stats.get(\"means\")\n","            self.stds = self.data_stats.get(\"stds\")\n","            self.mins = self.data_stats.get(\"mins\")\n","            self.maxs = self.data_stats.get(\"maxs\")\n","\n","        for feature, qts in zip(self.to_discretize, bins):\n","            n_bins = qts.shape[0]  # Actually number of borders (= #bins-1)\n","            boundaries = np.min(data[:, feature]), np.max(data[:, feature])\n","            name = feature_names[feature]\n","\n","            self.names[feature] = ['%s <= %.2f' % (name, qts[0])]\n","            for i in range(n_bins - 1):\n","                self.names[feature].append('%.2f < %s <= %.2f' %\n","                                           (qts[i], name, qts[i + 1]))\n","            self.names[feature].append('%s > %.2f' % (name, qts[n_bins - 1]))\n","\n","            self.lambdas[feature] = lambda x, qts=qts: np.searchsorted(qts, x)\n","            discretized = self.lambdas[feature](data[:, feature])\n","\n","            # If data stats are provided no need to compute the below set of details\n","            if data_stats:\n","                continue\n","\n","            self.means[feature] = []\n","            self.stds[feature] = []\n","            for x in range(n_bins + 1):\n","                selection = data[discretized == x, feature]\n","                mean = 0 if len(selection) == 0 else np.mean(selection)\n","                self.means[feature].append(mean)\n","                std = 0 if len(selection) == 0 else np.std(selection)\n","                std += 0.00000000001\n","                self.stds[feature].append(std)\n","            self.mins[feature] = [boundaries[0]] + qts.tolist()\n","            self.maxs[feature] = qts.tolist() + [boundaries[1]]\n","\n","    @abstractmethod\n","    def bins(self, data, labels):\n","        \"\"\"\n","        To be overridden\n","        Returns for each feature to discretize the boundaries\n","        that form each bin of the discretizer\n","        \"\"\"\n","        raise NotImplementedError(\"Must override bins() method\")\n","\n","    def discretize(self, data):\n","        \"\"\"Discretizes the data.\n","        Args:\n","            data: numpy 2d or 1d array\n","        Returns:\n","            numpy array of same dimension, discretized.\n","        \"\"\"\n","        ret = data.copy()\n","        for feature in self.lambdas:\n","            if len(data.shape) == 1:\n","                ret[feature] = int(self.lambdas[feature](ret[feature]))\n","            else:\n","                ret[:, feature] = self.lambdas[feature](\n","                    ret[:, feature]).astype(int)\n","        return ret\n","\n","    def get_undiscretize_values(self, feature, values):\n","        mins = np.array(self.mins[feature])[values]\n","        maxs = np.array(self.maxs[feature])[values]\n","\n","        means = np.array(self.means[feature])[values]\n","        stds = np.array(self.stds[feature])[values]\n","        minz = (mins - means) / stds\n","        maxz = (maxs - means) / stds\n","        min_max_unequal = (minz != maxz)\n","\n","        ret = minz\n","        ret[np.where(min_max_unequal)] = scipy.stats.truncnorm.rvs(\n","            minz[min_max_unequal],\n","            maxz[min_max_unequal],\n","            loc=means[min_max_unequal],\n","            scale=stds[min_max_unequal],\n","            random_state=self.random_state\n","        )\n","        return ret\n","\n","    def undiscretize(self, data):\n","        ret = data.copy()\n","        for feature in self.means:\n","            if len(data.shape) == 1:\n","                ret[feature] = self.get_undiscretize_values(\n","                    feature, ret[feature].astype(int).reshape(-1, 1)\n","                )\n","            else:\n","                ret[:, feature] = self.get_undiscretize_values(\n","                    feature, ret[:, feature].astype(int)\n","                )\n","        return ret\n","\n","\n","class StatsDiscretizer(BaseDiscretizer):\n","    \"\"\"\n","        Class to be used to supply the data stats info when discretize_continuous is true\n","    \"\"\"\n","\n","    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None,\n","                 data_stats=None):\n","\n","        BaseDiscretizer.__init__(self, data, categorical_features,\n","                                 feature_names, labels=labels,\n","                                 random_state=random_state,\n","                                 data_stats=data_stats)\n","\n","    def bins(self, data, labels):\n","        bins_from_stats = self.data_stats.get(\"bins\")\n","        bins = []\n","        if bins_from_stats is not None:\n","            for feature in self.to_discretize:\n","                bins_from_stats_feature = bins_from_stats.get(feature)\n","                if bins_from_stats_feature is not None:\n","                    qts = np.array(bins_from_stats_feature)\n","                    bins.append(qts)\n","        return bins\n","\n","\n","class QuartileDiscretizer(BaseDiscretizer):\n","    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):\n","\n","        BaseDiscretizer.__init__(self, data, categorical_features,\n","                                 feature_names, labels=labels,\n","                                 random_state=random_state)\n","\n","    def bins(self, data, labels):\n","        bins = []\n","        for feature in self.to_discretize:\n","            qts = np.array(np.percentile(data[:, feature], [25, 50, 75]))\n","            bins.append(qts)\n","        return bins\n","\n","\n","class DecileDiscretizer(BaseDiscretizer):\n","    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):\n","        BaseDiscretizer.__init__(self, data, categorical_features,\n","                                 feature_names, labels=labels,\n","                                 random_state=random_state)\n","\n","    def bins(self, data, labels):\n","        bins = []\n","        for feature in self.to_discretize:\n","            qts = np.array(np.percentile(data[:, feature],\n","                                         [10, 20, 30, 40, 50, 60, 70, 80, 90]))\n","            bins.append(qts)\n","        return bins\n","\n","\n","class EntropyDiscretizer(BaseDiscretizer):\n","    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):\n","        if(labels is None):\n","            raise ValueError('Labels must be not None when using \\\n","                             EntropyDiscretizer')\n","        BaseDiscretizer.__init__(self, data, categorical_features,\n","                                 feature_names, labels=labels,\n","                                 random_state=random_state)\n","\n","    def bins(self, data, labels):\n","        bins = []\n","        for feature in self.to_discretize:\n","            # Entropy splitting / at most 8 bins so max_depth=3\n","            dt = sklearn.tree.DecisionTreeClassifier(criterion='entropy',\n","                                                     max_depth=3,\n","                                                     random_state=self.random_state)\n","            x = np.reshape(data[:, feature], (-1, 1))\n","            dt.fit(x, labels)\n","            qts = dt.tree_.threshold[np.where(dt.tree_.children_left > -1)]\n","\n","            if qts.shape[0] == 0:\n","                qts = np.array([np.median(data[:, feature])])\n","            else:\n","                qts = np.sort(qts)\n","\n","            bins.append(qts)\n","\n","        return bins"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5074,"status":"ok","timestamp":1659884683089,"user":{"displayName":"Luca Adorni","userId":"07135966571450304185"},"user_tz":300},"id":"TFct3SC9F3RI","outputId":"07d05163-015f-428c-956b-8b8f2b411d99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyDOE2\n","  Downloading pyDOE2-1.3.0.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE2) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE2) (1.7.3)\n","Building wheels for collected packages: pyDOE2\n","  Building wheel for pyDOE2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyDOE2: filename=pyDOE2-1.3.0-py3-none-any.whl size=25539 sha256=649b29bb0401a6e9f0a63d88bd5224ab746e6b65f4a1a275cb6e2fac4d51e28e\n","  Stored in directory: /root/.cache/pip/wheels/49/91/2d/d08e80806bf7756193541f6c03c0492af288fcd6158d3d0998\n","Successfully built pyDOE2\n","Installing collected packages: pyDOE2\n","Successfully installed pyDOE2-1.3.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install pyDOE2"]},{"cell_type":"markdown","metadata":{"id":"XF0Tlx99PBcI"},"source":["#### Tabular LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcQSkhF_FVa2"},"outputs":[],"source":["\"\"\"\n","Functions for explaining classifiers that use tabular data (matrices).\n","\"\"\"\n","import collections\n","import copy\n","from functools import partial\n","import json\n","import warnings\n","\n","import numpy as np\n","import scipy as sp\n","import sklearn\n","import sklearn.preprocessing\n","from sklearn.utils import check_random_state\n","from pyDOE2 import lhs\n","from scipy.stats.distributions import norm\n","\n","\n","\n","class TableDomainMapper(DomainMapper):\n","    \"\"\"Maps feature ids to names, generates table views, etc\"\"\"\n","\n","    def __init__(self, feature_names, feature_values, scaled_row,\n","                 categorical_features, discretized_feature_names=None,\n","                 feature_indexes=None):\n","        \"\"\"Init.\n","        Args:\n","            feature_names: list of feature names, in order\n","            feature_values: list of strings with the values of the original row\n","            scaled_row: scaled row\n","            categorical_features: list of categorical features ids (ints)\n","            feature_indexes: optional feature indexes used in the sparse case\n","        \"\"\"\n","        self.exp_feature_names = feature_names\n","        self.discretized_feature_names = discretized_feature_names\n","        self.feature_names = feature_names\n","        self.feature_values = feature_values\n","        self.feature_indexes = feature_indexes\n","        self.scaled_row = scaled_row\n","        if sp.sparse.issparse(scaled_row):\n","            self.all_categorical = False\n","        else:\n","            self.all_categorical = len(categorical_features) == len(scaled_row)\n","        self.categorical_features = categorical_features\n","\n","    def map_exp_ids(self, exp):\n","        \"\"\"Maps ids to feature names.\n","        Args:\n","            exp: list of tuples [(id, weight), (id,weight)]\n","        Returns:\n","            list of tuples (feature_name, weight)\n","        \"\"\"\n","        names = self.exp_feature_names\n","        if self.discretized_feature_names is not None:\n","            names = self.discretized_feature_names\n","        return [(names[x[0]], x[1]) for x in exp]\n","\n","    def visualize_instance_html(self,\n","                                exp,\n","                                label,\n","                                div_name,\n","                                exp_object_name,\n","                                show_table=True,\n","                                show_all=False):\n","        \"\"\"Shows the current example in a table format.\n","        Args:\n","             exp: list of tuples [(id, weight), (id,weight)]\n","             label: label id (integer)\n","             div_name: name of div object to be used for rendering(in js)\n","             exp_object_name: name of js explanation object\n","             show_table: if False, don't show table visualization.\n","             show_all: if True, show zero-weighted features in the table.\n","        \"\"\"\n","        if not show_table:\n","            return ''\n","        weights = [0] * len(self.feature_names)\n","        for x in exp:\n","            weights[x[0]] = x[1]\n","        if self.feature_indexes is not None:\n","            # Sparse case: only display the non-zero values and importances\n","            fnames = [self.exp_feature_names[i] for i in self.feature_indexes]\n","            fweights = [weights[i] for i in self.feature_indexes]\n","            if show_all:\n","                out_list = list(zip(fnames,\n","                                    self.feature_values,\n","                                    fweights))\n","            else:\n","                out_dict = dict(map(lambda x: (x[0], (x[1], x[2], x[3])),\n","                                zip(self.feature_indexes,\n","                                    fnames,\n","                                    self.feature_values,\n","                                    fweights)))\n","                out_list = [out_dict.get(x[0], (str(x[0]), 0.0, 0.0)) for x in exp]\n","        else:\n","            out_list = list(zip(self.exp_feature_names,\n","                                self.feature_values,\n","                                weights))\n","            if not show_all:\n","                out_list = [out_list[x[0]] for x in exp]\n","        ret = u'''\n","            %s.show_raw_tabular(%s, %d, %s);\n","        ''' % (exp_object_name, json.dumps(out_list, ensure_ascii=False), label, div_name)\n","        return ret\n","\n","\n","class LimeTabularExplainer(object):\n","    \"\"\"Explains predictions on tabular (i.e. matrix) data.\n","    For numerical features, perturb them by sampling from a Normal(0,1) and\n","    doing the inverse operation of mean-centering and scaling, according to the\n","    means and stds in the training data. For categorical features, perturb by\n","    sampling according to the training distribution, and making a binary\n","    feature that is 1 when the value is the same as the instance being\n","    explained.\"\"\"\n","\n","    def __init__(self,\n","                 training_data,\n","                 mode=\"classification\",\n","                 training_labels=None,\n","                 feature_names=None,\n","                 categorical_features=None,\n","                 categorical_names=None,\n","                 kernel_width=None,\n","                 kernel=None,\n","                 verbose=False,\n","                 class_names=None,\n","                 feature_selection='auto',\n","                 discretize_continuous=True,\n","                 discretizer='quartile',\n","                 sample_around_instance=False,\n","                 random_state=None,\n","                 training_data_stats=None):\n","        \"\"\"Init function.\n","        Args:\n","            training_data: numpy 2d array\n","            mode: \"classification\" or \"regression\"\n","            training_labels: labels for training data. Not required, but may be\n","                used by discretizer.\n","            feature_names: list of names (strings) corresponding to the columns\n","                in the training data.\n","            categorical_features: list of indices (ints) corresponding to the\n","                categorical columns. Everything else will be considered\n","                continuous. Values in these columns MUST be integers.\n","            categorical_names: map from int to list of names, where\n","                categorical_names[x][y] represents the name of the yth value of\n","                column x.\n","            kernel_width: kernel width for the exponential kernel.\n","                If None, defaults to sqrt (number of columns) * 0.75\n","            kernel: similarity kernel that takes euclidean distances and kernel\n","                width as input and outputs weights in (0,1). If None, defaults to\n","                an exponential kernel.\n","            verbose: if true, print local prediction values from linear model\n","            class_names: list of class names, ordered according to whatever the\n","                classifier is using. If not present, class names will be '0',\n","                '1', ...\n","            feature_selection: feature selection method. can be\n","                'forward_selection', 'lasso_path', 'none' or 'auto'.\n","                See function 'explain_instance_with_data' in lime_base.py for\n","                details on what each of the options does.\n","            discretize_continuous: if True, all non-categorical features will\n","                be discretized into quartiles.\n","            discretizer: only matters if discretize_continuous is True\n","                and data is not sparse. Options are 'quartile', 'decile',\n","                'entropy' or a BaseDiscretizer instance.\n","            sample_around_instance: if True, will sample continuous features\n","                in perturbed samples from a normal centered at the instance\n","                being explained. Otherwise, the normal is centered on the mean\n","                of the feature data.\n","            random_state: an integer or numpy.RandomState that will be used to\n","                generate random numbers. If None, the random state will be\n","                initialized using the internal numpy seed.\n","            training_data_stats: a dict object having the details of training data\n","                statistics. If None, training data information will be used, only matters\n","                if discretize_continuous is True. Must have the following keys:\n","                means\", \"mins\", \"maxs\", \"stds\", \"feature_values\",\n","                \"feature_frequencies\"\n","        \"\"\"\n","        self.random_state = check_random_state(random_state)\n","        self.mode = mode\n","        self.categorical_names = categorical_names or {}\n","        self.sample_around_instance = sample_around_instance\n","        self.training_data_stats = training_data_stats\n","\n","        # Check and raise proper error in stats are supplied in non-descritized path\n","        if self.training_data_stats:\n","            self.validate_training_data_stats(self.training_data_stats)\n","\n","        if categorical_features is None:\n","            categorical_features = []\n","        if feature_names is None:\n","            feature_names = [str(i) for i in range(training_data.shape[1])]\n","\n","        self.categorical_features = list(categorical_features)\n","        self.feature_names = list(feature_names)\n","\n","        self.discretizer = None\n","        if discretize_continuous and not sp.sparse.issparse(training_data):\n","            # Set the discretizer if training data stats are provided\n","            if self.training_data_stats:\n","                discretizer = StatsDiscretizer(\n","                    training_data, self.categorical_features,\n","                    self.feature_names, labels=training_labels,\n","                    data_stats=self.training_data_stats,\n","                    random_state=self.random_state)\n","\n","            if discretizer == 'quartile':\n","                self.discretizer = QuartileDiscretizer(\n","                        training_data, self.categorical_features,\n","                        self.feature_names, labels=training_labels,\n","                        random_state=self.random_state)\n","            elif discretizer == 'decile':\n","                self.discretizer = DecileDiscretizer(\n","                        training_data, self.categorical_features,\n","                        self.feature_names, labels=training_labels,\n","                        random_state=self.random_state)\n","            elif discretizer == 'entropy':\n","                self.discretizer = EntropyDiscretizer(\n","                        training_data, self.categorical_features,\n","                        self.feature_names, labels=training_labels,\n","                        random_state=self.random_state)\n","            elif isinstance(discretizer, BaseDiscretizer):\n","                self.discretizer = discretizer\n","            else:\n","                raise ValueError('''Discretizer must be 'quartile',''' +\n","                                 ''' 'decile', 'entropy' or a''' +\n","                                 ''' BaseDiscretizer instance''')\n","            self.categorical_features = list(range(training_data.shape[1]))\n","\n","            # Get the discretized_training_data when the stats are not provided\n","            if(self.training_data_stats is None):\n","                discretized_training_data = self.discretizer.discretize(\n","                    training_data)\n","\n","        if kernel_width is None:\n","            kernel_width = np.sqrt(training_data.shape[1]) * .75\n","        kernel_width = float(kernel_width)\n","\n","        if kernel is None:\n","            def kernel(d, kernel_width):\n","                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n","\n","        kernel_fn = partial(kernel, kernel_width=kernel_width)\n","\n","        self.feature_selection = feature_selection\n","        self.base = LimeBase(kernel_fn, verbose, random_state=self.random_state)\n","        self.class_names = class_names\n","\n","        # Though set has no role to play if training data stats are provided\n","        self.scaler = sklearn.preprocessing.StandardScaler(with_mean=False)\n","        self.scaler.fit(training_data)\n","        self.feature_values = {}\n","        self.feature_frequencies = {}\n","\n","        for feature in self.categorical_features:\n","            if training_data_stats is None:\n","                if self.discretizer is not None:\n","                    column = discretized_training_data[:, feature]\n","                else:\n","                    column = training_data[:, feature]\n","\n","                feature_count = collections.Counter(column)\n","                values, frequencies = map(list, zip(*(sorted(feature_count.items()))))\n","            else:\n","                values = training_data_stats[\"feature_values\"][feature]\n","                frequencies = training_data_stats[\"feature_frequencies\"][feature]\n","\n","            self.feature_values[feature] = values\n","            self.feature_frequencies[feature] = (np.array(frequencies) /\n","                                                 float(sum(frequencies)))\n","            self.scaler.mean_[feature] = 0\n","            self.scaler.scale_[feature] = 1\n","\n","    @staticmethod\n","    def convert_and_round(values):\n","        return ['%.2f' % v for v in values]\n","\n","    @staticmethod\n","    def validate_training_data_stats(training_data_stats):\n","        \"\"\"\n","            Method to validate the structure of training data stats\n","        \"\"\"\n","        stat_keys = list(training_data_stats.keys())\n","        valid_stat_keys = [\"means\", \"mins\", \"maxs\", \"stds\", \"feature_values\", \"feature_frequencies\"]\n","        missing_keys = list(set(valid_stat_keys) - set(stat_keys))\n","        if len(missing_keys) > 0:\n","            raise Exception(\"Missing keys in training_data_stats. Details: %s\" % (missing_keys))\n","\n","    def explain_instance(self,\n","                         data_row,\n","                         predict_fn,\n","                         labels=(1,),\n","                         top_labels=None,\n","                         num_features=10,\n","                         num_samples=5000,\n","                         distance_metric='euclidean',\n","                         model_regressor=None,\n","                         sampling_method='gaussian'):\n","        \"\"\"Generates explanations for a prediction.\n","        First, we generate neighborhood data by randomly perturbing features\n","        from the instance (see __data_inverse). We then learn locally weighted\n","        linear models on this neighborhood data to explain each of the classes\n","        in an interpretable way (see lime_base.py).\n","        Args:\n","            data_row: 1d numpy array or scipy.sparse matrix, corresponding to a row\n","            predict_fn: prediction function. For classifiers, this should be a\n","                function that takes a numpy array and outputs prediction\n","                probabilities. For regressors, this takes a numpy array and\n","                returns the predictions. For ScikitClassifiers, this is\n","                `classifier.predict_proba()`. For ScikitRegressors, this\n","                is `regressor.predict()`. The prediction function needs to work\n","                on multiple feature vectors (the vectors randomly perturbed\n","                from the data_row).\n","            labels: iterable with labels to be explained.\n","            top_labels: if not None, ignore labels and produce explanations for\n","                the K labels with highest prediction probabilities, where K is\n","                this parameter.\n","            num_features: maximum number of features present in explanation\n","            num_samples: size of the neighborhood to learn the linear model\n","            distance_metric: the distance metric to use for weights.\n","            model_regressor: sklearn regressor to use in explanation. Defaults\n","                to Ridge regression in LimeBase. Must have model_regressor.coef_\n","                and 'sample_weight' as a parameter to model_regressor.fit()\n","            sampling_method: Method to sample synthetic data. Defaults to Gaussian\n","                sampling. Can also use Latin Hypercube Sampling.\n","        Returns:\n","            An Explanation object (see explanation.py) with the corresponding\n","            explanations.\n","        \"\"\"\n","        if sp.sparse.issparse(data_row) and not sp.sparse.isspmatrix_csr(data_row):\n","            # Preventative code: if sparse, convert to csr format if not in csr format already\n","            data_row = data_row.tocsr()\n","        data, inverse = self.__data_inverse(data_row, num_samples, sampling_method)\n","        if sp.sparse.issparse(data):\n","            # Note in sparse case we don't subtract mean since data would become dense\n","            scaled_data = data.multiply(self.scaler.scale_)\n","            # Multiplying with csr matrix can return a coo sparse matrix\n","            if not sp.sparse.isspmatrix_csr(scaled_data):\n","                scaled_data = scaled_data.tocsr()\n","        else:\n","            scaled_data = (data - self.scaler.mean_) / self.scaler.scale_\n","        distances = sklearn.metrics.pairwise_distances(\n","                scaled_data,\n","                scaled_data[0].reshape(1, -1),\n","                metric=distance_metric\n","        ).ravel()\n","\n","        yss = predict_fn(inverse)\n","\n","        # for classification, the model needs to provide a list of tuples - classes\n","        # along with prediction probabilities\n","        if self.mode == \"classification\":\n","            if len(yss.shape) == 1:\n","                raise NotImplementedError(\"LIME does not currently support \"\n","                                          \"classifier models without probability \"\n","                                          \"scores. If this conflicts with your \"\n","                                          \"use case, please let us know: \"\n","                                          \"https://github.com/datascienceinc/lime/issues/16\")\n","            elif len(yss.shape) == 2:\n","                if self.class_names is None:\n","                    self.class_names = [str(x) for x in range(yss[0].shape[0])]\n","                else:\n","                    self.class_names = list(self.class_names)\n","                if not np.allclose(yss.sum(axis=1), 1.0):\n","                    warnings.warn(\"\"\"\n","                    Prediction probabilties do not sum to 1, and\n","                    thus does not constitute a probability space.\n","                    Check that you classifier outputs probabilities\n","                    (Not log probabilities, or actual class predictions).\n","                    \"\"\")\n","            else:\n","                raise ValueError(\"Your model outputs \"\n","                                 \"arrays with {} dimensions\".format(len(yss.shape)))\n","\n","        # for regression, the output should be a one-dimensional array of predictions\n","        else:\n","            try:\n","                if len(yss.shape) != 1 and len(yss[0].shape) == 1:\n","                    yss = np.array([v[0] for v in yss])\n","                assert isinstance(yss, np.ndarray) and len(yss.shape) == 1\n","            except AssertionError:\n","                raise ValueError(\"Your model needs to output single-dimensional \\\n","                    numpyarrays, not arrays of {} dimensions\".format(yss.shape))\n","\n","            predicted_value = yss[0]\n","            min_y = min(yss)\n","            max_y = max(yss)\n","\n","            # add a dimension to be compatible with downstream machinery\n","            yss = yss[:, np.newaxis]\n","\n","        feature_names = copy.deepcopy(self.feature_names)\n","        if feature_names is None:\n","            feature_names = [str(x) for x in range(data_row.shape[0])]\n","\n","        if sp.sparse.issparse(data_row):\n","            values = self.convert_and_round(data_row.data)\n","            feature_indexes = data_row.indices\n","        else:\n","            values = self.convert_and_round(data_row)\n","            feature_indexes = None\n","\n","        for i in self.categorical_features:\n","            if self.discretizer is not None and i in self.discretizer.lambdas:\n","                continue\n","            name = int(data_row[i])\n","            if i in self.categorical_names:\n","                name = self.categorical_names[i][name]\n","            feature_names[i] = '%s=%s' % (feature_names[i], name)\n","            values[i] = 'True'\n","        categorical_features = self.categorical_features\n","\n","        discretized_feature_names = None\n","        if self.discretizer is not None:\n","            categorical_features = range(data.shape[1])\n","            discretized_instance = self.discretizer.discretize(data_row)\n","            discretized_feature_names = copy.deepcopy(feature_names)\n","            for f in self.discretizer.names:\n","                discretized_feature_names[f] = self.discretizer.names[f][int(\n","                        discretized_instance[f])]\n","\n","        domain_mapper = TableDomainMapper(feature_names,\n","                                          values,\n","                                          scaled_data[0],\n","                                          categorical_features=categorical_features,\n","                                          discretized_feature_names=discretized_feature_names,\n","                                          feature_indexes=feature_indexes)\n","        ret_exp = Explanation(domain_mapper,\n","                                          mode=self.mode,\n","                                          class_names=self.class_names)\n","        if self.mode == \"classification\":\n","            ret_exp.predict_proba = yss[0]\n","            if top_labels:\n","                labels = np.argsort(yss[0])[-top_labels:]\n","                ret_exp.top_labels = list(labels)\n","                ret_exp.top_labels.reverse()\n","        else:\n","            ret_exp.predicted_value = predicted_value\n","            ret_exp.min_value = min_y\n","            ret_exp.max_value = max_y\n","            labels = [0]\n","        for label in labels:\n","            (ret_exp.intercept[label],\n","             ret_exp.local_exp[label],\n","             ret_exp.score[label],\n","             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n","                    scaled_data,\n","                    yss,\n","                    distances,\n","                    label,\n","                    num_features,\n","                    model_regressor=model_regressor,\n","                    feature_selection=self.feature_selection)\n","\n","        if self.mode == \"regression\":\n","            ret_exp.intercept[1] = ret_exp.intercept[0]\n","            ret_exp.local_exp[1] = [x for x in ret_exp.local_exp[0]]\n","            ret_exp.local_exp[0] = [(i, -1 * j) for i, j in ret_exp.local_exp[1]]\n","\n","        return ret_exp\n","\n","    def __data_inverse(self,\n","                       data_row,\n","                       num_samples,\n","                       sampling_method):\n","        \"\"\"Generates a neighborhood around a prediction.\n","        For numerical features, perturb them by sampling from a Normal(0,1) and\n","        doing the inverse operation of mean-centering and scaling, according to\n","        the means and stds in the training data. For categorical features,\n","        perturb by sampling according to the training distribution, and making\n","        a binary feature that is 1 when the value is the same as the instance\n","        being explained.\n","        Args:\n","            data_row: 1d numpy array, corresponding to a row\n","            num_samples: size of the neighborhood to learn the linear model\n","            sampling_method: 'gaussian' or 'lhs'\n","        Returns:\n","            A tuple (data, inverse), where:\n","                data: dense num_samples * K matrix, where categorical features\n","                are encoded with either 0 (not equal to the corresponding value\n","                in data_row) or 1. The first row is the original instance.\n","                inverse: same as data, except the categorical features are not\n","                binary, but categorical (as the original data)\n","        \"\"\"\n","        is_sparse = sp.sparse.issparse(data_row)\n","        if is_sparse:\n","            num_cols = data_row.shape[1]\n","            data = sp.sparse.csr_matrix((num_samples, num_cols), dtype=data_row.dtype)\n","        else:\n","            num_cols = data_row.shape[0]\n","            data = np.zeros((num_samples, num_cols))\n","        categorical_features = range(num_cols)\n","        if self.discretizer is None:\n","            instance_sample = data_row\n","            scale = self.scaler.scale_\n","            mean = self.scaler.mean_\n","            if is_sparse:\n","                # Perturb only the non-zero values\n","                non_zero_indexes = data_row.nonzero()[1]\n","                num_cols = len(non_zero_indexes)\n","                instance_sample = data_row[:, non_zero_indexes]\n","                scale = scale[non_zero_indexes]\n","                mean = mean[non_zero_indexes]\n","\n","            if sampling_method == 'gaussian':\n","                data = self.random_state.normal(0, 1, num_samples * num_cols\n","                                                ).reshape(num_samples, num_cols)\n","                data = np.array(data)\n","            elif sampling_method == 'lhs':\n","                data = lhs(num_cols, samples=num_samples\n","                           ).reshape(num_samples, num_cols)\n","                means = np.zeros(num_cols)\n","                stdvs = np.array([1]*num_cols)\n","                for i in range(num_cols):\n","                    data[:, i] = norm(loc=means[i], scale=stdvs[i]).ppf(data[:, i])\n","                data = np.array(data)\n","            else:\n","                warnings.warn('''Invalid input for sampling_method.\n","                                 Defaulting to Gaussian sampling.''', UserWarning)\n","                data = self.random_state.normal(0, 1, num_samples * num_cols\n","                                                ).reshape(num_samples, num_cols)\n","                data = np.array(data)\n","\n","            if self.sample_around_instance:\n","                data = data * scale + instance_sample\n","            else:\n","                data = data * scale + mean\n","            if is_sparse:\n","                if num_cols == 0:\n","                    data = sp.sparse.csr_matrix((num_samples,\n","                                                 data_row.shape[1]),\n","                                                dtype=data_row.dtype)\n","                else:\n","                    indexes = np.tile(non_zero_indexes, num_samples)\n","                    indptr = np.array(\n","                        range(0, len(non_zero_indexes) * (num_samples + 1),\n","                              len(non_zero_indexes)))\n","                    data_1d_shape = data.shape[0] * data.shape[1]\n","                    data_1d = data.reshape(data_1d_shape)\n","                    data = sp.sparse.csr_matrix(\n","                        (data_1d, indexes, indptr),\n","                        shape=(num_samples, data_row.shape[1]))\n","            categorical_features = self.categorical_features\n","            first_row = data_row\n","        else:\n","            first_row = self.discretizer.discretize(data_row)\n","        data[0] = data_row.copy()\n","        inverse = data.copy()\n","        for column in categorical_features:\n","            values = self.feature_values[column]\n","            freqs = self.feature_frequencies[column]\n","            inverse_column = self.random_state.choice(values, size=num_samples,\n","                                                      replace=True, p=freqs)\n","            binary_column = (inverse_column == first_row[column]).astype(int)\n","            binary_column[0] = 1\n","            inverse_column[0] = data[0, column]\n","            data[:, column] = binary_column\n","            inverse[:, column] = inverse_column\n","        if self.discretizer is not None:\n","            inverse[1:] = self.discretizer.undiscretize(inverse[1:])\n","        inverse[0] = data_row\n","        return data, inverse\n","\n","\n","class RecurrentTabularExplainer(LimeTabularExplainer):\n","    \"\"\"\n","    An explainer for keras-style recurrent neural networks, where the\n","    input shape is (n_samples, n_timesteps, n_features). This class\n","    just extends the LimeTabularExplainer class and reshapes the training\n","    data and feature names such that they become something like\n","    (val1_t1, val1_t2, val1_t3, ..., val2_t1, ..., valn_tn)\n","    Each of the methods that take data reshape it appropriately,\n","    so you can pass in the training/testing data exactly as you\n","    would to the recurrent neural network.\n","    \"\"\"\n","\n","    def __init__(self, training_data, mode=\"classification\",\n","                 training_labels=None, feature_names=None,\n","                 categorical_features=None, categorical_names=None,\n","                 kernel_width=None, kernel=None, verbose=False, class_names=None,\n","                 feature_selection='auto', discretize_continuous=True,\n","                 discretizer='quartile', random_state=None):\n","        \"\"\"\n","        Args:\n","            training_data: numpy 3d array with shape\n","                (n_samples, n_timesteps, n_features)\n","            mode: \"classification\" or \"regression\"\n","            training_labels: labels for training data. Not required, but may be\n","                used by discretizer.\n","            feature_names: list of names (strings) corresponding to the columns\n","                in the training data.\n","            categorical_features: list of indices (ints) corresponding to the\n","                categorical columns. Everything else will be considered\n","                continuous. Values in these columns MUST be integers.\n","            categorical_names: map from int to list of names, where\n","                categorical_names[x][y] represents the name of the yth value of\n","                column x.\n","            kernel_width: kernel width for the exponential kernel.\n","            If None, defaults to sqrt(number of columns) * 0.75\n","            kernel: similarity kernel that takes euclidean distances and kernel\n","                width as input and outputs weights in (0,1). If None, defaults to\n","                an exponential kernel.\n","            verbose: if true, print local prediction values from linear model\n","            class_names: list of class names, ordered according to whatever the\n","                classifier is using. If not present, class names will be '0',\n","                '1', ...\n","            feature_selection: feature selection method. can be\n","                'forward_selection', 'lasso_path', 'none' or 'auto'.\n","                See function 'explain_instance_with_data' in lime_base.py for\n","                details on what each of the options does.\n","            discretize_continuous: if True, all non-categorical features will\n","                be discretized into quartiles.\n","            discretizer: only matters if discretize_continuous is True. Options\n","                are 'quartile', 'decile', 'entropy' or a BaseDiscretizer\n","                instance.\n","            random_state: an integer or numpy.RandomState that will be used to\n","                generate random numbers. If None, the random state will be\n","                initialized using the internal numpy seed.\n","        \"\"\"\n","\n","        # Reshape X\n","        n_samples, n_timesteps, n_features = training_data.shape\n","        training_data = np.transpose(training_data, axes=(0, 2, 1)).reshape(\n","                n_samples, n_timesteps * n_features)\n","        self.n_timesteps = n_timesteps\n","        self.n_features = n_features\n","        if feature_names is None:\n","            feature_names = ['feature%d' % i for i in range(n_features)]\n","\n","        # Update the feature names\n","        feature_names = ['{}_t-{}'.format(n, n_timesteps - (i + 1))\n","                         for n in feature_names for i in range(n_timesteps)]\n","\n","        # Send off the the super class to do its magic.\n","        super(RecurrentTabularExplainer, self).__init__(\n","                training_data,\n","                mode=mode,\n","                training_labels=training_labels,\n","                feature_names=feature_names,\n","                categorical_features=categorical_features,\n","                categorical_names=categorical_names,\n","                kernel_width=kernel_width,\n","                kernel=kernel,\n","                verbose=verbose,\n","                class_names=class_names,\n","                feature_selection=feature_selection,\n","                discretize_continuous=discretize_continuous,\n","                discretizer=discretizer,\n","                random_state=random_state)\n","\n","    def _make_predict_proba(self, func):\n","        \"\"\"\n","        The predict_proba method will expect 3d arrays, but we are reshaping\n","        them to 2D so that LIME works correctly. This wraps the function\n","        you give in explain_instance to first reshape the data to have\n","        the shape the the keras-style network expects.\n","        \"\"\"\n","\n","        def predict_proba(X):\n","            n_samples = X.shape[0]\n","            new_shape = (n_samples, self.n_features, self.n_timesteps)\n","            X = np.transpose(X.reshape(new_shape), axes=(0, 2, 1))\n","            return func(X)\n","\n","        return predict_proba\n","\n","    def explain_instance(self, data_row, classifier_fn, labels=(1,),\n","                         top_labels=None, num_features=10, num_samples=5000,\n","                         distance_metric='euclidean', model_regressor=None):\n","        \"\"\"Generates explanations for a prediction.\n","        First, we generate neighborhood data by randomly perturbing features\n","        from the instance (see __data_inverse). We then learn locally weighted\n","        linear models on this neighborhood data to explain each of the classes\n","        in an interpretable way (see lime_base.py).\n","        Args:\n","            data_row: 2d numpy array, corresponding to a row\n","            classifier_fn: classifier prediction probability function, which\n","                takes a numpy array and outputs prediction probabilities. For\n","                ScikitClassifiers , this is classifier.predict_proba.\n","            labels: iterable with labels to be explained.\n","            top_labels: if not None, ignore labels and produce explanations for\n","                the K labels with highest prediction probabilities, where K is\n","                this parameter.\n","            num_features: maximum number of features present in explanation\n","            num_samples: size of the neighborhood to learn the linear model\n","            distance_metric: the distance metric to use for weights.\n","            model_regressor: sklearn regressor to use in explanation. Defaults\n","                to Ridge regression in LimeBase. Must have\n","                model_regressor.coef_ and 'sample_weight' as a parameter\n","                to model_regressor.fit()\n","        Returns:\n","            An Explanation object (see explanation.py) with the corresponding\n","            explanations.\n","        \"\"\"\n","\n","        # Flatten input so that the normal explainer can handle it\n","        data_row = data_row.T.reshape(self.n_timesteps * self.n_features)\n","\n","        # Wrap the classifier to reshape input\n","        classifier_fn = self._make_predict_proba(classifier_fn)\n","        return super(RecurrentTabularExplainer, self).explain_instance(\n","            data_row, classifier_fn,\n","            labels=labels,\n","            top_labels=top_labels,\n","            num_features=num_features,\n","            num_samples=num_samples,\n","            distance_metric=distance_metric,\n","            model_regressor=model_regressor)"]},{"cell_type":"markdown","metadata":{"id":"BQCPukoWPFBn"},"source":["#### Text LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2ibvAQVFYxx"},"outputs":[],"source":["\"\"\"\n","Functions for explaining text classifiers.\n","\"\"\"\n","from functools import partial\n","import itertools\n","import json\n","import re\n","\n","import numpy as np\n","import scipy as sp\n","import sklearn\n","from sklearn.utils import check_random_state\n","\n","\n","class TextDomainMapper(DomainMapper):\n","    \"\"\"Maps feature ids to words or word-positions\"\"\"\n","\n","    def __init__(self, indexed_string):\n","        \"\"\"Initializer.\n","        Args:\n","            indexed_string: lime_text.IndexedString, original string\n","        \"\"\"\n","        self.indexed_string = indexed_string\n","\n","    def map_exp_ids(self, exp, positions=False):\n","        \"\"\"Maps ids to words or word-position strings.\n","        Args:\n","            exp: list of tuples [(id, weight), (id,weight)]\n","            positions: if True, also return word positions\n","        Returns:\n","            list of tuples (word, weight), or (word_positions, weight) if\n","            examples: ('bad', 1) or ('bad_3-6-12', 1)\n","        \"\"\"\n","        if positions:\n","            exp = [('%s_%s' % (\n","                self.indexed_string.word(x[0]),\n","                '-'.join(\n","                    map(str,\n","                        self.indexed_string.string_position(x[0])))), x[1])\n","                   for x in exp]\n","        else:\n","            exp = [(self.indexed_string.word(x[0]), x[1]) for x in exp]\n","        return exp\n","\n","    def visualize_instance_html(self, exp, label, div_name, exp_object_name,\n","                                text=True, opacity=True):\n","        \"\"\"Adds text with highlighted words to visualization.\n","        Args:\n","             exp: list of tuples [(id, weight), (id,weight)]\n","             label: label id (integer)\n","             div_name: name of div object to be used for rendering(in js)\n","             exp_object_name: name of js explanation object\n","             text: if False, return empty\n","             opacity: if True, fade colors according to weight\n","        \"\"\"\n","        if not text:\n","            return u''\n","        text = (self.indexed_string.raw_string()\n","                .encode('utf-8', 'xmlcharrefreplace').decode('utf-8'))\n","        text = re.sub(r'[<>&]', '|', text)\n","        exp = [(self.indexed_string.word(x[0]),\n","                self.indexed_string.string_position(x[0]),\n","                x[1]) for x in exp]\n","        all_occurrences = list(itertools.chain.from_iterable(\n","            [itertools.product([x[0]], x[1], [x[2]]) for x in exp]))\n","        all_occurrences = [(x[0], int(x[1]), x[2]) for x in all_occurrences]\n","        ret = '''\n","            %s.show_raw_text(%s, %d, %s, %s, %s);\n","            ''' % (exp_object_name, json.dumps(all_occurrences), label,\n","                   json.dumps(text), div_name, json.dumps(opacity))\n","        return ret\n","\n","\n","class IndexedString(object):\n","    \"\"\"String with various indexes.\"\"\"\n","\n","    def __init__(self, raw_string, split_expression=r'\\W+', bow=True,\n","                 mask_string=None):\n","        \"\"\"Initializer.\n","        Args:\n","            raw_string: string with raw text in it\n","            split_expression: Regex string or callable. If regex string, will be used with re.split.\n","                If callable, the function should return a list of tokens.\n","            bow: if True, a word is the same everywhere in the text - i.e. we\n","                 will index multiple occurrences of the same word. If False,\n","                 order matters, so that the same word will have different ids\n","                 according to position.\n","            mask_string: If not None, replace words with this if bow=False\n","                if None, default value is UNKWORDZ\n","        \"\"\"\n","        self.raw = raw_string\n","        self.mask_string = 'UNKWORDZ' if mask_string is None else mask_string\n","\n","        if callable(split_expression):\n","            tokens = split_expression(self.raw)\n","            self.as_list = self._segment_with_tokens(self.raw, tokens)\n","            tokens = set(tokens)\n","\n","            def non_word(string):\n","                return string not in tokens\n","\n","        else:\n","            # with the split_expression as a non-capturing group (?:), we don't need to filter out\n","            # the separator character from the split results.\n","            splitter = re.compile(r'(%s)|$' % split_expression)\n","            self.as_list = [s for s in splitter.split(self.raw) if s]\n","            non_word = splitter.match\n","\n","        self.as_np = np.array(self.as_list)\n","        self.string_start = np.hstack(\n","            ([0], np.cumsum([len(x) for x in self.as_np[:-1]])))\n","        vocab = {}\n","        self.inverse_vocab = []\n","        self.positions = []\n","        self.bow = bow\n","        non_vocab = set()\n","        for i, word in enumerate(self.as_np):\n","            if word in non_vocab:\n","                continue\n","            if non_word(word):\n","                non_vocab.add(word)\n","                continue\n","            if bow:\n","                if word not in vocab:\n","                    vocab[word] = len(vocab)\n","                    self.inverse_vocab.append(word)\n","                    self.positions.append([])\n","                idx_word = vocab[word]\n","                self.positions[idx_word].append(i)\n","            else:\n","                self.inverse_vocab.append(word)\n","                self.positions.append(i)\n","        if not bow:\n","            self.positions = np.array(self.positions)\n","\n","    def raw_string(self):\n","        \"\"\"Returns the original raw string\"\"\"\n","        return self.raw\n","\n","    def num_words(self):\n","        \"\"\"Returns the number of tokens in the vocabulary for this document.\"\"\"\n","        return len(self.inverse_vocab)\n","\n","    def word(self, id_):\n","        \"\"\"Returns the word that corresponds to id_ (int)\"\"\"\n","        return self.inverse_vocab[id_]\n","\n","    def string_position(self, id_):\n","        \"\"\"Returns a np array with indices to id_ (int) occurrences\"\"\"\n","        if self.bow:\n","            return self.string_start[self.positions[id_]]\n","        else:\n","            return self.string_start[[self.positions[id_]]]\n","\n","    def inverse_removing(self, words_to_remove):\n","        \"\"\"Returns a string after removing the appropriate words.\n","        If self.bow is false, replaces word with UNKWORDZ instead of removing\n","        it.\n","        Args:\n","            words_to_remove: list of ids (ints) to remove\n","        Returns:\n","            original raw string with appropriate words removed.\n","        \"\"\"\n","        mask = np.ones(self.as_np.shape[0], dtype='bool')\n","        mask[self.__get_idxs(words_to_remove)] = False\n","        if not self.bow:\n","            return ''.join(\n","                [self.as_list[i] if mask[i] else self.mask_string\n","                 for i in range(mask.shape[0])])\n","        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])\n","\n","    @staticmethod\n","    def _segment_with_tokens(text, tokens):\n","        \"\"\"Segment a string around the tokens created by a passed-in tokenizer\"\"\"\n","        list_form = []\n","        text_ptr = 0\n","        for token in tokens:\n","            inter_token_string = []\n","            while not text[text_ptr:].startswith(token):\n","                inter_token_string.append(text[text_ptr])\n","                text_ptr += 1\n","                if text_ptr >= len(text):\n","                    raise ValueError(\"Tokenization produced tokens that do not belong in string!\")\n","            text_ptr += len(token)\n","            if inter_token_string:\n","                list_form.append(''.join(inter_token_string))\n","            list_form.append(token)\n","        if text_ptr < len(text):\n","            list_form.append(text[text_ptr:])\n","        return list_form\n","\n","    def __get_idxs(self, words):\n","        \"\"\"Returns indexes to appropriate words.\"\"\"\n","        if self.bow:\n","            return list(itertools.chain.from_iterable(\n","                [self.positions[z] for z in words]))\n","        else:\n","            return self.positions[words]\n","\n","\n","class IndexedCharacters(object):\n","    \"\"\"String with various indexes.\"\"\"\n","\n","    def __init__(self, raw_string, bow=True, mask_string=None):\n","        \"\"\"Initializer.\n","        Args:\n","            raw_string: string with raw text in it\n","            bow: if True, a char is the same everywhere in the text - i.e. we\n","                 will index multiple occurrences of the same character. If False,\n","                 order matters, so that the same word will have different ids\n","                 according to position.\n","            mask_string: If not None, replace characters with this if bow=False\n","                if None, default value is chr(0)\n","        \"\"\"\n","        self.raw = raw_string\n","        self.as_list = list(self.raw)\n","        self.as_np = np.array(self.as_list)\n","        self.mask_string = chr(0) if mask_string is None else mask_string\n","        self.string_start = np.arange(len(self.raw))\n","        vocab = {}\n","        self.inverse_vocab = []\n","        self.positions = []\n","        self.bow = bow\n","        non_vocab = set()\n","        for i, char in enumerate(self.as_np):\n","            if char in non_vocab:\n","                continue\n","            if bow:\n","                if char not in vocab:\n","                    vocab[char] = len(vocab)\n","                    self.inverse_vocab.append(char)\n","                    self.positions.append([])\n","                idx_char = vocab[char]\n","                self.positions[idx_char].append(i)\n","            else:\n","                self.inverse_vocab.append(char)\n","                self.positions.append(i)\n","        if not bow:\n","            self.positions = np.array(self.positions)\n","\n","    def raw_string(self):\n","        \"\"\"Returns the original raw string\"\"\"\n","        return self.raw\n","\n","    def num_words(self):\n","        \"\"\"Returns the number of tokens in the vocabulary for this document.\"\"\"\n","        return len(self.inverse_vocab)\n","\n","    def word(self, id_):\n","        \"\"\"Returns the word that corresponds to id_ (int)\"\"\"\n","        return self.inverse_vocab[id_]\n","\n","    def string_position(self, id_):\n","        \"\"\"Returns a np array with indices to id_ (int) occurrences\"\"\"\n","        if self.bow:\n","            return self.string_start[self.positions[id_]]\n","        else:\n","            return self.string_start[[self.positions[id_]]]\n","\n","    def inverse_removing(self, words_to_remove):\n","        \"\"\"Returns a string after removing the appropriate words.\n","        If self.bow is false, replaces word with UNKWORDZ instead of removing\n","        it.\n","        Args:\n","            words_to_remove: list of ids (ints) to remove\n","        Returns:\n","            original raw string with appropriate words removed.\n","        \"\"\"\n","        mask = np.ones(self.as_np.shape[0], dtype='bool')\n","        mask[self.__get_idxs(words_to_remove)] = False\n","        if not self.bow:\n","            return ''.join(\n","                [self.as_list[i] if mask[i] else self.mask_string\n","                 for i in range(mask.shape[0])])\n","        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])\n","\n","    def __get_idxs(self, words):\n","        \"\"\"Returns indexes to appropriate words.\"\"\"\n","        if self.bow:\n","            return list(itertools.chain.from_iterable(\n","                [self.positions[z] for z in words]))\n","        else:\n","            return self.positions[words]\n","\n","\n","class LimeTextExplainer(object):\n","    \"\"\"Explains text classifiers.\n","       Currently, we are using an exponential kernel on cosine distance, and\n","       restricting explanations to words that are present in documents.\"\"\"\n","\n","    def __init__(self,\n","                 kernel_width=25,\n","                 kernel=None,\n","                 verbose=False,\n","                 class_names=None,\n","                 feature_selection='auto',\n","                 split_expression=r'\\W+',\n","                 bow=True,\n","                 mask_string=None,\n","                 random_state=None,\n","                 char_level=False):\n","        \"\"\"Init function.\n","        Args:\n","            kernel_width: kernel width for the exponential kernel.\n","            kernel: similarity kernel that takes euclidean distances and kernel\n","                width as input and outputs weights in (0,1). If None, defaults to\n","                an exponential kernel.\n","            verbose: if true, print local prediction values from linear model\n","            class_names: list of class names, ordered according to whatever the\n","                classifier is using. If not present, class names will be '0',\n","                '1', ...\n","            feature_selection: feature selection method. can be\n","                'forward_selection', 'lasso_path', 'none' or 'auto'.\n","                See function 'explain_instance_with_data' in lime_base.py for\n","                details on what each of the options does.\n","            split_expression: Regex string or callable. If regex string, will be used with re.split.\n","                If callable, the function should return a list of tokens.\n","            bow: if True (bag of words), will perturb input data by removing\n","                all occurrences of individual words or characters.\n","                Explanations will be in terms of these words. Otherwise, will\n","                explain in terms of word-positions, so that a word may be\n","                important the first time it appears and unimportant the second.\n","                Only set to false if the classifier uses word order in some way\n","                (bigrams, etc), or if you set char_level=True.\n","            mask_string: String used to mask tokens or characters if bow=False\n","                if None, will be 'UNKWORDZ' if char_level=False, chr(0)\n","                otherwise.\n","            random_state: an integer or numpy.RandomState that will be used to\n","                generate random numbers. If None, the random state will be\n","                initialized using the internal numpy seed.\n","            char_level: an boolean identifying that we treat each character\n","                as an independent occurence in the string\n","        \"\"\"\n","\n","        if kernel is None:\n","            def kernel(d, kernel_width):\n","                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n","\n","        kernel_fn = partial(kernel, kernel_width=kernel_width)\n","\n","        self.random_state = check_random_state(random_state)\n","        self.base = LimeBase(kernel_fn, verbose,\n","                                       random_state=self.random_state)\n","        self.class_names = class_names\n","        self.vocabulary = None\n","        self.feature_selection = feature_selection\n","        self.bow = bow\n","        self.mask_string = mask_string\n","        self.split_expression = split_expression\n","        self.char_level = char_level\n","\n","    def explain_instance(self,\n","                         text_instance,\n","                         classifier_fn,\n","                         labels=(1,),\n","                         top_labels=None,\n","                         num_features=10,\n","                         num_samples=5000,\n","                         distance_metric='cosine',\n","                         model_regressor=None):\n","        \"\"\"Generates explanations for a prediction.\n","        First, we generate neighborhood data by randomly hiding features from\n","        the instance (see __data_labels_distance_mapping). We then learn\n","        locally weighted linear models on this neighborhood data to explain\n","        each of the classes in an interpretable way (see lime_base.py).\n","        Args:\n","            text_instance: raw text string to be explained.\n","            classifier_fn: classifier prediction probability function, which\n","                takes a list of d strings and outputs a (d, k) numpy array with\n","                prediction probabilities, where k is the number of classes.\n","                For ScikitClassifiers , this is classifier.predict_proba.\n","            labels: iterable with labels to be explained.\n","            top_labels: if not None, ignore labels and produce explanations for\n","                the K labels with highest prediction probabilities, where K is\n","                this parameter.\n","            num_features: maximum number of features present in explanation\n","            num_samples: size of the neighborhood to learn the linear model\n","            distance_metric: the distance metric to use for sample weighting,\n","                defaults to cosine similarity\n","            model_regressor: sklearn regressor to use in explanation. Defaults\n","            to Ridge regression in LimeBase. Must have model_regressor.coef_\n","            and 'sample_weight' as a parameter to model_regressor.fit()\n","        Returns:\n","            An Explanation object (see explanation.py) with the corresponding\n","            explanations.\n","        \"\"\"\n","\n","        indexed_string = (IndexedCharacters(\n","            text_instance, bow=self.bow, mask_string=self.mask_string)\n","                          if self.char_level else\n","                          IndexedString(text_instance, bow=self.bow,\n","                                        split_expression=self.split_expression,\n","                                        mask_string=self.mask_string)) # here it tokenizes the string using IndexedString\n","        domain_mapper = TextDomainMapper(indexed_string)\n","        data, yss, distances = self.__data_labels_distances( # here it calculates various instances of the data with hidden tokens (0 and 1)\n","            indexed_string, classifier_fn, num_samples,\n","            distance_metric=distance_metric)\n","        if self.class_names is None:\n","            self.class_names = [str(x) for x in range(yss[0].shape[0])]\n","        ret_exp = Explanation(domain_mapper=domain_mapper,\n","                                          class_names=self.class_names,\n","                                          random_state=self.random_state)\n","        ret_exp.predict_proba = yss[0]\n","        if top_labels:\n","            labels = np.argsort(yss[0])[-top_labels:]\n","            ret_exp.top_labels = list(labels)\n","            ret_exp.top_labels.reverse()\n","        for label in labels:\n","            (ret_exp.intercept[label],\n","             ret_exp.local_exp[label],\n","             ret_exp.score[label],\n","             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n","                data, yss, distances, label, num_features,\n","                model_regressor=model_regressor,\n","                feature_selection=self.feature_selection)\n","        return ret_exp\n","\n","    def __data_labels_distances(self,\n","                                indexed_string,\n","                                classifier_fn,\n","                                num_samples,\n","                                distance_metric='cosine'):\n","        \"\"\"Generates a neighborhood around a prediction.\n","        Generates neighborhood data by randomly removing words from\n","        the instance, and predicting with the classifier. Uses cosine distance\n","        to compute distances between original and perturbed instances.\n","        Args:\n","            indexed_string: document (IndexedString) to be explained,\n","            classifier_fn: classifier prediction probability function, which\n","                takes a string and outputs prediction probabilities. For\n","                ScikitClassifier, this is classifier.predict_proba.\n","            num_samples: size of the neighborhood to learn the linear model\n","            distance_metric: the distance metric to use for sample weighting,\n","                defaults to cosine similarity.\n","        Returns:\n","            A tuple (data, labels, distances), where:\n","                data: dense num_samples * K binary matrix, where K is the\n","                    number of tokens in indexed_string. The first row is the\n","                    original instance, and thus a row of ones.\n","                labels: num_samples * L matrix, where L is the number of target\n","                    labels\n","                distances: cosine distance between the original instance and\n","                    each perturbed instance (computed in the binary 'data'\n","                    matrix), times 100.\n","        \"\"\"\n","\n","        def distance_fn(x):\n","            return sklearn.metrics.pairwise.pairwise_distances(\n","                x, x[0], metric=distance_metric).ravel() * 100\n","\n","        doc_size = indexed_string.num_words()\n","        sample = self.random_state.randint(1, doc_size + 1, num_samples - 1)\n","        data = np.ones((num_samples, doc_size))\n","        data[0] = np.ones(doc_size)\n","        features_range = range(doc_size)\n","        inverse_data = [indexed_string.raw_string()]\n","        for i, size in enumerate(sample, start=1):\n","            inactive = self.random_state.choice(features_range, size,\n","                                                replace=False)\n","            data[i, inactive] = 0\n","            inverse_data.append(indexed_string.inverse_removing(inactive))\n","        labels = classifier_fn(inverse_data)\n","        distances = distance_fn(sp.sparse.csr_matrix(data))\n","        return data, labels, distances"]},{"cell_type":"markdown","metadata":{"id":"94rs0s1sPLZ8"},"source":["#### MultiModal LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y67w4NpAF-2Z"},"outputs":[],"source":["class LimeMultimodalExplainer(object):\n","    \"\"\"Explains text classifiers.\n","       Currently, we are using an exponential kernel on cosine distance, and\n","       restricting explanations to words that are present in documents.\"\"\"\n","\n","    def __init__(self,\n","                 kernel_width=25,\n","                 kernel=None,\n","                 verbose=False,\n","                 class_names=None,\n","                 feature_selection='auto',\n","                 split_expression=r'\\W+',\n","                 bow=True,\n","                 mask_string=None,\n","                 random_state=None,\n","                 char_level=False):\n","        \"\"\"Init function.\n","        Args:\n","            kernel_width: kernel width for the exponential kernel.\n","            kernel: similarity kernel that takes euclidean distances and kernel\n","                width as input and outputs weights in (0,1). If None, defaults to\n","                an exponential kernel.\n","            verbose: if true, print local prediction values from linear model\n","            class_names: list of class names, ordered according to whatever the\n","                classifier is using. If not present, class names will be '0',\n","                '1', ...\n","            feature_selection: feature selection method. can be\n","                'forward_selection', 'lasso_path', 'none' or 'auto'.\n","                See function 'explain_instance_with_data' in lime_base.py for\n","                details on what each of the options does.\n","            split_expression: Regex string or callable. If regex string, will be used with re.split.\n","                If callable, the function should return a list of tokens.\n","            bow: if True (bag of words), will perturb input data by removing\n","                all occurrences of individual words or characters.\n","                Explanations will be in terms of these words. Otherwise, will\n","                explain in terms of word-positions, so that a word may be\n","                important the first time it appears and unimportant the second.\n","                Only set to false if the classifier uses word order in some way\n","                (bigrams, etc), or if you set char_level=True.\n","            mask_string: String used to mask tokens or characters if bow=False\n","                if None, will be 'UNKWORDZ' if char_level=False, chr(0)\n","                otherwise.\n","            random_state: an integer or numpy.RandomState that will be used to\n","                generate random numbers. If None, the random state will be\n","                initialized using the internal numpy seed.\n","            char_level: an boolean identifying that we treat each character\n","                as an independent occurence in the string\n","        \"\"\"\n","\n","        if kernel is None:\n","            def kernel(d, kernel_width):\n","                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n","\n","        kernel_fn = partial(kernel, kernel_width=kernel_width)\n","\n","        self.random_state = check_random_state(random_state)\n","        self.base = LimeBase(kernel_fn, verbose,\n","                                       random_state=self.random_state)\n","        self.class_names = class_names\n","        self.vocabulary = None\n","        self.feature_selection = feature_selection\n","        self.bow = bow\n","        self.mask_string = mask_string\n","        self.split_expression = split_expression\n","        self.char_level = char_level\n","\n","    def explain_instance(self,\n","                         sample_observation,\n","                         predict_fn,\n","                         labels=(1,),\n","                         top_labels=None,\n","                         num_features=10,\n","                         num_samples=5000,\n","                         distance_metric='cosine',\n","                         model_regressor=None):\n","        \"\"\"Generates explanations for a prediction.\n","        First, we generate neighborhood data by randomly hiding features from\n","        the instance (see __data_labels_distance_mapping). We then learn\n","        locally weighted linear models on this neighborhood data to explain\n","        each of the classes in an interpretable way (see lime_base.py).\n","        Args:\n","            text_instance: raw text string to be explained.\n","            predict_fn: classifier prediction probability function, which\n","                takes a list of d strings and outputs a (d, k) numpy array with\n","                prediction probabilities, where k is the number of classes.\n","                For ScikitClassifiers , this is classifier.predict_proba.\n","            labels: iterable with labels to be explained.\n","            top_labels: if not None, ignore labels and produce explanations for\n","                the K labels with highest prediction probabilities, where K is\n","                this parameter.\n","            num_features: maximum number of features present in explanation\n","            num_samples: size of the neighborhood to learn the linear model\n","            distance_metric: the distance metric to use for sample weighting,\n","                defaults to cosine similarity\n","            model_regressor: sklearn regressor to use in explanation. Defaults\n","            to Ridge regression in LimeBase. Must have model_regressor.coef_\n","            and 'sample_weight' as a parameter to model_regressor.fit()\n","        Returns:\n","            An Explanation object (see explanation.py) with the corresponding\n","            explanations.\n","        \"\"\"\n","        text_instance = sample_observation['discharge']\n","        tabular_instance = sample_observation.drop(['discharge'])\n","        indexed_string = (IndexedCharacters(\n","            text_instance, bow=self.bow, mask_string=self.mask_string)\n","                          if self.char_level else\n","                          IndexedString(text_instance, bow=self.bow,\n","                                        split_expression=self.split_expression,\n","                                        mask_string=self.mask_string)) # here it tokenizes the string using IndexedString\n","        domain_mapper = TextDomainMapper(indexed_string)\n","        data, yss, distances = self.__data_labels_distances( # here it calculates various instances of the data with hidden tokens (0 and 1)\n","            indexed_string, tabular_instance, predict_fn, num_samples, # CHANGED HERE\n","            distance_metric=distance_metric) # data is a list of 0/1 arrays (0 -> word is not used); yss are the labels for each array\n","        if self.class_names is None:\n","            self.class_names = [str(x) for x in range(yss[0].shape[0])]\n","        ret_exp = Explanation(domain_mapper=domain_mapper, # generates an explanation object\n","                                          class_names=self.class_names,\n","                                          random_state=self.random_state)\n","        ret_exp.predict_proba = yss[0] # the first yss is the original prediction for our original sample!\n","        if top_labels: # IGNORE, it just produces explanations for the top labels instead of all, usually not used\n","            labels = np.argsort(yss[0])[-top_labels:]\n","            ret_exp.top_labels = list(labels)\n","            ret_exp.top_labels.reverse()\n","        for label in labels: # for each class/label we have (in our case Low/High LOS)\n","            (ret_exp.intercept[label], # we fill the explanation object with -> the intercept\n","             ret_exp.local_exp[label], # the local explanation\n","             ret_exp.score[label], # the explanation score and the local prediction -> all is filled with the base.explain_instance_with_data\n","             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n","                data, yss, distances, label, num_features,\n","                model_regressor=model_regressor,\n","                feature_selection=self.feature_selection)\n","        return ret_exp\n","\n","    def __data_labels_distances(self,\n","                                indexed_string, tabular_instance, #CHANGED HERE ADDED TABULAR INSTANCE\n","                                predict_fn,\n","                                num_samples,\n","                                distance_metric='cosine'):\n","        \"\"\"Generates a neighborhood around a prediction.\n","        Generates neighborhood data by randomly removing words from\n","        the instance, and predicting with the classifier. Uses cosine distance\n","        to compute distances between original and perturbed instances.\n","        Args:\n","            indexed_string: document (IndexedString) to be explained,\n","            predict_fn: classifier prediction probability function, which\n","                takes a string and outputs prediction probabilities. For\n","                ScikitClassifier, this is classifier.predict_proba.\n","            num_samples: size of the neighborhood to learn the linear model\n","            distance_metric: the distance metric to use for sample weighting,\n","                defaults to cosine similarity.\n","        Returns:\n","            A tuple (data, labels, distances), where:\n","                data: dense num_samples * K binary matrix, where K is the\n","                    number of tokens in indexed_string. The first row is the\n","                    original instance, and thus a row of ones.\n","                labels: num_samples * L matrix, where L is the number of target\n","                    labels\n","                distances: cosine distance between the original instance and\n","                    each perturbed instance (computed in the binary 'data'\n","                    matrix), times 100.\n","        \"\"\"\n","\n","        def distance_fn(x):\n","            return sklearn.metrics.pairwise.pairwise_distances(\n","                x, x[0], metric=distance_metric).ravel() * 100\n","\n","        doc_size = indexed_string.num_words() # get the number of words we have\n","        sample = self.random_state.randint(1, doc_size + 1, num_samples - 1) # generates a 1 x doc_size array with num_samples - 1 samples\n","        data = np.ones((num_samples, doc_size)) # generates an array full of ones\n","        data[0] = np.ones(doc_size) # fills the first array with 1s\n","        features_range = range(doc_size) # get the number of features as a range\n","        inverse_data = [indexed_string.raw_string()] # we get the raw string\n","        inverse_data = [tabular_instance.append(pd.Series(inverse_data))] # here we are appending the tabular data\n","        for i, size in enumerate(sample, start=1): # for each sample we want to generate\n","            inactive = self.random_state.choice(features_range, size, # randomly generate what data is going to be equal to 0 from 1 to 768 (our document length)\n","                                                replace=False)\n","            data[i, inactive] = 0 # set to 0 certain words in each data array\n","            inverse_data.append(tabular_instance.append(pd.Series(indexed_string.inverse_removing(inactive)))) \n","            # here it creates different string perturbations (as many as sample size), it removes from the original full 1 data the 0 we have generated with inactive\n","      \n","        labels = predict_fn(inverse_data) # generates labels\n","        distances = distance_fn(sp.sparse.csr_matrix(data)) # calculates the distance\n","        return data, labels, distances"]},{"cell_type":"markdown","metadata":{"id":"EX6z6ZexPOOJ"},"source":["#### TextMixed LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2BGcz1uOG8m"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","from scipy import sparse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNx-19mBPUlO"},"outputs":[],"source":["class TextMixedDomainMapper(DomainMapper):\n","    \"\"\"Maps feature ids to names, generates table views, etc\"\"\"\n","\n","    def __init__(self, feature_names, feature_values, scaled_row,\n","                 categorical_features, indexed_string, discretized_feature_names=None,\n","                 feature_indexes=None):\n","        \"\"\"Init.\n","        Args:\n","            feature_names: list of feature names, in order\n","            feature_values: list of strings with the values of the original row\n","            scaled_row: scaled row\n","            categorical_features: list of categorical features ids (ints)\n","            feature_indexes: optional feature indexes used in the sparse case\n","        \"\"\"\n","        self.exp_feature_names = feature_names\n","        self.discretized_feature_names = discretized_feature_names\n","        self.feature_names = feature_names\n","        self.feature_values = feature_values\n","        self.feature_indexes = feature_indexes\n","        self.scaled_row = scaled_row\n","        self.indexed_string = indexed_string\n","        if sp.sparse.issparse(scaled_row):\n","            self.all_categorical = False\n","        else:\n","            self.all_categorical = len(categorical_features) == len(scaled_row)\n","        self.categorical_features = categorical_features\n","\n","    def map_exp_ids(self, exp, positions = False):\n","        \"\"\"Maps ids to feature names.\n","        Args:\n","            exp: list of tuples [(id, weight), (id,weight)]\n","            positions: if True, also return word positions\n","        Returns:\n","            list of tuples (feature_name, weight)\n","        \"\"\"\n","        names = self.exp_feature_names\n","        if self.discretized_feature_names is not None:\n","            names = self.discretized_feature_names\n","        exp_list = []\n","        for x in exp:\n","          # if so -> we are dealing with our text column (assuming it is the last column of our dataframe)\n","          if x[0] > max(self.categorical_features):\n","            if positions:\n","                single_exp = ('%s_%s' % (\n","                    self.indexed_string.word(x[0]),\n","                    '-'.join(\n","                        map(str,\n","                            self.indexed_string.string_position(x[0])))), x[1])\n","            else:\n","                \n","                single_exp = (self.indexed_string.word(x[0] - max(self.categorical_features)-1), x[1]) # need to subtract, since the vocabulary IDs starts from 0\n","          else:\n","              single_exp = (names[x[0]], x[1])\n","          exp_list.append(single_exp)\n","        return exp_list\n","\n","    def visualize_instance_html(self,\n","                                exp,\n","                                label,\n","                                div_name,\n","                                exp_object_name,\n","                                show_table=True,\n","                                show_all=False):\n","        \"\"\"Shows the current example in a table format.\n","        Args:\n","             exp: list of tuples [(id, weight), (id,weight)]\n","             label: label id (integer)\n","             div_name: name of div object to be used for rendering(in js)\n","             exp_object_name: name of js explanation object\n","             show_table: if False, don't show table visualization.\n","             show_all: if True, show zero-weighted features in the table.\n","        \"\"\"\n","        if not show_table:\n","            return ''\n","        weights = [0] * len(self.feature_names)\n","        for x in exp:\n","            weights[x[0]] = x[1]\n","        if self.feature_indexes is not None:\n","            # Sparse case: only display the non-zero values and importances\n","            fnames = [self.exp_feature_names[i] for i in self.feature_indexes]\n","            fweights = [weights[i] for i in self.feature_indexes]\n","            if show_all:\n","                out_list = list(zip(fnames,\n","                                    self.feature_values,\n","                                    fweights))\n","            else:\n","                out_dict = dict(map(lambda x: (x[0], (x[1], x[2], x[3])),\n","                                zip(self.feature_indexes,\n","                                    fnames,\n","                                    self.feature_values,\n","                                    fweights)))\n","                out_list = [out_dict.get(x[0], (str(x[0]), 0.0, 0.0)) for x in exp]\n","        else:\n","            out_list = list(zip(self.exp_feature_names,\n","                                self.feature_values,\n","                                weights))\n","            if not show_all:\n","                out_list = [out_list[x[0]] for x in exp]\n","        ret = u'''\n","            %s.show_raw_tabular(%s, %d, %s);\n","        ''' % (exp_object_name, json.dumps(out_list, ensure_ascii=False), label, div_name)\n","        return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFU-Nr5aAMaS"},"outputs":[],"source":["class LimeTextMixed(object):\n","    \"\"\"Explains predictions on tabular (i.e. matrix) data.\n","    For numerical features, perturb them by sampling from a Normal(0,1) and\n","    doing the inverse operation of mean-centering and scaling, according to the\n","    means and stds in the training data. For categorical features, perturb by\n","    sampling according to the training distribution, and making a binary\n","    feature that is 1 when the value is the same as the instance being\n","    explained.\"\"\"\n","\n","    def __init__(self,\n","                 training_data,\n","                 mode=\"classification\",\n","                 training_labels=None,\n","                 feature_names=None,\n","                 categorical_features=None,\n","                 text_features = None,\n","                 categorical_names=None,\n","                 kernel_width=25,\n","                 kernel=None,\n","                 verbose=False,\n","                 class_names=None,\n","                 feature_selection='auto',\n","                 discretize_continuous=True,\n","                 discretizer='quartile',\n","                 sample_around_instance=False,\n","                 random_state=None,\n","                 training_data_stats=None,\n","                 split_expression=r'\\W+',\n","                 bow=True,\n","                 mask_string=None,\n","                 char_level=False):\n","        \"\"\"Init function.\n","        Args:\n","            training_data: numpy 2d array\n","            mode: \"classification\" or \"regression\"\n","            training_labels: labels for training data. Not required, but may be\n","                used by discretizer.\n","            feature_names: list of names (strings) corresponding to the columns\n","                in the training data.\n","            categorical_features: list of indices (ints) corresponding to the\n","                categorical columns. Everything else will be considered\n","                continuous. Values in these columns MUST be integers.\n","            categorical_names: map from int to list of names, where\n","                categorical_names[x][y] represents the name of the yth value of\n","                column x.\n","            kernel_width: kernel width for the exponential kernel.\n","                If None, defaults to sqrt (number of columns) * 0.75\n","            kernel: similarity kernel that takes euclidean distances and kernel\n","                width as input and outputs weights in (0,1). If None, defaults to\n","                an exponential kernel.\n","            verbose: if true, print local prediction values from linear model\n","            class_names: list of class names, ordered according to whatever the\n","                classifier is using. If not present, class names will be '0',\n","                '1', ...\n","            feature_selection: feature selection method. can be\n","                'forward_selection', 'lasso_path', 'none' or 'auto'.\n","                See function 'explain_instance_with_data' in lime_base.py for\n","                details on what each of the options does.\n","            discretize_continuous: if True, all non-categorical features will\n","                be discretized into quartiles.\n","            discretizer: only matters if discretize_continuous is True\n","                and data is not sparse. Options are 'quartile', 'decile',\n","                'entropy' or a BaseDiscretizer instance.\n","            sample_around_instance: if True, will sample continuous features\n","                in perturbed samples from a normal centered at the instance\n","                being explained. Otherwise, the normal is centered on the mean\n","                of the feature data.\n","            random_state: an integer or numpy.RandomState that will be used to\n","                generate random numbers. If None, the random state will be\n","                initialized using the internal numpy seed.\n","            training_data_stats: a dict object having the details of training data\n","                statistics. If None, training data information will be used, only matters\n","                if discretize_continuous is True. Must have the following keys:\n","                means\", \"mins\", \"maxs\", \"stds\", \"feature_values\",\n","                \"feature_frequencies\"\n","            split_expression: Regex string or callable. If regex string, will be used with re.split.\n","                If callable, the function should return a list of tokens.\n","            bow: if True (bag of words), will perturb input data by removing\n","                all occurrences of individual words or characters.\n","                Explanations will be in terms of these words. Otherwise, will\n","                explain in terms of word-positions, so that a word may be\n","                important the first time it appears and unimportant the second.\n","                Only set to false if the classifier uses word order in some way\n","                (bigrams, etc), or if you set char_level=True.\n","            mask_string: String used to mask tokens or characters if bow=False\n","                if None, will be 'UNKWORDZ' if char_level=False, chr(0)\n","                otherwise.\n","            char_level: an boolean identifying that we treat each character\n","                as an independent occurence in the string\n","        \"\"\"\n","        self.random_state = check_random_state(random_state)\n","        self.mode = mode\n","        self.categorical_names = categorical_names or {}\n","        self.sample_around_instance = sample_around_instance\n","        self.training_data_stats = training_data_stats\n","        self.encoder = sklearn.preprocessing.OneHotEncoder(handle_unknown = 'ignore') # define our encoder\n","        self.vocabulary = None\n","        self.bow = bow\n","        self.mask_string = mask_string\n","        self.split_expression = split_expression\n","        self.char_level = char_level\n","        self.column_names = training_data.columns\n","        \n","\n","        # Check and raise proper error in stats are supplied in non-descritized path\n","        if self.training_data_stats:\n","            self.validate_training_data_stats(self.training_data_stats)\n","\n","        if categorical_features is None:\n","            categorical_features = []\n","        \n","\n","        categorical_features = list(categorical_features)\n","        # if the text feature is also in the categorical features, we remove it\n","        if text_features in categorical_features:\n","          categorical_features.remove(text_features)\n","\n","        \n","        self.text_features = text_features\n","\n","\n","        # LUCA -> Clean Pandas dataframe and prepare it to be used by LIME\n","\n","        training_data.reset_index(inplace = True, drop = True) # remove index order\n","        no_text_df = training_data.drop(text_features, axis = 1) # drop the text feature\n","        self.encoder.fit(no_text_df.loc[:,categorical_features]) # fit our encoder\n","        training_data_ohe = pd.DataFrame(self.encoder.transform(no_text_df.loc[:,categorical_features]).toarray(), # encode and transform to dataframe our categorical features\n","                                         columns = self.encoder.get_feature_names_out(categorical_features))\n","        training_data = pd.concat([no_text_df.drop(categorical_features, axis = 1), training_data_ohe], axis = 1) # merge encoded categorical features with numerical ones\n","        \n","        self.feature_names_nontext = list(training_data.columns)\n","\n","\n","        self.numerical_feat_name = no_text_df.drop(categorical_features, axis = 1).columns\n","        self.numerical_feat = training_data.columns.get_indexer(self.numerical_feat_name)\n","        self.original_cat_feat = categorical_features\n","        self.categorical_names = categorical_features\n","        self.categorical_features = training_data.columns.get_indexer(self.encoder.get_feature_names_out(self.original_cat_feat)) # get the indexes of the categorical features\n","\n","        self.cat_features_ohe = training_data.columns.get_indexer(self.encoder.get_feature_names_out(self.original_cat_feat)) # get the indexes of the categorical features\n","        \n","        training_data = training_data.to_numpy() # transform to numpy array\n","\n","        if feature_names is None:\n","            feature_names = [str(i) for i in range(training_data.shape[1]+1)] # add one for the text feature\n","        self.feature_names = list(feature_names)\n","\n","        \n","        \n","        self.discretizer = None\n","        if discretize_continuous and not sp.sparse.issparse(training_data):\n","            # Set the discretizer if training data stats are provided\n","            if self.training_data_stats:\n","                discretizer = StatsDiscretizer(\n","                    training_data, self.categorical_features,\n","                    self.feature_names_nontext, labels=training_labels,\n","                    data_stats=self.training_data_stats,\n","                    random_state=self.random_state)\n","\n","            if discretizer == 'quartile':\n","                self.discretizer = QuartileDiscretizer(\n","                        training_data, self.categorical_features,\n","                        self.feature_names_nontext, labels=training_labels,\n","                        random_state=self.random_state)\n","            elif discretizer == 'decile':\n","                self.discretizer = DecileDiscretizer(\n","                        training_data, self.categorical_features,\n","                        self.feature_names_nontext, labels=training_labels,\n","                        random_state=self.random_state)\n","            elif discretizer == 'entropy':\n","                self.discretizer = EntropyDiscretizer(\n","                        training_data, self.categorical_features,\n","                        self.feature_names_nontext, labels=training_labels,\n","                        random_state=self.random_state)\n","            elif isinstance(discretizer, BaseDiscretizer):\n","                self.discretizer = discretizer\n","            else:\n","                raise ValueError('''Discretizer must be 'quartile',''' +\n","                                 ''' 'decile', 'entropy' or a''' +\n","                                 ''' BaseDiscretizer instance''')\n","            self.categorical_features = list(range(training_data.shape[1])) # -> ??????????? <- \n","\n","            # Get the discretized_training_data when the stats are not provided\n","            if(self.training_data_stats is None):\n","                discretized_training_data = self.discretizer.discretize(\n","                    training_data)\n","        #self.categorical_features = list(range(training_data.shape[1])) # -> ??????????? <- \n","        if kernel_width is None:\n","            kernel_width = np.sqrt(training_data.shape[1]) * .75\n","        kernel_width = float(kernel_width)\n","\n","        if kernel is None:\n","            def kernel(d, kernel_width):\n","                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n","\n","        kernel_fn = partial(kernel, kernel_width=kernel_width)\n","\n","        self.feature_selection = feature_selection\n","        self.base = LimeBase(kernel_fn, verbose, random_state=self.random_state)\n","        self.class_names = class_names\n","\n","        # Though set has no role to play if training data stats are provided\n","        self.scaler = sklearn.preprocessing.StandardScaler(with_mean=False)\n","        self.scaler.fit(training_data)\n","        self.feature_values = {}\n","        self.feature_frequencies = {}\n","\n","        for feature in self.categorical_features:\n","            if training_data_stats is None:\n","                if self.discretizer is not None:\n","                    column = discretized_training_data[:, feature]\n","                else:\n","                    column = training_data[:, feature]\n","\n","                feature_count = collections.Counter(column)\n","                values, frequencies = map(list, zip(*(sorted(feature_count.items()))))\n","            else:\n","                values = training_data_stats[\"feature_values\"][feature]\n","                frequencies = training_data_stats[\"feature_frequencies\"][feature]\n","\n","            self.feature_values[feature] = values\n","            self.feature_frequencies[feature] = (np.array(frequencies) /\n","                                                 float(sum(frequencies)))\n","            self.scaler.mean_[feature] = 0\n","            self.scaler.scale_[feature] = 1\n","\n","    @staticmethod\n","    def convert_and_round(values):\n","        return ['%.2f' % v for v in values]\n","\n","    @staticmethod\n","    def validate_training_data_stats(training_data_stats):\n","        \"\"\"\n","            Method to validate the structure of training data stats\n","        \"\"\"\n","        stat_keys = list(training_data_stats.keys())\n","        valid_stat_keys = [\"means\", \"mins\", \"maxs\", \"stds\", \"feature_values\", \"feature_frequencies\"]\n","        missing_keys = list(set(valid_stat_keys) - set(stat_keys))\n","        if len(missing_keys) > 0:\n","            raise Exception(\"Missing keys in training_data_stats. Details: %s\" % (missing_keys))\n","\n","    def explain_instance(self,\n","                         data_row,\n","                         predict_fn,\n","                         labels=(1,),\n","                         top_labels=None,\n","                         num_features=10,\n","                         num_samples=5000,\n","                         distance_metric='euclidean',\n","                         model_regressor=None,\n","                         sampling_method='gaussian'):\n","        \"\"\"Generates explanations for a prediction.\n","        First, we generate neighborhood data by randomly perturbing features\n","        from the instance (see __data_inverse). We then learn locally weighted\n","        linear models on this neighborhood data to explain each of the classes\n","        in an interpretable way (see lime_base.py).\n","        Args:\n","            data_row: 1d numpy array or scipy.sparse matrix, corresponding to a row\n","            predict_fn: prediction function. For classifiers, this should be a\n","                function that takes a numpy array and outputs prediction\n","                probabilities. For regressors, this takes a numpy array and\n","                returns the predictions. For ScikitClassifiers, this is\n","                `classifier.predict_proba()`. For ScikitRegressors, this\n","                is `regressor.predict()`. The prediction function needs to work\n","                on multiple feature vectors (the vectors randomly perturbed\n","                from the data_row).\n","            labels: iterable with labels to be explained.\n","            top_labels: if not None, ignore labels and produce explanations for\n","                the K labels with highest prediction probabilities, where K is\n","                this parameter.\n","            num_features: maximum number of features present in explanation\n","            num_samples: size of the neighborhood to learn the linear model\n","            distance_metric: the distance metric to use for weights.\n","            model_regressor: sklearn regressor to use in explanation. Defaults\n","                to Ridge regression in LimeBase. Must have model_regressor.coef_\n","                and 'sample_weight' as a parameter to model_regressor.fit()\n","            sampling_method: Method to sample synthetic data. Defaults to Gaussian\n","                sampling. Can also use Latin Hypercube Sampling.\n","        Returns:\n","            An Explanation object (see explanation.py) with the corresponding\n","            explanations.\n","        \"\"\"\n","        data_row = pd.DataFrame(data_row).T\n","        data_row.reset_index(inplace = True, drop = True)\n","        text_only = data_row[self.text_features][0]\n","        no_text_df = data_row.drop(self.text_features, axis = 1) # drop the text feature\n","        data_row_ohe = pd.DataFrame(self.encoder.transform(no_text_df.loc[:,self.original_cat_feat]).toarray(), # encode and transform to dataframe our categorical features\n","                                         columns = self.encoder.get_feature_names_out(self.original_cat_feat))\n","        data_row = pd.concat([no_text_df.drop(self.original_cat_feat, axis = 1), data_row_ohe], axis = 1) # merge encoded categorical features with numerical ones\n","\n","        data_row = data_row.T.to_numpy()\n","        if sp.sparse.issparse(data_row) and not sp.sparse.isspmatrix_csr(data_row):\n","            # Preventative code: if sparse, convert to csr format if not in csr format already\n","            data_row = data_row.tocsr()\n","\n","        # -> TEXT ADDITION\n","        indexed_string = (IndexedCharacters(\n","            text_only, bow=self.bow, mask_string=self.mask_string)\n","                          if self.char_level else\n","                          IndexedString(text_only, bow=self.bow,\n","                                        split_expression=self.split_expression,\n","                                        mask_string=self.mask_string))\n","        \n","        \n","        \n","        data, inverse, data_text, inverse_data_text = self.__data_inverse(data_row, indexed_string, num_samples, sampling_method)\n","        \n","        if sp.sparse.issparse(data):\n","            # Note in sparse case we don't subtract mean since data would become dense\n","            scaled_data = data.multiply(self.scaler.scale_)\n","            # Multiplying with csr matrix can return a coo sparse matrix\n","            if not sp.sparse.isspmatrix_csr(scaled_data):\n","                scaled_data = scaled_data.tocsr()\n","        else:\n","            scaled_data = (data - self.scaler.mean_) / self.scaler.scale_\n","        \n","        scaled_data = np.concatenate((scaled_data, data_text), axis = 1)\n","        distances = sklearn.metrics.pairwise_distances(\n","                scaled_data,\n","                scaled_data[0].reshape(1, -1),\n","                metric=distance_metric\n","        ).ravel()\n","\n","        inverse_cat = pd.DataFrame(self.encoder.inverse_transform(inverse[:,self.cat_features_ohe]), columns = self.original_cat_feat)\n","        inverse_num = pd.DataFrame(inverse[:, self.numerical_feat], columns = self.numerical_feat_name)\n","        inverse_data_text = pd.DataFrame(inverse_data_text, columns = [self.text_features])\n","        inverse_cat.reset_index(inplace = True, drop = True)\n","        inverse_num.reset_index(inplace = True, drop = True)\n","        inverse_data_text.reset_index(inplace = True, drop = True)\n","        inverse = pd.concat([inverse_cat, inverse_num, inverse_data_text], axis = 1)\n","\n","        inverse = inverse[self.column_names]\n","        yss = predict_fn(inverse)\n","\n","        # for classification, the model needs to provide a list of tuples - classes\n","        # along with prediction probabilities\n","        if self.mode == \"classification\":\n","            if len(yss.shape) == 1:\n","                raise NotImplementedError(\"LIME does not currently support \"\n","                                          \"classifier models without probability \"\n","                                          \"scores. If this conflicts with your \"\n","                                          \"use case, please let us know: \"\n","                                          \"https://github.com/datascienceinc/lime/issues/16\")\n","            elif len(yss.shape) == 2:\n","                if self.class_names is None:\n","                    self.class_names = [str(x) for x in range(yss[0].shape[0])]\n","                else:\n","                    self.class_names = list(self.class_names)\n","                if not np.allclose(yss.sum(axis=1), 1.0):\n","                    warnings.warn(\"\"\"\n","                    Prediction probabilties do not sum to 1, and\n","                    thus does not constitute a probability space.\n","                    Check that you classifier outputs probabilities\n","                    (Not log probabilities, or actual class predictions).\n","                    \"\"\")\n","            else:\n","                raise ValueError(\"Your model outputs \"\n","                                 \"arrays with {} dimensions\".format(len(yss.shape)))\n","\n","        # for regression, the output should be a one-dimensional array of predictions\n","        else:\n","            try:\n","                if len(yss.shape) != 1 and len(yss[0].shape) == 1:\n","                    yss = np.array([v[0] for v in yss])\n","                assert isinstance(yss, np.ndarray) and len(yss.shape) == 1\n","            except AssertionError:\n","                raise ValueError(\"Your model needs to output single-dimensional \\\n","                    numpyarrays, not arrays of {} dimensions\".format(yss.shape))\n","\n","            predicted_value = yss[0]\n","            min_y = min(yss)\n","            max_y = max(yss)\n","\n","            # add a dimension to be compatible with downstream machinery\n","            yss = yss[:, np.newaxis]\n","\n","        feature_names = copy.deepcopy(self.feature_names)\n","        if feature_names is None:\n","            feature_names = [str(x) for x in range(data_row.shape[0])]\n","\n","        if sp.sparse.issparse(data_row):\n","            values = self.convert_and_round(data_row.data)\n","            feature_indexes = data_row.indices\n","        else:\n","            values = self.convert_and_round(data_row)\n","            feature_indexes = None\n","\n","        for i in self.categorical_features:\n","            if self.discretizer is not None and i in self.discretizer.lambdas:\n","                continue\n","            name = self.feature_names_nontext[i]\n","\n","            for cat_col in self.categorical_names:\n","              if bool(re.search(cat_col, name)): \n","                name = cat_col\n","            #if i in self.cat_features_ohe:\n","            #    print(i)\n","                #name = self.categorical_names[i][name]\n","            feature_names[i] = '%s=%s' % (name, self.feature_names_nontext[i])\n","            values[i] = 'True'\n","        if self.discretizer is None:\n","            for i in self.numerical_feat:\n","              feature_names[i] = self.numerical_feat_name[i]\n","        categorical_features = self.categorical_features\n","        \n","\n","        discretized_feature_names = None\n","        if self.discretizer is not None:\n","            categorical_features = range(data.shape[1])\n","            discretized_instance = self.discretizer.discretize(data_row[:,0])\n","            discretized_feature_names = copy.deepcopy(feature_names)\n","            for f in self.discretizer.names:\n","                discretized_feature_names[f] = self.discretizer.names[f][int(\n","                        discretized_instance[f])]\n","        \n","        \n","        domain_mapper = TextMixedDomainMapper(feature_names,\n","                                          values,\n","                                          scaled_data[0], indexed_string = indexed_string,\n","                                          categorical_features=categorical_features,\n","                                          discretized_feature_names=discretized_feature_names,\n","                                          feature_indexes=feature_indexes)\n","\n","        ret_exp = Explanation(domain_mapper,\n","                                          mode=self.mode,\n","                                          class_names=self.class_names)\n","        if self.mode == \"classification\":\n","            ret_exp.predict_proba = yss[0]\n","            if top_labels:\n","                labels = np.argsort(yss[0])[-top_labels:]\n","                ret_exp.top_labels = list(labels)\n","                ret_exp.top_labels.reverse()\n","        else:\n","            ret_exp.predicted_value = predicted_value\n","            ret_exp.min_value = min_y\n","            ret_exp.max_value = max_y\n","            labels = [0]\n","        #return scaled_data, yss, distances, labels, num_features\n","        for label in labels:\n","            (ret_exp.intercept[label],\n","             ret_exp.local_exp[label],\n","             ret_exp.score[label],\n","             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n","                    scaled_data,\n","                    yss,\n","                    distances,\n","                    label,\n","                    num_features,\n","                    model_regressor=model_regressor,\n","                    feature_selection=self.feature_selection)\n","\n","        if self.mode == \"regression\":\n","            ret_exp.intercept[1] = ret_exp.intercept[0]\n","            ret_exp.local_exp[1] = [x for x in ret_exp.local_exp[0]]\n","            ret_exp.local_exp[0] = [(i, -1 * j) for i, j in ret_exp.local_exp[1]]\n","\n","        return ret_exp\n","\n","    def __data_inverse(self,\n","                       data_row,\n","                       indexed_string,\n","                       num_samples,\n","                       sampling_method):\n","        \"\"\"Generates a neighborhood around a prediction.\n","        For numerical features, perturb them by sampling from a Normal(0,1) and\n","        doing the inverse operation of mean-centering and scaling, according to\n","        the means and stds in the training data. For categorical features,\n","        perturb by sampling according to the training distribution, and making\n","        a binary feature that is 1 when the value is the same as the instance\n","        being explained.\n","        Args:\n","            data_row: 1d numpy array, corresponding to a row\n","            num_samples: size of the neighborhood to learn the linear model\n","            sampling_method: 'gaussian' or 'lhs'\n","        Returns:\n","            A tuple (data, inverse), where:\n","                data: dense num_samples * K matrix, where categorical features\n","                are encoded with either 0 (not equal to the corresponding value\n","                in data_row) or 1. The first row is the original instance.\n","                inverse: same as data, except the categorical features are not\n","                binary, but categorical (as the original data)\n","        \"\"\"\n","        is_sparse = sp.sparse.issparse(data_row)\n","        if is_sparse:\n","            num_cols = data_row.shape[1]\n","            data = sp.sparse.csr_matrix((num_samples, num_cols), dtype=data_row.dtype)\n","        else:\n","            num_cols = data_row.shape[0]\n","            data = np.zeros((num_samples, num_cols))\n","        categorical_features = range(num_cols)\n","        if self.discretizer is None:\n","            instance_sample = data_row\n","            scale = self.scaler.scale_\n","            mean = self.scaler.mean_\n","            if is_sparse:\n","                # Perturb only the non-zero values\n","                non_zero_indexes = data_row.nonzero()[1]\n","                num_cols = len(non_zero_indexes)\n","                instance_sample = data_row[:, non_zero_indexes]\n","                scale = scale[non_zero_indexes]\n","                mean = mean[non_zero_indexes]\n","\n","            if sampling_method == 'gaussian':\n","                data = self.random_state.normal(0, 1, num_samples * num_cols\n","                                                ).reshape(num_samples, num_cols)\n","                data = np.array(data)\n","            elif sampling_method == 'lhs':\n","                data = lhs(num_cols, samples=num_samples\n","                           ).reshape(num_samples, num_cols)\n","                means = np.zeros(num_cols)\n","                stdvs = np.array([1]*num_cols)\n","                for i in range(num_cols):\n","                    data[:, i] = norm(loc=means[i], scale=stdvs[i]).ppf(data[:, i])\n","                data = np.array(data)\n","            else:\n","                warnings.warn('''Invalid input for sampling_method.\n","                                 Defaulting to Gaussian sampling.''', UserWarning)\n","                data = self.random_state.normal(0, 1, num_samples * num_cols\n","                                                ).reshape(num_samples, num_cols)\n","                data = np.array(data)\n","\n","            if self.sample_around_instance:\n","                data = data * scale + instance_sample\n","            else:\n","                data = data * scale + mean\n","            if is_sparse:\n","                if num_cols == 0:\n","                    data = sp.sparse.csr_matrix((num_samples,\n","                                                 data_row.shape[1]),\n","                                                dtype=data_row.dtype)\n","                else:\n","                    indexes = np.tile(non_zero_indexes, num_samples)\n","                    indptr = np.array(\n","                        range(0, len(non_zero_indexes) * (num_samples + 1),\n","                              len(non_zero_indexes)))\n","                    data_1d_shape = data.shape[0] * data.shape[1]\n","                    data_1d = data.reshape(data_1d_shape)\n","                    data = sp.sparse.csr_matrix(\n","                        (data_1d, indexes, indptr),\n","                        shape=(num_samples, data_row.shape[1]))\n","            categorical_features = self.categorical_features\n","            first_row = data_row\n","        else:\n","            first_row = self.discretizer.discretize(data_row.T)[0]\n","        \n","        \n","        data[0] = data_row.copy()[:,0]\n","        inverse = data.copy()\n","        for column in categorical_features:\n","            values = self.feature_values[column]\n","            freqs = self.feature_frequencies[column]\n","            inverse_column = self.random_state.choice(values, size=num_samples,\n","                                                      replace=True, p=freqs)\n","            binary_column = (inverse_column == first_row[column]).astype(int)\n","            binary_column[0] = 1\n","            inverse_column[0] = data[0, column]\n","            data[:, column] = binary_column\n","            inverse[:, column] = inverse_column\n","        if self.discretizer is not None:\n","            inverse[1:] = self.discretizer.undiscretize(inverse[1:])\n","        inverse[0] = data_row[:,0]\n","        # TEXT RANDOMIZATION\n","        doc_size = indexed_string.num_words()\n","        sample_text = self.random_state.randint(1, doc_size + 1, num_samples - 1)\n","        data_text = np.ones((num_samples, doc_size))\n","        data_text[0] = np.ones(doc_size)\n","        features_range_text = range(doc_size)\n","        inverse_data_text = [indexed_string.raw_string()]\n","        for i, size in enumerate(sample_text, start=1):\n","            inactive_text = self.random_state.choice(features_range_text, size,\n","                                                replace=False)\n","            data_text[i, inactive_text] = 0\n","            inverse_data_text.append(indexed_string.inverse_removing(inactive_text))\n","        \n","        return data, inverse, data_text, inverse_data_text"]},{"cell_type":"markdown","metadata":{"id":"baKDFpWqQQDz"},"source":["## LIME Explanation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cajSEuE7hVx5"},"outputs":[],"source":["def heading_clean(text):\n","  text = re.sub(r'\\[\\*\\*(.+?)\\*\\*\\]', \"\", text) # we take out information in brackets\n","  text = re.sub(r'(Admission Date:)|(Discharge Date:)|(Date of Birth:)|(Name:)|(Unit No:)', \"\", text, flags = re.I)\n","  return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHEJaDx3hVx6"},"outputs":[],"source":["class AutogluonWrapper:\n","    def __init__(self, predictor, feature_names):\n","        self.ag_model = predictor\n","        self.feature_names = feature_names\n","    \n","    def predict_binary_prob(self, X):\n","        if isinstance(X, pd.Series):\n","            X = X.values.reshape(1,-1)\n","        if not isinstance(X, pd.DataFrame):\n","            X = pd.DataFrame(X, columns=self.feature_names)\n","        prob = self.ag_model.predict_proba(X, as_multiclass=True)\n","        #prob.columns = [0, 1]\n","        #prob = prob.iloc[:, ::-1]\n","        return np.array(prob)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDx_c2E3_9px"},"outputs":[],"source":["def return_weights(exp):\n","    \n","    \"\"\"Get weights from LIME explanation object\"\"\"\n","    \n","    exp_list = exp.as_list()\n","    exp_weight = [x[1] for x in exp_list]\n","    exp_labels = [x[0] for x in exp_list]\n","    \n","    return exp_weight, exp_labels"]},{"cell_type":"markdown","metadata":{"id":"Xgt-2CiynP1e"},"source":["### Run 1"]},{"cell_type":"markdown","metadata":{"id":"sdkmaR7LkTSG"},"source":["#### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKeOPb_chVx4"},"outputs":[],"source":["# PARAMETERS\n","\n","preprocessing = False # set to true if we want to clean and perform some preprocessing\n","preproc_heavier = False # set to True if we want a heavier preprocessing\n","do_discretization = False # set to True if we want to discretize numerical features\n","model_explained = 'text_mixed' # either \"multimodal\" or \"text_mixed\"\n","lime_discharge_only = True # set to True if we want to explain only discharge note features\n","lemmatization = False # set to True if we want to lemmatize\n","lasso_selection = True # set to True if we want lasso selection\n","\n","\n","preproc_tag_2 = np.where(preproc_heavier, '_heavier', '')\n","preproc_tag = np.where(preprocessing, f'_preproc{preproc_tag_2}', f'{preproc_tag_2}')\n","preproc_tag = np.where(lemmatization, f'{preproc_tag}_lemmatization' , preproc_tag)\n","discretization_tag = np.where(do_discretization, '', '_no_discr')\n","discretization_tag = np.where(lime_discharge_only, '_disch_only', discretization_tag) # if we explain discharge only, we override discretization/not use it\n","discretization_tag = np.where(lasso_selection, discretization_tag, f'{discretization_tag}_no_lasso')\n","\n","\n","feat_selection = np.where(lasso_selection, 'lasso_path', 'auto')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10022,"status":"ok","timestamp":1659884696805,"user":{"displayName":"Luca Adorni","userId":"07135966571450304185"},"user_tz":300},"id":"-UAtd53IhVx5","outputId":"8524f2e2-d447-403d-9609-c4c27b4f4088"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unprocessed Dataframe\n","First quartile = 5.142, Third Quartile = 14.506, Interquartile Interval = 9.365\n","Lower Fence = -8.905, Upper Fence = 28.553\n"]}],"source":["if preprocessing:    \n","  df = pd.read_feather(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/df_los{preproc_tag}')\n","  print('Dataframe Loaded')\n","else:\n","  print('Unprocessed Dataframe')\n","  # import dataset\n","  file = '/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/df_mixed_discharge.csv'\n","  df = pd.read_csv(file, low_memory=False)\n","  # drop the variables to be exempted from the analysis and rename new dataset\n","  df = df.drop(columns = ['Unnamed: 0', 'HADM_ID', 'subject_id','icu_los'])\n","  # selection criterion : only patients 18 and older and with a length of stay or 1 day or greater\n","  df = df.loc[(df['age']>=18) & (df['los']>=1),:]\n","  # check proportion of missing values\n","  missing = pd.DataFrame(df.isna().mean(), columns = ['proportions'])\n","  # drop variables having more than 20 % missing values\n","  df = df.drop(columns=['albumin_min','patientweight','type_stay'])\n","  # save df\n","  df_copy = df.copy()\n","  # impute missing values\n","  df = df_copy.interpolate()\n","  # compute Lower and Upper Fence according to Tukey's criteria\n","  y = df['los']\n","  Q1 = np.percentile(y, 25)\n","  Q3 = np.percentile(y, 75)\n","  IQR = Q3-Q1\n","  LF = Q1 - 1.5*IQR\n","  UF = Q3 + 1.5*IQR\n","  print(f'First quartile = {Q1:.3f}, Third Quartile = {Q3:.3f}, Interquartile Interval = {IQR:.3f}')\n","  print(f'Lower Fence = {LF:.3f}, Upper Fence = {UF:.3f}')\n","  # create categorical LOS variable where prolonged LOS is any value greater than Upper Fence\n","  df['los_cat'] = df['los']> UF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzbX_TJEhVx5"},"outputs":[],"source":["# split the data into training and test\n","df_train, df_test = train_test_split(df, train_size=0.80, stratify = df['los_cat'], random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1659884696806,"user":{"displayName":"Luca Adorni","userId":"07135966571450304185"},"user_tz":300},"id":"F43jesRWhVx6","outputId":"7405ba5c-1d67-4bef-e3cf-9530dcf44bfb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((31284, 49), (7821, 49))"]},"metadata":{},"execution_count":21}],"source":["df_train.shape, df_test.shape"]},{"cell_type":"markdown","metadata":{"id":"zIv4dRh7kRZO"},"source":["#### Load Model"]},{"cell_type":"code","source":["from autogluon.text import TextPredictor"],"metadata":{"id":"MQXeEZFYrYww","executionInfo":{"status":"error","timestamp":1659884697979,"user_tz":300,"elapsed":1184,"user":{"displayName":"Luca Adorni","userId":"07135966571450304185"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"87404ace-017a-4826-b435-ab00db5ae8a3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-773ba8fa46a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautogluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autogluon/text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautomm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtext_prediction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtext_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpresets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_text_presets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autogluon/text/automm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autogluon/text/automm/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDataModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfer_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandaug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autogluon/text/automm/data/datamodule.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningDataModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningDataModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLightningModule\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_stats_monitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceStatsMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_func\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmove_data_to_device\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAllGatherGrad\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m from pytorch_lightning.utilities.enums import (  # noqa: F401\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/apply_func.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compare_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TORCHTEXT_LEGACY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrank_zero_deprecation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/imports.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0m_TORCH_QUANTIZE_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_engines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meg\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0m_TORCHTEXT_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0m_TORCHTEXT_LEGACY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TORCHTEXT_AVAILABLE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_compare_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0.11.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0m_TORCHVISION_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0m_WANDB_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wandb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/imports.py\u001b[0m in \u001b[0;36m_compare_version\u001b[0;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mpkg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the following import has to happen first in order to load the torchtext C++ library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_extension\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m_TEXT_BUCKET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://download.pytorch.org/models/text/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0m_init_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext C++ Extension is not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libtorchtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m# This import is for initializing the methods registered via PyBind11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# This has to happen after the base library is loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: /usr/local/lib/python3.7/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN3c1022getCustomClassTypeImplERKSt10type_index"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7zHTc_vhVx6"},"outputs":[],"source":["\n","# paramètres du modèle\n","# paramètres du modèle\n","# paramètres du modèle\n","if preprocessing:\n","  save_path = f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/models/{model_explained}{preproc_tag}'\n","  print('With Preprocessing')\n","else:\n","  save_path = f'/content/drive/MyDrive/AutoGluon/models/{model_explained}_2022-05-29'\n","  #save_path = f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/models/{model_explained}{preproc_tag}'\n","  print('Without Preprocessing')\n","  \n","if model_explained == 'multimodal':\n","  from autogluon.tabular import TabularPredictor\n","  predictor = TabularPredictor.load(save_path, require_version_match = False)\n","elif model_explained == 'text_mixed':\n","  from autogluon.text import TextPredictor\n","  predictor = TextPredictor.load(save_path)"]},{"cell_type":"code","source":["class AutogluonWrapper:\n","    def __init__(self, predictor, feature_names):\n","        self.ag_model = predictor\n","        self.feature_names = feature_names\n","    \n","    def predict_binary_prob(self, X):\n","        if isinstance(X, pd.Series):\n","            X = X.values.reshape(1,-1)\n","        if not isinstance(X, pd.DataFrame):\n","            X = pd.DataFrame(X, columns=self.feature_names)\n","        prob = self.ag_model.predict_proba(X, as_multiclass=True)\n","        #prob.columns = [0, 1]\n","        #prob = prob.iloc[:, ::-1]\n","        return np.array(prob)"],"metadata":{"id":"us_FilhozFpR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQr0RG7tomtG"},"outputs":[],"source":["x_test = df_test.drop(\"los_cat\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnKOq7YDhVx7"},"outputs":[],"source":["autogluon_wrap = AutogluonWrapper(predictor, list(x_test.columns))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtImPXY0orKq"},"outputs":[],"source":["autogluon_wrap.predict_binary_prob(x_test.iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"tHW9afVtkPc-"},"source":["#### LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wzE4iGwQUp3"},"outputs":[],"source":["# load weights\n","try :\n","    with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'rb') as handle:\n","        weights = pickle.load(handle)\n","        print('Weights loaded')\n","        print(len(weights))\n","except :\n","    weights = [] # initialize an empty dictionary if no existing file is present\n","    print('New Weight List')\n","# load labels\n","try :\n","    with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'rb') as handle:\n","        labels = pickle.load(handle)\n","        print('Label Loaded')\n","        print(len(labels))\n","except :\n","    labels = [] # initialize an empty dictionary if no existing file is present\n","    print('New Label List')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"au3lS9hCpFus"},"outputs":[],"source":["f'{model_explained}_weights{preproc_tag}{discretization_tag}'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ir6XLvfpXavi"},"outputs":[],"source":["x_train = df_train.drop(\"los_cat\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9ijaN0wNDb8"},"outputs":[],"source":["if lime_discharge_only:\n","  lime_explainer = LimeMultimodalExplainer(class_names = ['short', 'long'], random_state = 42, feature_selection = feat_selection)\n","else:\n","  lime_explainer = LimeTextMixed(x_train, categorical_features = x_train.select_dtypes('object').columns, text_features = 'discharge',class_names = ['short', 'long'],\n","                               random_state = 42, feature_selection = feat_selection, discretize_continuous = do_discretization)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLTkVvOcXZ3H"},"outputs":[],"source":["prova2 =lime_explainer.explain_instance(x_test.iloc[0,:], predict_fn = autogluon_wrap.predict_binary_prob, num_samples = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CutsDXx5xE_z"},"outputs":[],"source":["prova2.as_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siC2z8l0wWcc"},"outputs":[],"source":["minimum = len(weights)  # beginning of our list\n","maximum = len(x_test) # end of our list len(user_list)\n","steps = 500 # how many users per batch\n","range_list = list(np.arange(minimum,maximum,steps))\n","\n","iter_count = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogMCV5vrwd9I"},"outputs":[],"source":["if maximum != len(weights):\n","  for i in range(len(range_list)):\n","    min_r = range_list[i]\n","    try:\n","      max_r = range_list[i+1]\n","    except:\n","      max_r = maximum\n","    print(\"Range: {} to {}\".format(min_r, max_r))\n","    batch_pred = x_test.iloc[min_r:max_r]\n","    #Iterate over first 100 rows in feature matrix\n","    for index, row in batch_pred.iterrows():\n","        \n","        #Get explanation\n","        exp = lime_explainer.explain_instance(row, \n","                                    autogluon_wrap.predict_binary_prob, \n","                                    num_features=100,\n","                                    num_samples = 500)\n","        \n","        #Get weights\n","        exp_weight, exp_labels = return_weights(exp)\n","        weights.append(exp_weight)\n","        labels.append(exp_labels)\n","    iter_count += steps\n","    if iter_count % 500 == 0 and max_r != maximum:\n","      # save the file\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(weights, f)\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(labels, f)\n","      print(\"Output Saved\")\n","    if max_r == maximum:\n","      # save the file\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(weights, f)\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(labels, f)\n","      print(\"Output Saved, Maximum Reached\")"]},{"cell_type":"markdown","metadata":{"id":"OPEibVHNnM04"},"source":["### Run 2"]},{"cell_type":"markdown","metadata":{"id":"wMPeRhTHnLIL"},"source":["#### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOuUYiNwnLIL"},"outputs":[],"source":["# PARAMETERS\n","\n","preprocessing = True # set to true if we want to clean and perform some preprocessing\n","preproc_heavier = False # set to True if we want a heavier preprocessing\n","do_discretization = False # set to True if we want to discretize numerical features\n","model_explained = 'text_mixed' # either \"multimodal\" or \"text_mixed\"\n","lime_discharge_only = True # set to True if we want to explain only discharge note features\n","lemmatization = False # set to True if we want to lemmatize\n","lasso_selection = True # set to True if we want lasso selection\n","\n","\n","preproc_tag_2 = np.where(preproc_heavier, '_heavier', '')\n","preproc_tag = np.where(preprocessing, f'_preproc{preproc_tag_2}', f'{preproc_tag_2}')\n","preproc_tag = np.where(lemmatization, f'{preproc_tag}_lemmatization' , preproc_tag)\n","discretization_tag = np.where(do_discretization, '', '_no_discr')\n","discretization_tag = np.where(lime_discharge_only, '_disch_only', discretization_tag) # if we explain discharge only, we override discretization/not use it\n","discretization_tag = np.where(lasso_selection, discretization_tag, f'{discretization_tag}_no_lasso')\n","\n","\n","feat_selection = np.where(lasso_selection, 'lasso_path', 'auto')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irNuNJZ4nLIM"},"outputs":[],"source":["if preprocessing:    \n","  df = pd.read_feather(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/df_los{preproc_tag}')\n","  print('Dataframe Loaded')\n","else:\n","  print('Unprocessed Dataframe')\n","  # import dataset\n","  file = '/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/df_mixed_discharge.csv'\n","  df = pd.read_csv(file, low_memory=False)\n","  # drop the variables to be exempted from the analysis and rename new dataset\n","  df = df.drop(columns = ['Unnamed: 0', 'HADM_ID', 'subject_id','icu_los'])\n","  # selection criterion : only patients 18 and older and with a length of stay or 1 day or greater\n","  df = df.loc[(df['age']>=18) & (df['los']>=1),:]\n","  # check proportion of missing values\n","  missing = pd.DataFrame(df.isna().mean(), columns = ['proportions'])\n","  # drop variables having more than 20 % missing values\n","  df = df.drop(columns=['albumin_min','patientweight','type_stay'])\n","  # save df\n","  df_copy = df.copy()\n","  # impute missing values\n","  df = df_copy.interpolate()\n","  # compute Lower and Upper Fence according to Tukey's criteria\n","  y = df['los']\n","  Q1 = np.percentile(y, 25)\n","  Q3 = np.percentile(y, 75)\n","  IQR = Q3-Q1\n","  LF = Q1 - 1.5*IQR\n","  UF = Q3 + 1.5*IQR\n","  print(f'First quartile = {Q1:.3f}, Third Quartile = {Q3:.3f}, Interquartile Interval = {IQR:.3f}')\n","  print(f'Lower Fence = {LF:.3f}, Upper Fence = {UF:.3f}')\n","  # create categorical LOS variable where prolonged LOS is any value greater than Upper Fence\n","  df['los_cat'] = df['los']> UF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DeCprckKnLIM"},"outputs":[],"source":["# split the data into training and test\n","df_train, df_test = train_test_split(df, train_size=0.80, stratify = df['los_cat'], random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTY7Rm39nLIN"},"outputs":[],"source":["df_train.shape, df_test.shape"]},{"cell_type":"markdown","metadata":{"id":"F7RV9kDhnLIN"},"source":["#### Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pvBLaiRRnLIN"},"outputs":[],"source":["\n","# paramètres du modèle\n","# paramètres du modèle\n","# paramètres du modèle\n","if preprocessing:\n","  save_path = f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/models/{model_explained}{preproc_tag}'\n","  print('With Preprocessing')\n","else:\n","  save_path = f'/content/drive/MyDrive/AutoGluon/models/{model_explained}_2022-05-29'\n","  save_path = f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/models/{model_explained}{preproc_tag}'\n","  print('Without Preprocessing')\n","  \n","if model_explained == 'multimodal':\n","  from autogluon.tabular import TabularPredictor\n","  predictor = TabularPredictor.load(save_path, require_version_match = False)\n","if model_explained == 'text_mixed':\n","  from autogluon.text import TextPredictor\n","  predictor = TextPredictor.load(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3eGVZUoo1qg"},"outputs":[],"source":["x_test = df_test.drop(\"los_cat\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yW8hHTCwo1qh"},"outputs":[],"source":["autogluon_wrap = AutogluonWrapper(predictor, list(x_test.columns))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkS7OnrZo1qh"},"outputs":[],"source":["autogluon_wrap.predict_binary_prob(x_test.iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"gFsDdVignLIO"},"source":["#### LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tr8xAuGenLIO"},"outputs":[],"source":["# load weights\n","try :\n","    with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'rb') as handle:\n","        weights = pickle.load(handle)\n","        print('Weights loaded')\n","        print(len(weights))\n","except :\n","    weights = [] # initialize an empty dictionary if no existing file is present\n","    print('New Weight List')\n","# load labels\n","try :\n","    with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'rb') as handle:\n","        labels = pickle.load(handle)\n","        print('Label Loaded')\n","        print(len(labels))\n","except :\n","    labels = [] # initialize an empty dictionary if no existing file is present\n","    print('New Label List')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmgPYR-gnLIP"},"outputs":[],"source":["x_train = df_train.drop(\"los_cat\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sdf0kLmnLIP"},"outputs":[],"source":["if lime_discharge_only:\n","  lime_explainer = LimeMultimodalExplainer(class_names = ['short', 'long'], random_state = 42, feature_selection = feat_selection)\n","else:\n","  lime_explainer = LimeTextMixed(x_train, categorical_features = x_train.select_dtypes('object').columns, text_features = 'discharge',class_names = ['short', 'long'],\n","                               random_state = 42, feature_selection = feat_selection, discretize_continuous = do_discretization)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DgvnkrRnLIP"},"outputs":[],"source":["prova2 =lime_explainer.explain_instance(x_test.iloc[0,:], predict_fn = autogluon_wrap.predict_binary_prob, num_samples = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOfC8ki5nLIQ"},"outputs":[],"source":["prova2.as_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-JoQ4-RFnLIQ"},"outputs":[],"source":["minimum = len(weights)  # beginning of our list\n","maximum = len(x_test) # end of our list len(user_list)\n","steps = 500 # how many users per batch\n","range_list = list(np.arange(minimum,maximum,steps))\n","\n","iter_count = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUxtULZ9nLIQ"},"outputs":[],"source":["if maximum != len(weights):\n","  for i in range(len(range_list)):\n","    min_r = range_list[i]\n","    try:\n","      max_r = range_list[i+1]\n","    except:\n","      max_r = maximum\n","    print(\"Range: {} to {}\".format(min_r, max_r))\n","    batch_pred = x_test.iloc[min_r:max_r]\n","    #Iterate over first 100 rows in feature matrix\n","    for index, row in batch_pred.iterrows():\n","        \n","        #Get explanation\n","        exp = lime_explainer.explain_instance(row, \n","                                    autogluon_wrap.predict_binary_prob, \n","                                    num_features=100,\n","                                    num_samples = 500)\n","        \n","        #Get weights\n","        exp_weight, exp_labels = return_weights(exp)\n","        weights.append(exp_weight)\n","        labels.append(exp_labels)\n","    iter_count += steps\n","    if iter_count % 500 == 0 and max_r != maximum:\n","      # save the file\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(weights, f)\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(labels, f)\n","      print(\"Output Saved\")\n","    if max_r == maximum:\n","      # save the file\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(weights, f)\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(labels, f)\n","      print(\"Output Saved, Maximum Reached\")"]},{"cell_type":"markdown","metadata":{"id":"-0Epk5NHnYnR"},"source":["### Run 3"]},{"cell_type":"markdown","metadata":{"id":"D2XMeJb9nYnR"},"source":["#### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9G2d_yrnYnR"},"outputs":[],"source":["# PARAMETERS\n","\n","preprocessing = True # set to true if we want to clean and perform some preprocessing\n","preproc_heavier = True # set to True if we want a heavier preprocessing\n","do_discretization = False # set to True if we want to discretize numerical features\n","model_explained = 'text_mixed' # either \"multimodal\" or \"text_mixed\"\n","lime_discharge_only = True # set to True if we want to explain only discharge note features\n","lemmatization = False  # set to True if we want to lemmatize\n","lasso_selection = True # set to True if we want lasso selection\n","\n","\n","preproc_tag_2 = np.where(preproc_heavier, '_heavier', '')\n","preproc_tag = np.where(preprocessing, f'_preproc{preproc_tag_2}', f'{preproc_tag_2}')\n","preproc_tag = np.where(lemmatization, f'{preproc_tag}_lemmatization' , preproc_tag)\n","discretization_tag = np.where(do_discretization, '', '_no_discr')\n","discretization_tag = np.where(lime_discharge_only, '_disch_only', discretization_tag) # if we explain discharge only, we override discretization/not use it\n","discretization_tag = np.where(lasso_selection, discretization_tag, f'{discretization_tag}_no_lasso')\n","\n","\n","feat_selection = np.where(lasso_selection, 'lasso_path', 'auto')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0lJyqIqnYnS"},"outputs":[],"source":["if preprocessing:    \n","  df = pd.read_feather(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/df_los{preproc_tag}')\n","  print('Dataframe Loaded')\n","else:\n","  print('Unprocessed Dataframe')\n","  # import dataset\n","  file = '/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/df_mixed_discharge.csv'\n","  df = pd.read_csv(file, low_memory=False)\n","  # drop the variables to be exempted from the analysis and rename new dataset\n","  df = df.drop(columns = ['Unnamed: 0', 'HADM_ID', 'subject_id','icu_los'])\n","  # selection criterion : only patients 18 and older and with a length of stay or 1 day or greater\n","  df = df.loc[(df['age']>=18) & (df['los']>=1),:]\n","  # check proportion of missing values\n","  missing = pd.DataFrame(df.isna().mean(), columns = ['proportions'])\n","  # drop variables having more than 20 % missing values\n","  df = df.drop(columns=['albumin_min','patientweight','type_stay'])\n","  # save df\n","  df_copy = df.copy()\n","  # impute missing values\n","  df = df_copy.interpolate()\n","  # compute Lower and Upper Fence according to Tukey's criteria\n","  y = df['los']\n","  Q1 = np.percentile(y, 25)\n","  Q3 = np.percentile(y, 75)\n","  IQR = Q3-Q1\n","  LF = Q1 - 1.5*IQR\n","  UF = Q3 + 1.5*IQR\n","  print(f'First quartile = {Q1:.3f}, Third Quartile = {Q3:.3f}, Interquartile Interval = {IQR:.3f}')\n","  print(f'Lower Fence = {LF:.3f}, Upper Fence = {UF:.3f}')\n","  # create categorical LOS variable where prolonged LOS is any value greater than Upper Fence\n","  df['los_cat'] = df['los']> UF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGz42S0QnYnS"},"outputs":[],"source":["# split the data into training and test\n","df_train, df_test = train_test_split(df, train_size=0.80, stratify = df['los_cat'], random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9YQQaE9nYnT"},"outputs":[],"source":["df_train.shape, df_test.shape"]},{"cell_type":"markdown","metadata":{"id":"mqn_fAMZnYnT"},"source":["#### Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VH9nzQprnYnT"},"outputs":[],"source":["\n","# paramètres du modèle\n","# paramètres du modèle\n","# paramètres du modèle\n","if preprocessing:\n","  save_path = f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/models/{model_explained}{preproc_tag}'\n","  print('With Preprocessing')\n","else:\n","  save_path = f'/content/drive/MyDrive/AutoGluon/models/{model_explained}_2022-05-29'\n","  save_path = f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/models/{model_explained}{preproc_tag}'\n","  print('Without Preprocessing')\n","  \n","if model_explained == 'multimodal':\n","  from autogluon.tabular import TabularPredictor\n","  predictor = TabularPredictor.load(save_path, require_version_match = False)\n","if model_explained == 'text_mixed':\n","  from autogluon.text import TextPredictor\n","  predictor = TextPredictor.load(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDHQ214_o3rT"},"outputs":[],"source":["x_test = df_test.drop(\"los_cat\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JozWEl8io3rT"},"outputs":[],"source":["autogluon_wrap = AutogluonWrapper(predictor, list(x_test.columns))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYciSuySo3rU"},"outputs":[],"source":["autogluon_wrap.predict_binary_prob(x_test.iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"OZRN4uSWnYnU"},"source":["#### LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wssWlYBInYnU"},"outputs":[],"source":["# load weights\n","try :\n","    with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'rb') as handle:\n","        weights = pickle.load(handle)\n","        print('Weights loaded')\n","        print(len(weights))\n","except :\n","    weights = [] # initialize an empty dictionary if no existing file is present\n","    print('New Weight List')\n","# load labels\n","try :\n","    with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'rb') as handle:\n","        labels = pickle.load(handle)\n","        print('Label Loaded')\n","        print(len(labels))\n","except :\n","    labels = [] # initialize an empty dictionary if no existing file is present\n","    print('New Label List')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7T3ej_5nYnU"},"outputs":[],"source":["x_train = df_train.drop(\"los_cat\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L76LoP0xnYnV"},"outputs":[],"source":["if lime_discharge_only:\n","  lime_explainer = LimeMultimodalExplainer(class_names = ['short', 'long'], random_state = 42, feature_selection = feat_selection)\n","else:\n","  lime_explainer = LimeTextMixed(x_train, categorical_features = x_train.select_dtypes('object').columns, text_features = 'discharge',class_names = ['short', 'long'],\n","                               random_state = 42, feature_selection = feat_selection, discretize_continuous = do_discretization)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBfIOuUjnYnV"},"outputs":[],"source":["prova2 =lime_explainer.explain_instance(x_test.iloc[0,:], predict_fn = autogluon_wrap.predict_binary_prob, num_samples = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ro_deTiKnYnV"},"outputs":[],"source":["prova2.as_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pgr9qaVUnYnW"},"outputs":[],"source":["minimum = len(weights)  # beginning of our list\n","maximum = len(x_test) # end of our list len(user_list)\n","steps = 500 # how many users per batch\n","range_list = list(np.arange(minimum,maximum,steps))\n","\n","iter_count = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-HjthRKCnYnW"},"outputs":[],"source":["if maximum != len(weights):\n","  for i in range(len(range_list)):\n","    min_r = range_list[i]\n","    try:\n","      max_r = range_list[i+1]\n","    except:\n","      max_r = maximum\n","    print(\"Range: {} to {}\".format(min_r, max_r))\n","    batch_pred = x_test.iloc[min_r:max_r]\n","    #Iterate over first 100 rows in feature matrix\n","    for index, row in batch_pred.iterrows():\n","        \n","        #Get explanation\n","        exp = lime_explainer.explain_instance(row, \n","                                    autogluon_wrap.predict_binary_prob, \n","                                    num_features=100,\n","                                    num_samples = 500)\n","        \n","        #Get weights\n","        exp_weight, exp_labels = return_weights(exp)\n","        weights.append(exp_weight)\n","        labels.append(exp_labels)\n","    iter_count += steps\n","    if iter_count % 500 == 0 and max_r != maximum:\n","      # save the file\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(weights, f)\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(labels, f)\n","      print(\"Output Saved\")\n","    if max_r == maximum:\n","      # save the file\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(weights, f)\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(labels, f)\n","      print(\"Output Saved, Maximum Reached\")"]},{"cell_type":"markdown","metadata":{"id":"XW3M1YpW--vp"},"source":["### Run 4"]},{"cell_type":"markdown","metadata":{"id":"FwocPX7X--vp"},"source":["#### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbkLzyzF--vp"},"outputs":[],"source":["# PARAMETERS\n","\n","preprocessing = True # set to true if we want to clean and perform some preprocessing\n","preproc_heavier = True # set to True if we want a heavier preprocessing\n","do_discretization = False # set to True if we want to discretize numerical features\n","model_explained = 'text_mixed' # either \"multimodal\" or \"text_mixed\"\n","lime_discharge_only = True # set to True if we want to explain only discharge note features\n","lemmatization = True # set to True if we want to lemmatize\n","lasso_selection = True # set to True if we want lasso selection\n","\n","\n","preproc_tag_2 = np.where(preproc_heavier, '_heavier', '')\n","preproc_tag = np.where(preprocessing, f'_preproc{preproc_tag_2}', f'{preproc_tag_2}')\n","preproc_tag = np.where(lemmatization, f'{preproc_tag}_lemmatization' , preproc_tag)\n","discretization_tag = np.where(do_discretization, '', '_no_discr')\n","discretization_tag = np.where(lime_discharge_only, '_disch_only', discretization_tag) # if we explain discharge only, we override discretization/not use it\n","discretization_tag = np.where(lasso_selection, discretization_tag, f'{discretization_tag}_no_lasso')\n","\n","\n","feat_selection = np.where(lasso_selection, 'lasso_path', 'auto')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jO8uPMWF--vp"},"outputs":[],"source":["if preprocessing:    \n","  df = pd.read_feather(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/df_los{preproc_tag}')\n","  print('Dataframe Loaded')\n","else:\n","  print('Unprocessed Dataframe')\n","  # import dataset\n","  file = '/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/df_mixed_discharge.csv'\n","  df = pd.read_csv(file, low_memory=False)\n","  # drop the variables to be exempted from the analysis and rename new dataset\n","  df = df.drop(columns = ['Unnamed: 0', 'HADM_ID', 'subject_id','icu_los'])\n","  # selection criterion : only patients 18 and older and with a length of stay or 1 day or greater\n","  df = df.loc[(df['age']>=18) & (df['los']>=1),:]\n","  # check proportion of missing values\n","  missing = pd.DataFrame(df.isna().mean(), columns = ['proportions'])\n","  # drop variables having more than 20 % missing values\n","  df = df.drop(columns=['albumin_min','patientweight','type_stay'])\n","  # save df\n","  df_copy = df.copy()\n","  # impute missing values\n","  df = df_copy.interpolate()\n","  # compute Lower and Upper Fence according to Tukey's criteria\n","  y = df['los']\n","  Q1 = np.percentile(y, 25)\n","  Q3 = np.percentile(y, 75)\n","  IQR = Q3-Q1\n","  LF = Q1 - 1.5*IQR\n","  UF = Q3 + 1.5*IQR\n","  print(f'First quartile = {Q1:.3f}, Third Quartile = {Q3:.3f}, Interquartile Interval = {IQR:.3f}')\n","  print(f'Lower Fence = {LF:.3f}, Upper Fence = {UF:.3f}')\n","  # create categorical LOS variable where prolonged LOS is any value greater than Upper Fence\n","  df['los_cat'] = df['los']> UF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnzPVx09--vp"},"outputs":[],"source":["# split the data into training and test\n","df_train, df_test = train_test_split(df, train_size=0.80, stratify = df['los_cat'], random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuQg4klO--vq"},"outputs":[],"source":["df_train.shape, df_test.shape"]},{"cell_type":"markdown","metadata":{"id":"uRHF1tZA--vq"},"source":["#### Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWOh2MWI--vq"},"outputs":[],"source":["\n","# paramètres du modèle\n","# paramètres du modèle\n","# paramètres du modèle\n","if preprocessing:\n","  save_path = f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/models/{model_explained}{preproc_tag}'\n","  print('With Preprocessing')\n","else:\n","  save_path = f'/content/drive/MyDrive/AutoGluon/models/{model_explained}_2022-05-29'\n","  save_path = f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/models/{model_explained}{preproc_tag}'\n","  print('Without Preprocessing')\n","  \n","if model_explained == 'multimodal':\n","  from autogluon.tabular import TabularPredictor\n","  predictor = TabularPredictor.load(save_path, require_version_match = False)\n","if model_explained == 'text_mixed':\n","  from autogluon.text import TextPredictor\n","  predictor = TextPredictor.load(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrymHIdo--vq"},"outputs":[],"source":["x_test = df_test.drop(\"los_cat\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SySNDcDw--vq"},"outputs":[],"source":["autogluon_wrap = AutogluonWrapper(predictor, list(x_test.columns))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsIh1T-U--vq"},"outputs":[],"source":["autogluon_wrap.predict_binary_prob(x_test.iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"DJI-258d--vq"},"source":["#### LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRQAdeEv--vq"},"outputs":[],"source":["# load weights\n","try :\n","    with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'rb') as handle:\n","        weights = pickle.load(handle)\n","        print('Weights loaded')\n","        print(len(weights))\n","except :\n","    weights = [] # initialize an empty dictionary if no existing file is present\n","    print('New Weight List')\n","# load labels\n","try :\n","    with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'rb') as handle:\n","        labels = pickle.load(handle)\n","        print('Label Loaded')\n","        print(len(labels))\n","except :\n","    labels = [] # initialize an empty dictionary if no existing file is present\n","    print('New Label List')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3ZZYe_H--vq"},"outputs":[],"source":["x_train = df_train.drop(\"los_cat\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDw8EG0J--vq"},"outputs":[],"source":["if lime_discharge_only:\n","  lime_explainer = LimeMultimodalExplainer(class_names = ['short', 'long'], random_state = 42, feature_selection = feat_selection)\n","else:\n","  lime_explainer = LimeTextMixed(x_train, categorical_features = x_train.select_dtypes('object').columns, text_features = 'discharge',class_names = ['short', 'long'],\n","                               random_state = 42, feature_selection = feat_selection, discretize_continuous = do_discretization)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45cVLbov--vq"},"outputs":[],"source":["prova2 =lime_explainer.explain_instance(x_test.iloc[0,:], predict_fn = autogluon_wrap.predict_binary_prob, num_samples = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3ymToRp--vq"},"outputs":[],"source":["prova2.as_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_VkgT7W--vq"},"outputs":[],"source":["minimum = len(weights)  # beginning of our list\n","maximum = len(x_test) # end of our list len(user_list)\n","steps = 500 # how many users per batch\n","range_list = list(np.arange(minimum,maximum,steps))\n","\n","iter_count = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HwYrcgky--vr"},"outputs":[],"source":["if maximum != len(weights):\n","  for i in range(len(range_list)):\n","    min_r = range_list[i]\n","    try:\n","      max_r = range_list[i+1]\n","    except:\n","      max_r = maximum\n","    print(\"Range: {} to {}\".format(min_r, max_r))\n","    batch_pred = x_test.iloc[min_r:max_r]\n","    #Iterate over first 100 rows in feature matrix\n","    for index, row in batch_pred.iterrows():\n","        \n","        #Get explanation\n","        exp = lime_explainer.explain_instance(row, \n","                                    autogluon_wrap.predict_binary_prob, \n","                                    num_features=100,\n","                                    num_samples = 500)\n","        \n","        #Get weights\n","        exp_weight, exp_labels = return_weights(exp)\n","        weights.append(exp_weight)\n","        labels.append(exp_labels)\n","    iter_count += steps\n","    if iter_count % 500 == 0 and max_r != maximum:\n","      # save the file\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(weights, f)\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(labels, f)\n","      print(\"Output Saved\")\n","    if max_r == maximum:\n","      # save the file\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_weights{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(weights, f)\n","      with open(f'/content/drive/MyDrive/MIMIC-III Text Mining/LOS/data/feature_importance/{model_explained}_labels{preproc_tag}{discretization_tag}.pkl', 'wb') as f:\n","          pickle.dump(labels, f)\n","      print(\"Output Saved, Maximum Reached\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"[RUN A] 06_LIME_Expansion.ipynb","provenance":[],"machine_shape":"hm"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}