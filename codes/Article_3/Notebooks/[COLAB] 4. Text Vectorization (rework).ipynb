{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52faebb4",
   "metadata": {
    "id": "52faebb4"
   },
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fc56254",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3273,
     "status": "ok",
     "timestamp": 1646427938684,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "5fc56254",
    "outputId": "757e09ba-b490-4ead-e5a4-fa1d50d204db"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import utils\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "871960c0",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1646427938685,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "871960c0"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "icu_stays = True # set to TRUE if we want to have only ICU stays\n",
    "med_7 = True # set to false if we want to avoid using Med7 preprocessing\n",
    "\n",
    "if med_7 == False: \n",
    "    tag_med7 = '_nomed7'\n",
    "else:\n",
    "    tag_med7 = ''\n",
    "    \n",
    "if icu_stays == True:\n",
    "    tag_icu = '_icu'\n",
    "    icu_folder = 'icu_only'\n",
    "else:\n",
    "    tag_icu = ''\n",
    "    icu_folder = 'all_hosp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bef05b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1740,
     "status": "ok",
     "timestamp": 1646427940414,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "4bef05b5",
    "outputId": "451cd2d8-0601-4abb-aaf6-56906fc7b182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "if IN_COLAB:  \n",
    "  # Mount the Google Drive at mount\n",
    "  mount='/content/gdrive'\n",
    "  print(\"Colab: mounting Google drive on \", mount)\n",
    "  # connect your colab with the drive\n",
    "  drive.mount(mount)\n",
    "\n",
    " # Switch to the directory on the Google Drive that you want to use\n",
    "  import os\n",
    "  path_to_repo = mount + \"/My Drive/MIMIC-III Text Mining\"\n",
    "\n",
    "else:\n",
    "  path_to_repo = os.path.dirname(os.getcwd())\n",
    "\n",
    "  \n",
    "print(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8c67129",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1646427940415,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "b8c67129",
    "outputId": "9ac46762-d672-4c74-8158-7275aef93748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\\Readmission\\data\\icu_only\\\n"
     ]
    }
   ],
   "source": [
    "path_to_data = os.path.join(path_to_repo,\"Readmission\",\"data\", icu_folder,\"\")\n",
    "print(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32f9a910",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1646427940416,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "32f9a910",
    "outputId": "880776e5-6560-469c-984f-a898d5276dc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\\Readmission\\data\\icu_only\\processed\\\n"
     ]
    }
   ],
   "source": [
    "path_to_processed = os.path.join(path_to_data,\"processed\",\"\")\n",
    "os.makedirs(path_to_processed, exist_ok=True) # we create the directory if it does not exist\n",
    "print(path_to_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e602728c",
   "metadata": {
    "executionInfo": {
     "elapsed": 7832,
     "status": "ok",
     "timestamp": 1646427948241,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "e602728c"
   },
   "outputs": [],
   "source": [
    "# load it back\n",
    "df_final = pd.read_feather(f'{path_to_data}df_stem{tag_med7}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8a0fd",
   "metadata": {
    "id": "45e8a0fd"
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0feac56",
   "metadata": {
    "id": "b0feac56"
   },
   "source": [
    "First we need to split our dataset into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b48bf8b",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1646427948242,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "6b48bf8b"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "session_seed = 42 # set seed for our session\n",
    "include_diag = True # set to True if we want to also process the diagnosis column\n",
    "include_test = True # set to True if we want to also process the test set\n",
    "test_proportion = 0.2\n",
    "val_proportion = 0.1\n",
    "train_proportion = 1 - test_proportion - val_proportion\n",
    "\n",
    "random.seed(session_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c09492",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646427948242,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "15c09492",
    "outputId": "b31b613c-fba9-462d-ea72-6e94b315d3a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.7 ms\n",
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "%time train, test = train_test_split(df_final, test_size = test_proportion, random_state = session_seed, stratify = df_final.target)\n",
    "# furtherly split into validation and train\n",
    "%time train, val = train_test_split(train, test_size = val_proportion, random_state = session_seed, stratify = train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e624ab7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1646427045014,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "3e624ab7",
    "outputId": "499b1a5a-9ae3-4844-a9bd-478077cc07ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:(6662, 21)\n",
      "Val:(2665, 21)\n",
      "Train:(23982, 21)\n"
     ]
    }
   ],
   "source": [
    "print('Test:{}'.format(test.shape))\n",
    "print('Val:{}'.format(val.shape))\n",
    "print('Train:{}'.format(train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f983fa",
   "metadata": {
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1646427045310,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "03f983fa"
   },
   "outputs": [],
   "source": [
    "def corpus_to_df(corpus, dictionary):\n",
    "    \"\"\"\n",
    "    Utility to transofmr the corpus to a dataframe\n",
    "    \"\"\"\n",
    "    corpus_matrix = gensim.matutils.corpus2dense(corpus, num_terms = len(dictionary)).T\n",
    "    print(corpus_matrix.shape)\n",
    "\n",
    "    df_corpus = pd.DataFrame(corpus_matrix, columns = dictionary.values())\n",
    "    return df_corpus\n",
    "    \n",
    "    \n",
    "def gensim_vectorizer(df_clean, id2word, method = 'frequency'):# Create Dictionary\n",
    "    \"\"\"\n",
    "    Create our dataframe through the usage of gensim\n",
    "    id2word : dictionary from our training dataset\n",
    "    method : additional argument to perform one hot or TF-IDF encoding instead of frequency encoding,\n",
    "    arguments are 'frequency', 'one_hot', 'tf_idf'\n",
    "    \"\"\"\n",
    "   \n",
    "    if method == 'frequency':\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in df_clean]\n",
    "    elif method == 'one_hot':\n",
    "        # One Hot Encoding\n",
    "        corpus = [[(token[0],1) for token in id2word.doc2bow(doc)]for doc in df_clean]\n",
    "    elif method == 'tf_idf':\n",
    "        tfidf = gensim.models.TfidfModel(dictionary=id2word, normalize=True)\n",
    "        corpus = [tfidf[id2word.doc2bow(doc)] for doc in df_clean]\n",
    "\n",
    "    # We transform our corpus to a numpy matrix - WE NEED TO TRANSPOSE IT\n",
    "    df_corpus = corpus_to_df(corpus, dictionary = id2word)\n",
    "    return df_corpus, corpus\n",
    "\n",
    "def gensim_vectorize_diagnosis(df, id2word, include_diag = True, id2word_diag = '', method = 'frequency'):\n",
    "    \"\"\" \n",
    "    Function to performe vectorization of our dataframe\n",
    "    df: our dataframe\n",
    "    id2word: main dictionary from the full text\n",
    "    include_diag: set to True if we want to include vectorization of our diagnosis column\n",
    "    id2word_diag: dictionary of the diagnosis column\n",
    "    method: see gensim_vectorizer\n",
    "    \"\"\"\n",
    "    df_vectorized, _ = gensim_vectorizer(df.text_def, id2word, method = method)\n",
    "    if include_diag == True:\n",
    "        df_vectorized_diagnosis, _ = gensim_vectorizer(df.diagnosis_def, id2word_diag, method = method)\n",
    "        # then merge the two datasets\n",
    "        df_vectorized = df_vectorized.merge(df_vectorized_diagnosis, left_index = True, right_index = True, suffixes = (\"\",\"_diag\"))\n",
    "    return df_vectorized\n",
    "\n",
    "\n",
    "def gensim_traintest_vectorizer(train, val, id2word, include_diag = True, id2word_diag = '', method = 'frequency', include_test = True, test = ''):\n",
    "    \"\"\"\n",
    "    Function to perform joint vectorization of train, validation and test set\n",
    "    train: our training dataset\n",
    "    val: our validation dataset\n",
    "    id2word: dictionary of our main final text column\n",
    "    include_diag: set to True if we want to include vectorization of our diagnosis column\n",
    "    id2word_diag: dictionary of the diagnosis column\n",
    "    method: see gensim_vectorizer\n",
    "    include_test: set to True if we want to also perform vectorization of our test set\n",
    "    test: our test set\n",
    "    \"\"\"\n",
    "    train_vectorized = gensim_vectorize_diagnosis(train, id2word, include_diag = include_diag, id2word_diag = id2word_diag, method = method)\n",
    "    val_vectorized = gensim_vectorize_diagnosis(val, id2word, include_diag = include_diag, id2word_diag = id2word_diag, method = method)\n",
    "    if include_test == True:\n",
    "        test_vectorized = gensim_vectorize_diagnosis(test, id2word, include_diag = include_diag, id2word_diag = id2word_diag, method = method)\n",
    "    else:\n",
    "        # empty list\n",
    "        test_vectorized = []\n",
    "    return train_vectorized, val_vectorized, test_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f835fc88",
   "metadata": {
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1646428603804,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "f835fc88"
   },
   "outputs": [],
   "source": [
    "def save_dataframes(train_processed, val_processed, method, include_test = True, include_diag = True, test_processed = ''):\n",
    "    \"\"\"\n",
    "    Function to save our dataframes\n",
    "    train_processed: train set to be saved\n",
    "    val_processed: validation set to be saved\n",
    "    method: method through which we have processed the dataframes, needed as save keyword\n",
    "    include_test: True if we want to save also the test set\n",
    "    include_diag: True if we have included the diagnosis\n",
    "    test_processed: test set to be saved\n",
    "    \"\"\"\n",
    "    if include_diag == True: diag_tag = '_diag'\n",
    "    else: diag_tag = ''\n",
    "    # need to reset the index\n",
    "    train_processed.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    train_processed.to_feather('{}train_{}{}{}'.format(path_to_processed, method, diag_tag, tag_med7))\n",
    "    # need to reset the index\n",
    "    val_processed.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    val_processed.to_feather('{}val_{}{}{}'.format(path_to_processed, method, diag_tag, tag_med7))\n",
    "    if include_test:\n",
    "        # need to reset the index\n",
    "        test_processed.reset_index(inplace=True, drop = True)\n",
    "        # save our dataset up to now in feather format\n",
    "        test_processed.to_feather('{}test_{}{}{}'.format(path_to_processed, method, diag_tag, tag_med7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "183b5ce0",
   "metadata": {
    "executionInfo": {
     "elapsed": 5684,
     "status": "ok",
     "timestamp": 1646427953919,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "183b5ce0"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary for main text\n",
    "id2word = corpora.Dictionary(train.text_def)\n",
    "# Create Dictionary for diagnosis\n",
    "id2word_diag = corpora.Dictionary(train.diagnosis_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deea7b10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110597,
     "status": "ok",
     "timestamp": 1646427161319,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "deea7b10",
    "outputId": "b8e924c1-f5b7-4d74-cb55-9a13bc6a2a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency\n",
      "(23982, 58456)\n",
      "(23982, 1704)\n",
      "(2665, 58456)\n",
      "(2665, 1704)\n",
      "(6662, 58456)\n",
      "(6662, 1704)\n",
      "Wall time: 58.8 s\n",
      "one_hot\n",
      "(23982, 58456)\n",
      "(23982, 1704)\n",
      "(2665, 58456)\n",
      "(2665, 1704)\n",
      "(6662, 58456)\n",
      "(6662, 1704)\n",
      "Wall time: 1min 5s\n",
      "tf_idf\n",
      "(23982, 58456)\n",
      "(23982, 1704)\n",
      "(2665, 58456)\n",
      "(2665, 1704)\n",
      "(6662, 58456)\n",
      "(6662, 1704)\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "# perform vectorization\n",
    "method_list = ['frequency', 'one_hot','tf_idf']\n",
    "\n",
    "for i in method_list:\n",
    "    method = i\n",
    "    print(i)\n",
    "    %time train_processed, val_processed, test_processed = gensim_traintest_vectorizer(train, val, id2word, include_diag = include_diag, id2word_diag = id2word_diag, method = method, include_test = include_test, test = test)\n",
    "    save_dataframes(train_processed, val_processed, method = method, include_test = include_test, include_diag = include_diag, test_processed = test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af86f8c",
   "metadata": {
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1646427162158,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "2af86f8c"
   },
   "outputs": [],
   "source": [
    "# need to reset the index\n",
    "y_train = pd.DataFrame(train.target)\n",
    "y_train.reset_index(inplace=True, drop = True)\n",
    "# save our dataset up to now in feather format\n",
    "y_train.to_feather('{}y_train{}'.format(path_to_processed, tag_med7))\n",
    "# need to reset the index\n",
    "y_val = pd.DataFrame(val.target)\n",
    "y_val.reset_index(inplace=True, drop = True)\n",
    "# save our dataset up to now in feather format\n",
    "y_val.to_feather('{}y_val{}'.format(path_to_processed, tag_med7))\n",
    "if include_test:\n",
    "    # need to reset the index\n",
    "    y_test = pd.DataFrame(test.target)\n",
    "    y_test.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    y_test.to_feather('{}y_test{}'.format(path_to_processed, tag_med7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd226a",
   "metadata": {
    "id": "b4fd226a"
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617dd3e",
   "metadata": {
    "id": "b617dd3e"
   },
   "source": [
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c29846fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646427953920,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "c29846fd",
    "outputId": "767abf64-51c6-43a2-c616-4c9385efeb55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12933    [family, patient, increase, short, temper, orn...\n",
       "1098     [surgery, patient, record, know, allergy, drug...\n",
       "9062     [recurrent, right, pleural, effusion, trap, ri...\n",
       "26494    [percocet, severe, number, vessel, coronary, a...\n",
       "27062    [medicine, penicillin, chest, pain, transfer, ...\n",
       "                               ...                        \n",
       "4180     [medicine, patient, record, know, allergy, dru...\n",
       "12541    [codeine, atenolol, time, day, chest, pain, lo...\n",
       "15585    [medicine, penicillin, withdrawl, rhabdomyolys...\n",
       "19714    [penicillin, chest, pain, transfer, cardiac, c...\n",
       "26578    [medicine, patient, record, know, allergy, dru...\n",
       "Name: text_def, Length: 23982, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a24417c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1646427954278,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "7a24417c",
    "outputId": "9046b6ab-5baa-4fe2-9621-12c3cce10a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdc5897b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75655,
     "status": "ok",
     "timestamp": 1646428461539,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "cdc5897b",
    "outputId": "5df35110-55f1-4635-d7cb-35cbb059ea04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from repository\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # path for the model\n",
    "    glove_path = os.path.join(path_to_repo, \"Readmission\", \"data\", \"word_embeddings\", \"glove-wiki-gigaword-300\", \"glove-wiki-gigaword-300.txt\")\n",
    "    # load the model\n",
    "    glove_wiki = KeyedVectors.load_word2vec_format(datapath(glove_path), binary = False)\n",
    "    print(\"Loaded from repository\")\n",
    "except:\n",
    "    # if the code above gives permission denied error, simply load the model (or download it) from the default directory\n",
    "    glove_wiki = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d37c87d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59648,
     "status": "ok",
     "timestamp": 1646427656556,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "d37c87d5",
    "outputId": "05bee5fa-38d5-41f8-a2b5-a10ac7107abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from repository\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # path for the model\n",
    "    word2vec_path = os.path.join(path_to_repo, \"Readmission\", \"data\", \"word_embeddings\", \"word2vec_google_news_300\", \"GoogleNews_vectors_negative300.bin\")\n",
    "    # load the model\n",
    "    w2v_google_news = KeyedVectors.load_word2vec_format(datapath(word2vec_path), binary = True)\n",
    "    print(\"Loaded from repository\")\n",
    "except:\n",
    "    # if the code above gives permission denied error, simply load the model (or download it) from the default directory\n",
    "    w2v_google_news = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5f7b3c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114431,
     "status": "ok",
     "timestamp": 1646428068706,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "e5f7b3c5",
    "outputId": "27722c62-703f-41a8-ba2d-008c67c3d1f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from repository\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # path for the model\n",
    "    word2vec_med_path = os.path.join(path_to_repo, \"Readmission\", \"data\", \"word_embeddings\", \"wikipedia_pubmed_and_PMC_w2v.bin\")\n",
    "    # load the model\n",
    "    w2v_med = KeyedVectors.load_word2vec_format(datapath(word2vec_med_path), binary = True)\n",
    "    print(\"Loaded from repository\")\n",
    "except:\n",
    "    # if the code above gives permission denied error, simply load the model (or download it) from the default directory\n",
    "    print(\"No Embeddings Found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0899cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114431,
     "status": "ok",
     "timestamp": 1646428068706,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "e5f7b3c5",
    "outputId": "27722c62-703f-41a8-ba2d-008c67c3d1f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from repository\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # path for the model\n",
    "    bio_vec_path = os.path.join(path_to_repo, \"Readmission\", \"data\", \"word_embeddings\", \"BioWordVec_PubMed_MIMICIII_d200.vec.bin\")\n",
    "    # load the model\n",
    "    bio_w2v = KeyedVectors.load_word2vec_format(datapath(bio_vec_path), binary = True)\n",
    "    print(\"Loaded from repository\")\n",
    "except:\n",
    "    # if the code above gives permission denied error, simply load the model (or download it) from the default directory\n",
    "    print(\"No Embeddings Found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aba026ae",
   "metadata": {
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1646428496208,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "aba026ae"
   },
   "outputs": [],
   "source": [
    "def embedding_feats(list_of_lists, DIMENSION, w2v_model):\n",
    "    \"\"\"\n",
    "    Function that takes in the input text dataset in form of list of lists (or pandas Series) where each sentence is a \n",
    "    list of words all the sentences are inside a list \n",
    "    list_of_lists: our list of sentences\n",
    "    DIMENSION: the dimension of the word embeddings\n",
    "    w2w_model: our word embedding model\n",
    "    credits - https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3\n",
    "    \"\"\"\n",
    "    zeros_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    missing = set()\n",
    "    missing_sentences = set()\n",
    "    #Traverse over each sentence\n",
    "    for tokens in tqdm(list_of_lists):\n",
    "        # Initially assign zeroes as the embedding vector for the sentence\n",
    "        feat_for_this = zeros_vector\n",
    "        #Count the number of words in the embedding for this sentence\n",
    "        count_for_this = 0\n",
    "        #Traverse over each word of a sentence\n",
    "        for token in tokens:\n",
    "            #Check if the word is in the embedding vector\n",
    "            if token in w2v_model:\n",
    "                #Add the vector of the word to vector for the sentence\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this +=1\n",
    "            #Else assign the missing word to missing set just to have a look at it\n",
    "            else:\n",
    "                missing.add(token)\n",
    "        #If no words are found in the embedding for the sentence\n",
    "        if count_for_this == 0:\n",
    "            #Assign all zeroes vector for that sentence\n",
    "            feats.append(feat_for_this)\n",
    "            #Assign the missing sentence to missing_sentences just to have a look at it\n",
    "            missing_sentences.add(' '.join(tokens))\n",
    "        #Else take average of the values of the embedding for each word to get the embedding of the sentence\n",
    "        else:\n",
    "            feats.append(feat_for_this/count_for_this)\n",
    "    print(\"Total missing words: {}\".format(len(missing)))\n",
    "    print(\"Total missing sentences: {}\".format(len(missing_sentences)))\n",
    "    # convert our list of arrays to a DataFrame\n",
    "    feats = pd.DataFrame(feats)\n",
    "    return feats, missing, missing_sentences\n",
    "\n",
    "def embedding_diag(list_of_lists, DIMENSION, w2v_model, include_diag = True, lemmatization = True):\n",
    "    \"\"\"\n",
    "    Function to apply word embeddins to both final text and diagnosis\n",
    "    include_diag: True if we want to apply it also to diagnosis embeddings\n",
    "    lemmatization: True if we want to use the lemmatized text\n",
    "    \"\"\"\n",
    "    print(\"\\nFinal Text:\")\n",
    "    if lemmatization == True:\n",
    "      list_vectorization, missing, missing_sentences = embedding_feats(list_of_lists.text_def, DIMENSION, w2v_model)\n",
    "    else:\n",
    "      list_vectorization, missing, missing_sentences = embedding_feats(list_of_lists.text_def_nolemma, DIMENSION, w2v_model)\n",
    "    missing = list(missing)\n",
    "    missing_sentences = list(missing_sentences)\n",
    "    if include_diag == True:\n",
    "        print(\"\\nDiagnosis:\")\n",
    "        if lemmatization == True:\n",
    "          df_vectors_diag, missing_diag, missing_sentences_diag = embedding_feats(list_of_lists.diagnosis_def, DIMENSION, w2v_model)\n",
    "        else:\n",
    "          df_vectors_diag, missing_diag, missing_sentences_diag = embedding_feats(list_of_lists.diagnosis_def_nolemma, DIMENSION, w2v_model)\n",
    "        df_final = pd.merge(list_vectorization, df_vectors_diag, left_index = True, right_index = True, suffixes = (\"\",\"_diag\"))\n",
    "        missing.append(list(missing_diag))\n",
    "        missing_sentences.append(list(missing_sentences_diag))\n",
    "    else:\n",
    "        df_final = df_vectors.copy()\n",
    "    return df_final, missing, missing_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "wt4EsUV3QQzR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50362,
     "status": "ok",
     "timestamp": 1646428659630,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "wt4EsUV3QQzR",
    "outputId": "39d5a802-9aad-4cb3-d4f2-12f2df1a7d53",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/23982 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec\n",
      "Train Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23982/23982 [01:46<00:00, 225.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 55656\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████▍                                                            | 4371/23982 [00:00<00:00, 42125.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 23982/23982 [00:00<00:00, 51598.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 1082\n",
      "Total missing sentences: 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                              | 29/2665 [00:00<00:09, 274.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n",
      "Validation Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2665/2665 [00:10<00:00, 254.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 12589\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2665/2665 [00:00<00:00, 48846.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n",
      "Total missing words: 172\n",
      "Total missing sentences: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|▍                                                                              | 32/6662 [00:00<00:22, 293.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 s\n",
      "Test Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6662/6662 [00:24<00:00, 268.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 24078\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6662/6662 [00:00<00:00, 72683.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n",
      "Total missing words: 419\n",
      "Total missing sentences: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/23982 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe\n",
      "Train Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23982/23982 [01:32<00:00, 257.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 56450\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▏                                                                | 2987/23982 [00:00<00:00, 29607.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 23982/23982 [00:00<00:00, 34606.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 1069\n",
      "Total missing sentences: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                              | 32/2665 [00:00<00:09, 286.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 37s\n",
      "Validation Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2665/2665 [00:09<00:00, 283.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2665/2665 [00:00<00:00, 52913.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 12894\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n",
      "Total missing words: 161\n",
      "Total missing sentences: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|▎                                                                              | 24/6662 [00:00<00:28, 235.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.74 s\n",
      "Test Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6662/6662 [00:23<00:00, 281.70it/s]\n",
      "  0%|                                                                                         | 0/6662 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 24649\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6662/6662 [00:00<00:00, 53119.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 407\n",
      "Total missing sentences: 51\n",
      "Wall time: 24.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 1/23982 [00:00<1:04:22,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V_Med\n",
      "Train Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23982/23982 [01:47<00:00, 224.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 39758\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████                                                          | 5200/23982 [00:00<00:00, 51279.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 23982/23982 [00:00<00:00, 56519.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 571\n",
      "Total missing sentences: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                              | 32/2665 [00:00<00:09, 277.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 49s\n",
      "Validation Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2665/2665 [00:10<00:00, 264.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2665/2665 [00:00<00:00, 65557.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 7136\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n",
      "Total missing words: 69\n",
      "Total missing sentences: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                              | 26/6662 [00:00<00:28, 231.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n",
      "Test Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6662/6662 [00:25<00:00, 258.21it/s]\n",
      "  0%|                                                                                         | 0/6662 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 15231\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6662/6662 [00:00<00:00, 55782.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 185\n",
      "Total missing sentences: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/23982 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.5 s\n",
      "Bio_W2V\n",
      "Train Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23982/23982 [01:54<00:00, 209.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 1234\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████▍                                                         | 5333/23982 [00:00<00:00, 47242.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 23982/23982 [00:00<00:00, 49509.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 71\n",
      "Total missing sentences: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                             | 42/2665 [00:00<00:12, 206.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 57s\n",
      "Validation Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2665/2665 [00:11<00:00, 239.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2665/2665 [00:00<00:00, 48325.03it/s]\n",
      "  0%|                                                                                         | 0/6662 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 179\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n",
      "Total missing words: 13\n",
      "Total missing sentences: 1\n",
      "Wall time: 11.4 s\n",
      "Test Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6662/6662 [00:27<00:00, 242.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6662/6662 [00:00<00:00, 50824.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 435\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 21\n",
      "Total missing sentences: 3\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "# perform vectorization WITHOUT LEMMAS\n",
    "embedding_dict = {'word2vec': w2v_google_news, 'GloVe': glove_wiki, 'W2V_Med': w2v_med, 'Bio_W2V': bio_w2v}\n",
    "\n",
    "\n",
    "for method, embedding in embedding_dict.items():\n",
    "  if method == 'W2V_Med' or method == 'Bio_W2V':\n",
    "    vector_dim = 200\n",
    "  else:\n",
    "    vector_dim = 300      \n",
    "  print(method)\n",
    "  print(\"Train Set:\")\n",
    "  %time train_processed, missing_train, missing_sentences_train = embedding_diag(train, vector_dim, embedding, include_diag = include_diag, lemmatization = False)\n",
    "  print(\"Validation Set:\")\n",
    "  %time val_processed, missing_val, missing_sentences_val = embedding_diag(val, vector_dim, embedding, include_diag = include_diag, lemmatization = False)\n",
    "  if include_test == True:\n",
    "    print(\"Test Set:\")\n",
    "    %time test_processed, missing_test, missing_sentences_test = embedding_diag(test, vector_dim, embedding, include_diag = include_diag, lemmatization = False)\n",
    "  else:\n",
    "    test_processed = []\n",
    "  save_dataframes(train_processed, val_processed, method = method+'_nolemma', include_test = include_test, include_diag = include_diag, test_processed = test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aef3cec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43251,
     "status": "ok",
     "timestamp": 1646428702876,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "aef3cec5",
    "outputId": "30de6f0c-d3ce-47d8-8a6b-49e8c4ce7850",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 30/23982 [00:00<01:26, 276.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec\n",
      "Train Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23982/23982 [01:12<00:00, 330.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 35868\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████▎                                             | 9182/23982 [00:00<00:00, 82372.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 23982/23982 [00:00<00:00, 85874.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 447\n",
      "Total missing sentences: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                             | 39/2665 [00:00<00:06, 381.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n",
      "Validation Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2665/2665 [00:06<00:00, 399.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2665/2665 [00:00<00:00, 88835.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 8173\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n",
      "Total missing words: 81\n",
      "Total missing sentences: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                              | 44/6662 [00:00<00:15, 428.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7 s\n",
      "Test Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6662/6662 [00:16<00:00, 405.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 15568\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6662/6662 [00:00<00:00, 88842.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 177\n",
      "Total missing sentences: 39\n",
      "Wall time: 17.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                             | 41/23982 [00:00<01:05, 364.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe\n",
      "Train Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23982/23982 [00:59<00:00, 404.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 36883\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████▌                                        | 10521/23982 [00:00<00:00, 105137.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 23982/23982 [00:00<00:00, 105885.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 478\n",
      "Total missing sentences: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                             | 47/2665 [00:00<00:06, 428.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n",
      "Validation Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2665/2665 [00:06<00:00, 425.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2665/2665 [00:00<00:00, 84250.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 8632\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n",
      "Total missing words: 82\n",
      "Total missing sentences: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                              | 52/6662 [00:00<00:14, 453.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.61 s\n",
      "Test Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6662/6662 [00:15<00:00, 422.27it/s]\n",
      "  0%|                                                                                         | 0/6662 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 16246\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6662/6662 [00:00<00:00, 82152.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 182\n",
      "Total missing sentences: 39\n",
      "Wall time: 16.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 36/23982 [00:00<01:15, 316.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V_Med\n",
      "Train Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23982/23982 [01:01<00:00, 390.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 25255\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████▍          | 20469/23982 [00:00<00:00, 100249.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 23982/23982 [00:00<00:00, 100407.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 222\n",
      "Total missing sentences: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                             | 43/2665 [00:00<00:06, 413.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 3s\n",
      "Validation Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2665/2665 [00:07<00:00, 380.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2665/2665 [00:00<00:00, 68364.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 4641\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n",
      "Total missing words: 34\n",
      "Total missing sentences: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                              | 29/6662 [00:00<00:25, 260.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.31 s\n",
      "Test Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6662/6662 [00:16<00:00, 397.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 9738\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 6662/6662 [00:00<00:00, 111034.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n",
      "Total missing words: 82\n",
      "Total missing sentences: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/23982 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.3 s\n",
      "Bio_W2V\n",
      "Train Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23982/23982 [01:02<00:00, 382.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 4559\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████▉                                         | 10303/23982 [00:00<00:00, 100615.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 23982/23982 [00:00<00:00, 101600.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 42\n",
      "Total missing sentences: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                             | 42/2665 [00:00<00:07, 362.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 4s\n",
      "Validation Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2665/2665 [00:06<00:00, 396.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2665/2665 [00:00<00:00, 91894.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 813\n",
      "Total missing sentences: 0\n",
      "\n",
      "Diagnosis:\n",
      "Total missing words: 9\n",
      "Total missing sentences: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                              | 44/6662 [00:00<00:16, 407.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.94 s\n",
      "Test Set:\n",
      "\n",
      "Final Text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6662/6662 [00:16<00:00, 402.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing words: 1754\n",
      "Total missing sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6662/6662 [00:00<00:00, 89776.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis:\n",
      "Total missing words: 18\n",
      "Total missing sentences: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "# perform vectorization\n",
    "embedding_dict = {'word2vec': w2v_google_news, 'GloVe': glove_wiki, 'W2V_Med': w2v_med, 'Bio_W2V': bio_w2v}\n",
    "\n",
    "for method, embedding in embedding_dict.items():\n",
    "    if method == 'W2V_Med' or method == 'Bio_W2V':\n",
    "      vector_dim = 200\n",
    "    else:\n",
    "      vector_dim = 300   \n",
    "    print(method)\n",
    "    print(\"Train Set:\")\n",
    "    %time train_processed, missing_train, missing_sentences_train = embedding_diag(train, vector_dim, embedding, include_diag = include_diag)\n",
    "    print(\"Validation Set:\")\n",
    "    %time val_processed, missing_val, missing_sentences_val = embedding_diag(val, vector_dim, embedding, include_diag = include_diag)\n",
    "    if include_test == True:\n",
    "        print(\"Test Set:\")\n",
    "        %time test_processed, missing_test, missing_sentences_test = embedding_diag(test, vector_dim, embedding, include_diag = include_diag)\n",
    "    else:\n",
    "        test_processed = []\n",
    "    save_dataframes(train_processed, val_processed, method = method, include_test = include_test, include_diag = include_diag, test_processed = test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841842e6",
   "metadata": {},
   "source": [
    "# Create DFs without diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a4eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(method, include_diag = True, include_test = True):\n",
    "    \"\"\"\n",
    "    Function to load train, test and validation set based on the chosen method\n",
    "    method: string for the processing method we want to load\n",
    "    include_diag: if we want to load the dataframes with the diagnosis text, default True\n",
    "    include_test: if we want to load also the test set, default True\n",
    "    \"\"\"\n",
    "    global path_to_processed\n",
    "    if include_diag == True: diag_tag = '_diag'\n",
    "    else: diag_tag = ''\n",
    "    # load it back\n",
    "    train = pd.read_feather(f'{path_to_processed}train_{method}{diag_tag}{tag_med7}')\n",
    "    val = pd.read_feather(f'{path_to_processed}val_{method}{diag_tag}{tag_med7}')\n",
    "    if include_test == True:\n",
    "        test = pd.read_feather(f'{path_to_processed}test_{method}{diag_tag}{tag_med7}')\n",
    "    else: test = []\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f199f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency\n",
      "Dataframes saved\n",
      "one_hot\n",
      "Dataframes saved\n",
      "tf_idf\n",
      "Dataframes saved\n",
      "word2vec\n",
      "Dataframes saved\n",
      "GloVe\n",
      "Dataframes saved\n",
      "W2V_Med\n",
      "Dataframes saved\n",
      "Bio_W2V\n",
      "Dataframes saved\n"
     ]
    }
   ],
   "source": [
    "method_list = ['frequency', 'one_hot','tf_idf', 'word2vec', 'GloVe', 'W2V_Med', 'Bio_W2V']\n",
    "\n",
    "for method in method_list:\n",
    "    print(method)\n",
    "    train, val, test = load_datasets(method, include_diag = True, include_test = include_test)\n",
    "    diag_exclusion = [x for x in train.columns if \"_diag\" not in x]\n",
    "    train = train[diag_exclusion]\n",
    "    val = val[diag_exclusion]\n",
    "    if include_test == True: \n",
    "        test = test[diag_exclusion]\n",
    "    save_dataframes(train, val, method = method, include_test = include_test, include_diag = False, test_processed = test)\n",
    "    print(\"Dataframes saved\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[COLAB] 4. Text Vectorization (rework).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
